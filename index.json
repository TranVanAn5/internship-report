[{"uri":"https://tranvanan5.github.io/internship-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS Lambda introduces tiered pricing for Amazon CloudWatch logs and additional logging targets by Shridhar Pandey and Matthew Barker | May 1, 2025 | [Announcements], [AWS Lambda], [Intermediate (200)], [Serverless]\nEffective logging is an important part of an observability strategy when building serverless applications with AWS Lambda.\nLambda automatically collects and sends logs to Amazon CloudWatch Logs. This allows you to focus on building application logic rather than setting up logging infrastructure, and makes it easier for operators to troubleshoot errors and performance.\nOn May 1, 2025, AWS announced changes to Lambda logging that reduce the cost of Lambda CloudWatch logging and make it easier and more cost-effective to use multiple monitoring tools. Lambda Logs is now available with tiered pricing based on volume when using the CloudWatch Logs Standard and Infrequent Access log classes. When you create Lambda logs at scale, you can expect to see immediate cost reductions under this new pricing model. In addition to CloudWatch Logs, Lambda now also supports Amazon S3 and Amazon Data Firehose as additional destinations for Lambda logs. Lambda logs are sent to S3, and Firehose is also available with tiered pricing based on volume.\nThis blog post covers some recent Lambda logging improvements and describes how these changes provide a simpler, more cost-effective logging experience for Lambda.\nOverview Logging provides developers and operators with valuable data to debug and troubleshoot application behavior, performance issues, and potential errors. This becomes even more important for serverless applications built with Lambda due to the transient and stateless nature of the Lambda execution environment. Lambda\u0026rsquo;s built-in integration with CloudWatch Logs ensures that logs for every function call are available for analysis. The log data collected includes application logs generated by your Lambda function code and system logs generated by the Lambda service when running the function code. CloudWatch Logs allows you to search, filter, and analyze log data for troubleshooting, monitoring metrics, and setting up alerts.\nLogging requirements are growing as serverless applications become more complex and scale, sometimes consisting of hundreds or thousands of Lambda functions, generating significant log volumes. Organizations need sophisticated logging solutions that can handle this scale while remaining cost-effective. Some scenarios, such as monitoring critical business transactions, require real-time log analysis, while others focus on post-event forensic analysis. Debug logs from development and staging environments typically require high granularity, while you may want lower granularity in production logs to improve signal-to-noise ratio.\nRecent Improvements to Lambda Logging In recent years, Lambda and CloudWatch Logs have expanded Lambda\u0026rsquo;s logging capabilities to meet the growing needs of serverless applications. These capabilities provide deeper insights, better control, and more cost-effective solutions for collecting, processing, and consuming logs, thereby enhancing the serverless observability experience. Lambda advanced logging controls give developers control over log creation and content. These controls allow you to collect Lambda logs in a structured JSON format. You don\u0026rsquo;t need to use a logging library and customize separate log levels (INFO, DEBUG, WARN, ERROR) for application logs and system logs. This reduces logging costs by ensuring only the logs needed are generated while maintaining appropriate visibility across different environments. For example, you can set a granular DEBUG logging level in development environments while limiting production logging to ERROR to improve signal-to-noise ratio and control costs.\nThe Infrequent Access log class for CloudWatch Logs introduces a cost-effective solution for logs that need to be retained but are rarely accessed. The Infrequent Access class has a 50% lower ingestion price per GB than the Standard log class. This tailored feature set allows you to reduce logging costs while maintaining access to historical data for compliance, audit, or forensic analysis purposes.\nCloudWatch Logs Live Tail is a real-time, interactive log analysis and streaming feature. Live Tail simplifies debugging and monitoring; it allows you to observe log output as your functions execute without leaving the Lambda console. This makes it easier to identify and diagnose issues during development and troubleshooting. Live Tail Logs is also available in the Visual Code IDE.\nTiered Pricing for Lambda Logs in CloudWatch Logs Starting today, Lambda logs sent to CloudWatch Logs will be classified as Vended Logs, which are logs from specific AWS services available at volume pricing. This replaces the previous flat pricing model when using the CloudWatch Logs Standard log class. For example, in the AWS US East (N. Virginia) Region, you were charged $0.50 per GB for using the Standard log class for your Lambda logs. Under the new pricing model, you are charged for sending Lambda logs to CloudWatch Logs starting at $0.50 per GB for the first use. As log volume increases, the price per GB automatically decreases across tiers, down to as low as $0.05 per GB at the lowest tier. This price change is automatically applied to all Lambda logs sent to CloudWatch Logs, requiring no code or configuration changes.\nIngested Data CloudWatch Standard Logs CloudWatch Logs Infrequent Access First 10 TB/month $0.50/GB $0.25/GB Next 20 TB/month $0.25/GB $0.15/GB Next 20 TB/month $0.10/GB $0.075/GB More than 50 TB/month $0.05/GB $0.05/GB *Table 1: Tiered pricing for Lambda logs in CloudWatch Logs in the US East (N. Virginia) Region *\nWhen you create Lambda logs at scale, you will see immediate cost reductions under this new pricing model. For example, if you create 60 TB of Lambda logs per month in CloudWatch Logs, your cost will decrease by 58% (from $30,000 to $12,500). The pricing tiers scale with your logging volume, ensuring cost benefits increase as your application grows. This allows you to maintain comprehensive logging practices that could have been prohibitively expensive in the past. Tiered pricing for Vended logs applies to all Vended logs ingested into CloudWatch, not per-service tiers.\nWhen collecting other off-the-shelf logs, such as Amazon Virtual Private Cloud flow logs and Amazon Route 53 resolver query logs, you will see a larger discount when tiering is applied to the consolidated log collection volume.\nNew Lambda Logging Destinations: Amazon S3 and Amazon Data Firehose Starting today, in addition to CloudWatch Logs, Lambda also supports Amazon S3 and Amazon Data Firehose as destinations for Lambda logs. When using S3 or Firehose as a destination, logging costs start at $0.25/GB. Tiered pricing also applies, with prices dropping to $0.05/GB at the lowest tier. This tiered pricing also applies to consolidated log volumes.\nIngested Data CloudWatch Standard Logs CloudWatch Logs Infrequent Accesses First 10 TB/month $0.50/GB $0.25/GB Next 20 TB/month $0.25/GB $0.15/GB Next 20 TB/month $0.10/GB $0.075/GB More than 50 TB/month $0.05/GB $0.05/GB *Table 2: Tiered Pricing for Lambda Log Delivery to Amazon S3 and Amazon Data Firehose in the US East (N. Virginia) Region *\nDelivering Lambda logs directly to S3 provides more flexibility in log management. Firehose support simplifies the delivery of Lambda logs to additional destinations such as Amazon OpenSearch Service, HTTP endpoints, and third-party observability providers. This fits with the established log delivery model used with other AWS compute services such as Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Compute Cloud (Amazon EC2).\nThis new capability provides significant cost benefits and streamlines the delivery of logs to additional logging destinations, making it easier to use multiple monitoring tools (including CloudWatch) when building serverless applications with Lambda.\nSetting up a new Lambda logging destination [cite_start]All new and existing Lambda functions have CloudWatch Logs as the default logging destination, with S3 and Firehose as alternatives[cite: 1]. [cite_start]When you choose S3 or Firehose, Lambda will send logs via a new CloudWatch Logs delivery log class**[cite: 1].\nBasic setup steps in the Lambda console:\nAll new and existing Lambda functions have CloudWatch Logs as the logging destinationdefault logs, with S3 and Firehose as alternatives. When you select S3 or Firehose as the log destination, Lambda sends logs to the selected destination via a new CloudWatch Logs delivery log class. This log class enables efficient routing but does not support CloudWatch Logs Standard log class features, such as Logs Insights and Live Tail.\nTo set up S3 or Firehose as the destination for your Lambda logs in the Lambda console:\nNavigate to the Lambda console, and select or create a function to set up an S3 or Firehose log destination.\nIn the Configuration tab, select Monitoring and operations tools in the left pane.\nSelect Edit under Logging configuration. This opens the Edit logging configuration page\nIn the \u0026ldquo;Log destination\u0026rdquo; section, select Amazon S3 or Amazon Data Firehose. Amazon CloudWatch Logs is the default selection.\nIn CloudWatch delivery log group, select Create new log group or Existing log group.\nTo create a new delivery log group to send logs to S3, enter a log group name and specify the destination S3 bucket. Provide an AWS Identity and Access Management (IAM) role for CloudWatch Logs to deliver logs to S3.\nTo use an existing delivery log group, select one from the Delivery log group pool. The selected delivery log group must have a configured destination (S3 or Firehose) and match the destination you selected.\nAdvanced logging controls are also available for S3 and Firehose destinations. These controls include JSON structured format selection and log-level filters for both application and system logs. This gives you advanced log management controls for easier searching, filtering, and analysis. You can also use the AWS Command Line Interface (AWS CLI) and infrastructure as code (IaC) tools like AWS CloudFormation and AWS Cloud Development Kit (AWS CDK) to set up Lambda log delivery to S3 and Firehose.\nBest Practices To get the most out of the changes announced today, ensure your logging strategy closely aligns with your workload requirements. For example, consider sending critical production logs to CloudWatch Logs to take advantage of advanced real-time alerting and analytics features. You now automatically get volume discounts through tiered pricing in CloudWatch Logs for high-volume logging scenarios. For logs that require long-term retention for historical analysis, you can use S3 storage classes to further reduce costs. When using existing or third-party monitoring tools, direct integration through Firehose eliminates the need for custom forwarding solutions and associated costs.\nOptimizing logging costs goes beyond destination selection. Regularly monitor log volumes to understand the impact of pricing tiers. Implement appropriate retention policies to prevent unnecessary archiving of old logs and log sampling for high-volume debug logs. Consider using different logging strategies across development, staging, and production environments to balance the need for visibility with cost-effectiveness.\nConclusion Tiered pricing for Lambda logs in CloudWatch Logs and S3 and Firehose support for additional logging destinations improve Lambda application observability. You can now manage logging costs at scale and extend your Lambda monitoring solutions through cost-effective, easy-to-configure integrations. Whether you are building new serverless applications or optimizing existing ones, these enhancements help you deploy comprehensive logging strategies that scale cost-effectively across your workloads.\nNew features announced today are available in all AWS Regions Support for configuring log delivery to S3 and Firehose in the Lambda console is currently available in the US East (Ohio), US East (N. Virginia), US West (Oregon), and EU (Ireland) Regions, with additional Regions coming soon. Review the Lambda documentation and CloudWatch Logs documentation to learn more about these features and how to use them. Review the CloudWatch pricing page to learn more about how these features are priced.\nFor more serverless learning resources, visit Serverless Land.\n"},{"uri":"https://tranvanan5.github.io/internship-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AWS DMS Deployment Guide: Building Resilient Database Migrations Through Testing, Monitoring, and SOPs By Sushant Deshmukh, Alex Anto Kizhakeyyepunnil Joy, and Sanyam Jain | May 5, 2025 | in Advanced (300), AWS Database Migration Service (AWS DMS), AWS Well-Architected,Best Practices |\nAWS Database Migration Service (AWS DMS) simplifies database migration and replication, providing a managed solution for customers. Our observations across multiple enterprise deployments show that investing time in proactive database migration planning pays off. Organizations that adopt a comprehensive setup strategy consistently experience fewer disruptions and better migration outcomes.\nIn this article, we outline proactive measures to optimize AWS DMS deployments from the initial setup phase. By using strategic planning and architectural vision, organizations can improve the reliability of their replication systems, improve performance, and avoid common pitfalls.\nWe explore strategies and best practices in the following key areas:\nPlan and run proof-of-concept (PoC) tests Implement system failure testing Develop standard operating procedures (SOPs) Monitor and alert Apply the principles of the AWS Well-Architected Framework Plan and execute PoCs Executing PoCs helps detect and fix environmental issues early. It also generates information that you can use to estimate your total migration time and resource commitments.\nHere are the basic steps to a successful PoC:\nPlan and deploy a test environment with the appropriate replicated instances, tasks, and AWS DMS endpoints. For more information on resource planning and provisioning, you can refer to [AWS Database Migration Service (AWS DMS) Best Practices].](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html) Use workloads similar to those in the production environment. It is imperative to simulate your production environment as closely as possible to increase the likelihood of encountering various issues. Perform error testing based on the scenarios discussed in the table in the next section. Monitor resource utilization and bottlenecks that occur in the PoC and review the resource planning and deployment accordingly. Record your observations and perform migration evaluation by comparing them with business outcomes. This includes evaluating post-migration recovery times and application Service Level Agreements (SLAs) for both the migration and ongoing business operations. If these migration and operational requirements are not met, revisit the planning phase to ensure alignment with your business needs. Conduct systematic failure testing All systems, regardless of their resilience, are subject to failures and downtime. For organizations operating mission-critical workloads, proactive planning is essential to maintaining business continuity and meeting SLAs. This section provides a strategic framework for developing Standard Operating Procedures (SOPs) that establish clear recovery protocols and minimize operational impact in the event of a system disruption.\nWhen implementing AWS DMS, understanding potential points of failure becomes critical to building resilient systems. The following table outlines common failure scenarios in AWS DMS replication systems, which should serve as a foundation for your testing strategy. While this table is comprehensive, we encourage you to expand on these scenarios based on your specific architecture, compliance requirements, and business objectives to fully cover the potential failure modes in your environment.\nPoint of Failure Potential Downtime Scenarios Testing Potential Mitigation Strategies Source and Target Databases Performance bottlenecks on the database server such as high CPU, memory constraints. Stress your system with a benchmark tool like sysbench to simulate high load on the database server. You can provide at Read-only database nodes for AWS DMS-supported engines, using a read replica as the source. For more information, see Data Migration Source. You can also scale database resources and optimize database parameters. Data access issues due to insufficient permissions Using a database account for AWS DMS that does not have sufficient permissions. Create a database user using the least-privilege principle. Refer to the documentation for the corresponding AWS DMS source endpoint for required DMS privileges for each database engine. Failover database (if using a primary or standby setup) Perform a database failover from the primary node to the secondary node. In case AWS DMS may attempt to reconnect to the old primary after failover, this behavior depends on the Time to Live (TTL). You will need to restart the task after the TTL is refreshed. See Why is my connection being redirected to the reader instance when I try to connect to my Amazon Aurora writer endpoint? Shut down the database OR a failure occurs Stop the database that is replicating DMS. Record all DMS task behavior for SOP development, then resume the task when the database issue has been resolved. Transaction logs not available Delete shorter retention logs when the task is offline or running slow. Log DMS task observations to generate SOP and continue the task after generating the transaction log or performing a new full load if the log is not available. Make structural changes such as schemas, tables, indexes, partitions, and data types Run various data definition language (DDL) statements to modify the relevant table. See the list of supported DDL statements and task settings. Network errors (applies to source and destination) Connectivity issues including network, DNS, and SSL errors Remove the source IP from the AWS DMS security group OR modify iptables; Remove the certificate from the DMS endpoint; Modify the MTU (maximum transmission unit) value. Refer to troubleshooting AWS DMS endpoint connectivity and problems connecting to Amazon RDS. Packet loss Use the traffic control (tc) command on Linux systems OR use the AWS Fault Injection Simulator (FIS). See Troubleshooting Networking Issues During Database Migration with the AWS DMS Diagnostic Support AMI and Working with the AWS DMS Diagnostic Support AMI AWS DMS Error Restart Single-AZ Replication Instance error Restart AWS DMS replication instance. DMS will automatically resume tasks after the replication instance restarts. Restart Multi-AZ Replication instance with failover during replication in progress Restart AWS DMS replication instance, selecting the “Restart with planned failover?” option. DMS automatically resumes tasks after a Multi-AZ failover of the replication instance. EBS storage full Enables detailed debug logging for multiple log elements resulting in full storage due to AWS DMS logs. Set up alerts when storage reaches 80% and expand the storage associated with the DMS replication instance. For more information, see Why is my AWS DMS replication DB instance in a storage full state? Apply changes in the maintenance window Modify the configuration for your DMS replication instance that results in downtime and select the “Apply in the next scheduled maintenance window” option. DMS automatically resumes tasks after maintenance. Resource contention on the replica instance (high CPU, memory contention, higher than baseline IOPS) Create multiple DMS tasks with high values ​​for MaxFullLoadSubTasks on a small DMS replica instance. Set up monitoring and alerting on key CloudWatch metrics, as discussed in the Monitoring and Alerting section. Extend the instance class or you can move the tasks to a new replica instance. Upgrade the DMS replica instance Upgrade the DMS backup instance. DMS no longer supports instancesThe DMS version is outdated, so users need to upgrade their backup instance. For more information, please refer to the AWS DMS Release Notes. To minimize downtime related to this activity, we recommend that you perform a thorough PoC. After testing the PoC, you can plan to create new replication instances running on the latest DMS version and move all your tasks to off-peak hours when the change data capture (CDC) latency is zero or minimal. For more information, see Migrate a Task. You can also see Perform a side-by-side upgrade in AWS DMS by moving tasks to minimize business impact. Data issues Data replication Run a fully loaded task only twice, the first time stopping the task in the middle and the second time running the task with \u0026ldquo;DO NOTHING\u0026rdquo; configured for Target table prepare mode. Use DMS validation for supported database engines. If validation reports any mismatches, you need to investigate based on the exact error. To mitigate the issue, you can perform a backfill by creating a full load or table reload (if applicable) task for the specific tables, and then create a continuous replication task. Data loss Create triggers on the target to delete or truncate random records. We recommend using DMS authentication to avoid these issues. You can perform a table reload or task, or create a new full load task and modify the data collection task to perform a new data load for the affected table(s). Table errors Obtaining exclusive access locks on tables before starting the DMS task OR using unsupported data types. This issue may be due to an unsupported feature or configuration of DMS. Investigation based on the exact error is required. For more information, please refer to Why is my AWS DMS task in an error state? Latency issues Swap file accumulation on the replicated instance Initiate long-running transactions with large change counts and monitor the CloudWatch metric CDCChangesDiskSource. Monitor the CDCChangesDiskSource and CDCChangesDiskTarget metrics. Refer to this Knowledge Center article for how to create an SOP: What are swap files and why are they taking up space on my AWS DMS instance? BatchApply error Delete a record on the target and update it on the source using BatchApply on the task. Set up an alert on the DMS CloudWatch log for a failed bulk apply operation, please refer to the Monitoring and Alerting section for detailed instructions. For troubleshooting and creating SOPs, please refer to this Knowledge Center article: How can I troubleshoot why Amazon Redshift switches to batch mode? Data validation issues Missing source These can be simulated due to missing data on the source and target. Review the supported use cases and limitations with AWS DMS Data Validation and refer to the following Knowledge Center article for more information: Why did my AWS DMS task validation fail or why is validation not progressing? Missing Targets These can be simulated due to missing data about the source and target. Review the supported use cases and limitations with AWS DMS Data Validation and refer to the following Knowledge Center article for more information: Why did my AWS DMS task validation fail or why is validation not progressing? Record Differences You can create different table schemas in the source and target to simulate this scenario. Review the supported use cases and limitations with AWS DMS Data Validation and refer to the following Knowledge Center article for more information: Why did my AWS DMS task validation fail or why is validation not progressing? No tFound a qualified primary/unique key Validation requires a primary or unique key in the table. LOBs and some other data types are not supported with DMS validation. For more details, see validation restrictions. Review the supported use cases and limitations with AWS DMS Data Validation and refer to the following Knowledge Center article for more information: Why did my AWS DMS task validation fail or why is validation not progressing? By systematically testing these scenarios and documenting the results, organizations can develop robust recovery processes that address both common and unique failure modes. This proactive approach not only helps maintain system reliability but also provides operations teams with clear protocols for addressing issues as they arise.\nDevelop Standard Operating Procedures (SOPs) In your failure testing scenarios, carefully document the impact of each incident on your replication system. This documentation forms the basis for creating custom SOPs that your team can rely on when managing your AWS DMS deployment. The mitigation strategies outlined in our failure testing framework serve as a great starting point for developing these procedures.\nYour initial SOPs will appear early in the PoC testing process. These procedures should be considered a living document, requiring regular updates and refinements as you gain operational experience and encounter new scenarios. The dynamic nature of database migrations means that your SOPs will evolve with your understanding of system behavior and emerging challenges.\nFor additional guidance on handling complex migration scenarios, we recommend reviewing our three-part blog series on debugging AWS DMS migrations. These resources provide valuable insights that can help you develop systematic approaches to troubleshooting, even for scenarios not covered in your existing SOPs. You can find these detailed instructions at:\nDebugging Your AWS DMS Migrations: What to Do When Things Go Wrong (Part 1) Debugging Your AWS DMS Migrations: What to Do When Things Go Wrong (Part 2) Debugging Your AWS DMS Migrations: What to Do When Things Go Wrong (Part 3) By documenting and testing these processes, organizations can accurately measure and validate the ability of their replication system to meet SLAs, especially in the event of a catastrophic failure. This proactive approach helps identify potential bottlenecks and areas for improvement in their disaster recovery strategy, thereby strengthening the resilience and reliability of their data replication architecture.\nWhen designing a data replication strategy with AWS DMS, it is important to establish comprehensive contingency plans for scenarios involving service unavailability or data discrepancies. A thorough assessment of the organization’s RTO and RPO will drive the development of SOPs. This strategic planning not only promotes business continuity, but also provides valuable insights into the actual performance metrics of your replication system in failure scenarios.\nMonitoring and Alerting To maximize the effectiveness of AWS DMS, a strategic approach to monitoring and reporting is required. A robust monitoring framework is essential to maintain seamless replication operations and promote data integrity throughout the migration process.\nConfiguring appropriate alerts during the initial setup process will provide real-time visibility into replication tasks and enable rapid response to anomalies. These monitoring capabilities act as an early warning system, helping to maintain the health and efficiency of your database migration infrastructure.\nImplementing proactive monitoring and alerting improves operational reliability and provides insight into resource utilization and performance patterns. This systematic approach enables data-driven decision making and maintains optimal replication performance throughout the migration lifecycle.\nAWS DMS providesprovides the following monitoring features:\nAmazon CloudWatch metrics – These metrics are automatically populated by AWS DMS for users to get insights into resource utilization and related metrics for individual tasks and at the replication instance level. For a list of all the metrics available with AWS DMS, refer to AWS Database Migration Service Metrics.\nAWS DMS CloudWatch logs and Time Travel logs – AWS DMS creates error logs and populates them based on the logging level set by the user for each component. For more information, see View and manage AWS DMS task logs. When CloudWatch logs are enabled, AWS DMS enables context logging by default. DMS also includes Time Travel logging to assist in debugging replication tasks. For more information about Time Travel logging, see Setting Up Time Travel Tasks. For best practices when using Time Travel logs, see Troubleshooting Replication Tasks with Time Travel.\nTask and Table Status – AWS DMS provides near-real-time dashboards to report the status of tasks and tables. For a detailed list of task statuses, see Task Status. For table status, see Table Status During Task Execution.\nAWS CloudTrail logs– AWS DMS is integrated with AWS CloudTrail, a service that provides a log of actions performed by users, roles, or AWS services in AWS DMS. CloudTrail records all AWS DMS API calls as events, including calls from the AWS DMS console and from code calls to AWS DMS API operations. For more information on setting up CloudTrail, see the AWS CloudTrail User Guide.\nMonitoring dashboard – The enhanced monitoring dashboard provides comprehensive visibility into key metrics related to your monitoring tasks and replication instances; filtering, aggregating, and visualizing metrics for specific resources you want to monitor. The dashboard directly publishes existing CloudWatch metrics to monitor resource performance without changing the data point sampling time. For more information, please refer to Overview of the enhanced monitoring dashboard.\nWe recommend setting up CloudWatch alarms on key metrics and event logs to proactively identify potential issues before they escalate into system-wide disruptions. While this basic monitoring approach is just a starting point, it is important to expand your monitoring strategy based on your specific use case requirements and business goals.\nMetric Type Metric Name Remediation Server Metrics CPU Utilization You should set up alerts based on these metrics to alert operators that resource contention is impacting the performance of your DMS tasks. Based on the resource limits on your server, you may need to upgrade your DMS instance class if there is CPU and memory contention, or increase storage if storage is low or baseline IOPS are limited. For more information on how to choose the right replication instance, you can refer to the article Choosing the best size for a replication instance.. Free Memory Replication Task Metrics CDCLatencySource Based on SLA requirements, you can set alarm thresholds for latency metrics. In DMS, latency can be caused by many reasons. To troubleshoot and create SOPs, you can refer to Troubleshooting Latency in AWS Database Migration Service. (AWS DMS) CDCLatencyTarget DMS Event for Replication Instance ReplaceChange Configuration Each of these categories is a category with different events associated with it. You can set up notifications for specific events based on your needs. For a detailed list and description of these events, please refer to the AWS DMS Events and Event Notifications for SNS Notifications category.. Creation Delete Maintenance Low Storage Error DMS Events for Replication Tasks Error Each of these categories is a category with different events associated with it. You can set up notifications for specific events based on your needs. For a detailed list and description of these events, please refer to the [AWS DMS events and event messages for SNS notifications.](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Events.html#USER_Events.Messages]category. Status Change Creation Delete For a complete list of available metrics, you can refer to the AWS DMS User Guide for AWS Database Migration Service metrics.\nYou can use Amazon EventBridge to provide notifications when AWS DMS events occur or use Amazon Simple Notification Service (Amazon SNS) to create alerts for important events. For more information about EventBridge events in DMS, see EventBridge Event User Guide. For more information about using Amazon SNS with AWS DMS, see Amazon SNS Event User Guide.\nIn addition to setting up CloudWatch alarms, you can create custom alarms based on AWS DMS CloudWatch error logs using metric filters. For step-by-step instructions on how to implement these custom alarms, see the blog post titled \u0026quot;Send Alerts on Custom AWS DMS Errors from Amazon CloudWatch Logs\u0026quot;. This resource provides a comprehensive guide to enhancing your custom error monitoring capabilities.\nApply the principles of the AWS Well-Architected Framework The AWS Well-Architected Framework helps you understand the pros and cons of cloud-building decisions. The six pillars of the framework guide you through architectural best practices for designing and operating systems that are reliable, secure, efficient, cost-effective, and sustainable.\nUsing the AWS Well-Architected Tool, available for free in the AWS Management Console, you can review your workloads against these best practices by answering a set of questions for each pillar.\nFor more expert guidance and best practices for your cloud architecture—reference architecture implementations, diagrams, and white papers—see the AWS Architecture Center\nConclusion In this article, we presented a comprehensive framework for building resilient AWS DMS deployments. The effectiveness of this guide is directly related to the depth of your implementation and its adaptability to your specific environment. We strongly encourage organizations to carefully review each section of this guide and use it as a foundation to develop a customized migration strategy that fits your unique use case.\nBy carefully evaluating and incorporating these recommendations into your migration planning process, you can develop a comprehensive and reliable approach to using AWS DMS that will set you up for long-term success in your data migration strategies.\nFor additional support and resources, visit AWS DMS documentation and engage with AWS Support.\nAbout the Author Sanyam Jain is a Database Engineer on the AWS Database Migration Service (AWS DMS) team. He works closely with customers, providing technical guidance to migrate on-premises workloads to the AWS Cloud. He also plays a key role in enhancing the quality and functionality of AWS data migration products.\nSushant Deshmukh is a Senior Partner Solutions Architect, working with Global System Integrators. He is passionate about designing highly available, scalable, resilient, and secure architectures on AWS. He helps AWS Customers and Partners successfully migrate and modernize their applications to the AWS Cloud. In addition to hiswork, he enjoys traveling to explore new places and cuisines, playing volleyball, and spending quality time with family and friends.\nAlex Anto is a Data Migration Specialist Solutions Architect on the Amazon Database Migration Accelerator team at Amazon Web Services. He works as an Amazon DMA Consultant, assisting AWS customers in migrating their on-premises data to AWS Cloud database solutions.\n"},{"uri":"https://tranvanan5.github.io/internship-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"AWS KMS metrics in CloudWatch help you monitor and better understand KMS key usage By Norman Li and Haiyu Zhen | March 17, 2025 | in AWS Key Management Service, Best Practices, Intermediate (200), Security, Identity, \u0026amp; Compliance, Technical How-to |\nAWS Key Management Service (AWS KMS) is pleased to introduce key-level filtering for AWS KMS API usage in Amazon CloudWatch metrics, providing enhanced visibility to help customers improve operational efficiency and support security and compliance risk management.\nAWS KMS now publishes account-level AWS KMS API usage metrics with Amazon CloudWatch, allowing you to track and manage API usage. However, if you are using multiple KMS keys, it can be difficult to determine exactly which keys are using the most request rate quota or causing significant API costs. For example, if you have more than 10 active KMS keys in your account, prior to this feature launch, you would need to build a custom solution based on CloudTrail and Amazon Athena to identify which keys are driving the majority of your API usage and costs. With the new CloudWatch metrics available in the AWS/KMS namespace in CloudWatch, you can monitor, understand, and alarm on granular API usage at the individual KMS key level without building a costly custom solution.\nThis blog post explores several use cases that help you better leverage these newly introduced CloudWatch metrics to manage AWS KMS API usage and costs. Use cases include viewing and understanding your API usage at the key level, as well as creating CloudWatch alarms to detect unintended overage.\nOverview of New CloudWatch Metrics for KMS Keys With CloudWatch metrics for KMS keys, you can now do the following:\nView API usage for a specific KMS key, filtered by each API operation (e.g., Encrypt, Decrypt, or GenerateDataKey).\nView aggregate usage across encryption operations for a given KMS key.\nSet up alerts if a specific KMS key exceeds a specified threshold on a single API operation or a set of API operations.\nThis streamlined approach allows you to quickly monitor, understand, and troubleshoot KMS key API usage patterns without the multi-step process that was previously required. Let\u0026rsquo;s dive into how to use these key-level API usage metrics in two real-world examples.\nExample 1: How to Identify KMS Keys Consuming the Most API Quota or Contributing the Most API Charges When you exceed your AWS KMS API request quota, you can view your AWS KMS API usage in the Service Quotas console. However, you may still have difficulty identifying the KMS keys that are taking up the most request quota. When you receive an excess AWS KMS API charge, you can check the detailed usage in each AWS Region in Cost Explorer, but you cannot easily find the KMS keys with the highest API charges. This process becomes even more difficult when you manage a large number of KMS keys.\nWith CloudWatch key-level API usage metrics, you can use the advanced metrics query option to query CloudWatch Metrics Insights data in user-friendly SQL to identify the KMS keys that account for the majority of API usage quota or contribute the most API charges.\nWalkthrough To use Amazon CloudWatch Metrics Insights to identify the top 20 KMS keys with the most encryption API usage over the past 3 hours, complete the following steps:\nOpen the CloudWatch console.\nIn the navigation pane, select Metrics, then select All metrics.\nSelect the Multi source query tab.\nFor the data source, select CloudWatch Metrics Insights.\nYou can enter the following example query in Editor view:\nNote: In Builder view, the metric namespace, metric name, filter by, group by, sort by, and limit options are displayed. In Editor view, the same options as in Builder view are displayed in query format.\nSELECT SUM(SuccessfulRequest) FROM SCHEMA(\u0026quot;AWS/KMS\u0026quot;, KeyArn, Operation) GROUP BY KeyArn ORDER BY MAX () DESC LIMIT 20 Select Run in Editor view or Graph query in Builder view. Example 2: How to set up new granular alerts for unintended AWS KMS API usage Running big data workflows that read Amazon Simple Storage Service (Amazon S3) files encrypted with KMS keys is a common scenario for analytics, business reporting, or machine learning projects. Typically, these processes only read a limited number of files from S3 per call. However, misconfigured processes can accidentally read a large number of S3 files, leading to exceeding your AWS KMS API request rate quotas or incurring unexpected charges due to spiky AWS KMS API usage. Previously, to address this issue, you would have to build a custom alerting system by following these steps: 1) sending AWS CloudTrail events generated by AWS KMS to Amazon CloudWatch Logs; 2) writing queries in Amazon CloudWatch Logs Insights to track your API request usage; and 3) enabling anomaly detection on the corresponding CloudWatch Log Insights mathematical expression.\nNow, with CloudWatch API usage metrics at the key level, you can enable anomaly detection directly on these metrics to set up alerts for unusual AWS KMS API usage patterns. This provides a more streamlined and effective way to monitor and detect potentially risky workflows. Using CloudWatch metrics and this anomaly detection capability, you can proactively identify and address unintended increases in AWS KMS API usage, helping to avoid unexpected charges or service disruptions in your analytics, reporting, or machine learning workflows.\nWalkthrough Consider a scenario where you have an analytics workflow that runs regularly, using the Decrypt AWS KMS API on a KMS key to decrypt and read data from S3. You want to enable anomaly detection on the KMS key to trigger an alarm when a Decrypt call to a specific KMS key detects a clear trend or pattern. To do this, complete the following steps:\nOpen the CloudWatch console.\nIn the navigation pane, select Metrics, then select All metrics.\nSelect KMS, then select KeyArn, Operation.\nIn the search bar, enter the Amazon Resource Name (ARN) of the key, then select Search. Select the CloudWatch metric for which you want to enable anomaly detection.\nNavigate to Graphed metrics, and using the Statistic and Period drop-down lists, select the statistic and period you want to monitor. You can then enable anomaly detection by selecting the Pulse icon.\nYou can tune anomaly detection by setting the sensitivity to adjust bandwidth, if needed.\nConclusion This blog post highlighted the newly introduced key-level filtering capabilities for AWS KMS API usage in CloudWatch. We presented two real-world use cases to illustrate how you can use the new CloudWatch metrics. These use cases include improving operational visibility, setting up proactive alerts for anomalies in KMS API usage patterns, and being able to track granular key usage for compliance purposes.\nIf you have feedback on this article, please post it in the Comments section below. If you have questions about this article, please create a new topic in AWS Key Management Service re:Post.\nNorman LiNorman is the Director of Software Development for AWS KMS. In this role, Norman leads the development of visibility features, as well as internal scaling initiatives. Outside of work, Norman enjoys spending time in the beautiful mountains of the Pacific Northwest. Haiyu ZhenHaiyu is a Senior Software Development Engineer for AWS KMS. She specializes in building secure, large-scale distributed systems and is passionate about enhancing cloud-native application security without compromising performance. TAGS: AWS Key Management Service, AWS Key Management Service (KMS), Security Blog\n"},{"uri":"https://tranvanan5.github.io/internship-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #1 Workshop” Event Objectives An overview of the AWS AI/ML Services ecosystem, especially Amazon SageMaker.\nProviding knowledge about the end-to-end Machine Learning process from data preparation to model deployment.\nIn-depth presentation on Generative AI with Amazon Bedrock and prominent Foundation Models such as Claude, Llama and Titan.\nTechnical guidance on Prompt Engineering, RAG architecture and Bedrock Agents for multi-step workflows.\nSpeakers AWS AI/ML Specialist Team – AWS Vietnam AWS Solutions Architect AI/ML Community Leader in Vietnam Key Highlights AWS AI/ML Services Overview – Amazon SageMaker Amazon SageMaker is introduced as a comprehensive ML platform, fully supporting the lifecycle of Machine Learning.\nData preparation \u0026amp; labeling: Introducing SageMaker Data Wrangler, Ground Truth, to help standardize and label data effectively.\nModel training, tuning, deployment: Workflow for model training, optimization (hyperparameter tuning) and deployment on endpoints.\nIntegrated MLOPS: SageMaker Pipelines \u0026amp; Model Registry to help automate CI/CD for ML models.\nGenerative AI with Amazon Bedrock Foundation Models Overview\nCompare Claude, Llama, and Titan based on context length, reasoning quality, inference speed, fine-tuning, and suitability for each use case. Prompt Engineering Techniques\nApply Chain-of-Thought, few-shot prompting, and clear prompt structure to increase accuracy. RAG (Retrieval-Augmented Generation)\nExplain the RAG architecture and how to integrate a Knowledge Base in Amazon Bedrock to optimize answers based on business data. Bedrock Agents\nIntroduce the ability to create agents to execute multi-step workflows and integrate with external systems. Key Takeaways Knowledge About Traditional AI/ML (Machine Learning) Understand the standard process of an ML project: data collection, processing, training, optimization, deployment, MLOps.\nKnow how to use Amazon SageMaker as a standardized ML platform, suitable for businesses.\nKnowledge About Generative AI Ability to analyze and choose a Foundation Model suitable for the problem.\nEffective prompt writing skills using advanced techniques (CoT, few-shot).\nMaster the RAG architecture — an important foundation in building enterprise chatbots.\nUnderstand the operating mechanism of Bedrock Agents and how to automate workflows.\nDeployment Skills Understand how to combine Bedrock + SageMaker in an integrated ML system.\nUnderstand how to build a real GenAI chatbot with guardrails to ensure safety.\nApplying to Work Building a GenAI chatbot: Can apply Bedrock Agents and RAG knowledge to internal chatbot projects.\nOptimizing Prompt Engineering: Using CoT, few-shot into the current workflow to increase accuracy.\nUpgrading AI architecture: Better understanding how to combine MLOps and GenAI into real products.\nEvent Experience Participating in the workshop is a valuable experience:\nDeep knowledge, easy to understand\nLecturer presents clearly, from traditional ML knowledge to modern GenAI.\nReal demo for easy visualization\nSageMaker Studio and Bedrock Agent are demoed live, helping to understand how to build end-to-end.\nLearn many new techniques\nRAG, advanced prompt engineering, guardrails are all important skills in modern AI.\nOpportunity to exchange and ask questions\nThe workshop creates opportunities to interact with other AI/ML practitioners, expand networks and think about AI applications in practice.\nLessons learned Machine Learning and GenAI need to be combined to create more powerful applications.\nRAG architecture is important in enterprise AI applications.\nBedrock Agents can automate many complex workflows.\nSageMaker helps standardize and automate ML processes, especially in large organizations.\nSome event photos "},{"uri":"https://tranvanan5.github.io/internship-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “BUILDING AGENTIC AI” Event Objectives Optimizing Context with Amazon Bedrock Build AI agent automation with Amazon Bedrock through practical techniques and real-world use cases Speakers Nguyen Gia Hung - Head of Solutions Architect, AWS Kien Nguyen - Solutions Architect, AWS Viet Pham - Founder \u0026amp; CEO, Diagflow Kha Van - Community Leader, AWS Thang Ton - Co-Founder \u0026amp; COO, Cloud Thinker Henry Bui - Head of Engineering, Cloud Thinker Key Highlights Techniques to optimize costs and performance for AI agent systems Long product release times → Lost revenue/missed opportunities Poor performance → Lost productivity, costly costs Non-compliance with security regulations → Lost security, reputation Four “Quick Wins” techniques to optimize Prompt Caching This is the most important technique mentioned, which can reduce costs by 70-90% and increase processing speed: Context structure: The Context window is divided into 3 parts: (1) System \u0026amp; Tool Schema, (2) Conversation History, and (3) Objective Prompt. Common mistake: Most people only cache the System Prompt and Tool parts. However, the Conversation History part is the most expensive part (can account for 80-90% of the cost) and is often overlooked. The right strategy: Set a “checkpoint” so that the entire conversation history is also cached. Although the first run (cache write) costs 25% more, subsequent runs will save 90%\nContext Compaction Cloud Thinker has pointed out a smart summary technique to avoid losing cache The old way: Create a new agent to summarize the old conversation. This method loses all previous cache and often reduces quality (performance degradation). The new way (Cloudthinker technique): Keep the agent and conversation history intact (to take advantage of the existing cache), only change the “Objective Prompt” part to a new task “summarize this conversation”. This approach helps to take advantage of cache hits, reduce the summarization cost from e.g. $0.30 to e.g. $0.030 (~90% reduction) and improve the output quality\nTool Consolidation The problem with newer protocols like MCP (Model Context Protocol) is that putting too many tools (e.g. 50 tools) into the context will cause context flooding. Solution: Instead of putting the entire complex schema (data structure) into the prompt, use a simple “dictionary” and group the instructions. Just-in-time Instruction: The agent can use a special command (get instruction) to get detailed instructions on how to use the tool only when needed. This reduces the number of tokens that must be fed into the input continuously\nParallel Tool Calling Instead of running sequentially like the old ReAct model (2022), modern models allow multiple tools to run in parallel to save time. However, this feature is often not enabled by default; programmers need to add specific instructions (e.g., “maximize efficiency”) to force the model to run in parallel\nKey Takeaways Cost Management Strategy Input cost: accounts for the majority of the operating costs of AI agents running in a loop. That is, each round, the Agent must reload the entire context (Conversation history, system prompt, memory). So the longer the Agent loop runs, the more money it will burn -\u0026gt; The longer the History, the more input tokens each round will increase.\nSolution: use Prompt Caching and checkpointing to minimize this cost. Reduce costs by up to 80-90%.\nSmart “Summarization” technique Summarization: Keep the current agent and conversation history intact to take advantage of the existing cache, and maintain better context quality. With this technique, the summarization cost is reduced from $0.3 to $0.03 (reduced by ~90%) and the output quality is improved.\nTool Design: Avoid context flooding Tool Design: Problem: Too many tools are included (e.g., MCP with 50 tools) will cause context flooding. Solution: Create a special command for the Agent to call to get detailed instructions when needed, instead of stuffing the entire schema into the prompt. Make the context compact, reduce token usage and increase performance. Performance optimization: Force parallel running (Parallel Tool Calling) Maximize Efficiency: Add specific instructions to the prompt to force the model to implement tasks in parallel instead of sequentially. Event Experience Participating in the “Building Agentic AI” workshop was a very interesting experience, improving knowledge about Agentic AI. Some outstanding experiences:\nLearn from highly specialized speakers Speakers from AWS, Cloudthinker, Diaflow shared best practices in modern application design. Some event photos "},{"uri":"https://tranvanan5.github.io/internship-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Van An\nPhone Number: 0378374762\nEmail: antvse181691@fpt.edu.vn\nUniversity: FPT University of Campus HCM\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 28/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.1-introduction/","title":"Introduction","tags":[],"description":"","content":"Problem Statement Traditional chatbot systems struggle without the ability to access specific information from internal documents, leading to inaccurate or irrelevant responses. This workshop solves this problem by building an architecture capable of:\nAutomation: Automatically process and index PDF documents Content Inquiry: Receive queries and guide users to relevant content Document Retrieval: Answer complex questions with accurate source citations from documents Solution Architecture The system is designed following the RAG (Retrieval-Augmented Generation) model combined with AWS Serverless to ensure scalability:\nFrontend Interface: Users interact via React Web Application\nAmazon API Gateway receives requests from Frontend AWS Amplify hosting with CloudFront CDN Amazon Cognito handles authentication Request Handling:\nApplication Load Balancer routes traffic to EC2 FastAPI Backend processes REST API requests Amazon SQS (FIFO) ensures document processing order Backend Processing:\nChatHandler: Manages conversations, saves sessions to Amazon DynamoDB RAG Service: Orchestrates vector search and LLM generation Qdrant Vector Database: Self-hosted on EC2 for vector search AI \u0026amp; Data Layer:\nAmazon Bedrock: Uses Claude 3.5 Sonnet (LLM) and Cohere Embed Multilingual v3 (Embeddings) Amazon Textract: OCR and text extraction from PDF Amazon S3: Document storage Amazon DynamoDB: Metadata and chat history Admin Dashboard:\nReact-based interface hosted on AWS Amplify Upload and manage documents Monitor processing status View chat history Architecture Key Technologies In this workshop, you will work with the following key AWS services:\nAmazon Bedrock: The heart of AI, providing Foundation Models (Claude, Cohere) for language processing and embedding generation Amazon Textract: Build IDP pipeline to extract text from PDF documents Amazon EC2 \u0026amp; VPC: Compute and network infrastructure for backend services Amazon S3: Document and static asset storage Amazon DynamoDB: Store metadata, chat history, and document status Amazon Cognito: Authentication and user management AWS Amplify: Frontend application hosting with integrated CI/CD Amazon SQS: Message queue for document processing pipeline Qdrant (Self-hosted): Vector database for semantic search Terraform (IaC): Deploy entire infrastructure as code "},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read and understand FCJ regulations. - Learn more about Cloud, AI, DevOps. 09/08/2025 09/08/2025 3 - Explore and learn about what you will learn and AWS Infrastructure 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn AWS service management tools - Practice: + Create AWS account + Setup with Virtual MFA Service + Create Admin Group and Admin User + Support account authentication 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Cost management with AWS Budget -Practice: + Create Budget + Create Cost Budget + Create Usage Budget + Create RI Budget + Create Saving Plans Budget + Resource Cleanup 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Using AWS Support - Learn about AWS support packages -Practice:\n+ Access AWS Support\n+ Manage support requests 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understand what AWS is and its global structure.\nSuccessfully created and configured AWS Free Tier account.\nGet familiar with AWS Management Console and know how to find, access, and use services from the web interface.\nWork with AWS Budget to manage and optimize costs.\nLearn and use AWS Support.\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about VPC. + Subnets\n+ Route Table\n+ Internet Gateway\n+ NAT Gateway\n- VPC and Multi-VPC security features 09/15/2025 09/15/2025 https://cloudjourney.awsstudygroup.com/ 3 -Practice:\n+ Subnets\n+ Route Table\n+ Internet Gateway\n+ NAT Gateway + Create Security Group 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 -Practice: + Create EC2 server + Check connection + Create NAT Gateway + Use Reachability Analyzer + Create EC2 Instance connect Endpoint 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 Join Cloud Day Vietnam 2025 09/18/2025 09/18/2025 6 -Practice: + Set up Hydrid DNS with Route 53 Resolver + Create Keypair + Initialize CloudFormation Template + Security Group configuration 09/19/2025 09/19/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understand about Amazon VPC:\nWhat is VPC (Virtual Private Cloud). Subnet (Public \u0026amp; Private subnet) Route Table Internet Gateway NAT Gateway \u0026hellip; Know how to implement VPC environment deployment\nCreate VPC Create Subnet Create Internet Gateway Create Route Table Create Security Group Enable VPC Flow Logs Know how to deploy Amazon EC2 Instances\nJoin Cloud Day Vietnam 2025 to get closer to technology, Cloud, and update trends\n\u0026hellip;\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Learn more about AWS Networking:\nRoute 53 Resolver (Inbound/Outbound)\nVPC Peering\nTransit Gateway\nRouting Tables, SG, Network ACLs\nDeploy infrastructure using CloudFormation\nUnderstand AWS Compute \u0026amp; Storage services\nUnderstand Hybrid Cloud:\nAWS Backup\nStorage Gateway\nFile Shares \u0026amp; S3 Integration\nStrengthened practical skills:\nDeploy – configure – test – cleanup resources\nArchitectural thinking according to AWS Best Practices\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Amazon S3: - S3 Bucket and S3 Object - Main functions of S3 - Common applications of S3 - Benefits. 09/29/2025 09/29/2025 3 - Practice: - Create S3 Bucket and download data - Enable static website feature - Configure Block Public Access - Configure public object - Check Website - Block all public access to S3 - Configure Amazon CloudFront - Check Amazon CloudFront 09/30/2025 09/30/2025 https://cloudjourney.awsstudygroup.com/ 4 - Understanding Bucket Versioning - Key Benefits of Versioning - How Versioning Works - Versioning Status - Practice: + Enable versioning for bucket + Change the content of index.html file + Test versioning on S3 + Test versioning on Cloudfront + Move Object 10/01/2025 10/01/2025 https://cloudjourney.awsstudygroup.com/ 5 - Understanding Amazon S3 Cross-Region Replication (CRR): + Key Benefits of CRR + How CRR Works + AWS Well-Architected Framework - Practice: + Copy S3 Object to another region + Clean up resources 10/02/2025 10/02/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn: + Storage Services on AWS + Amazon Simple Storage Service ( S3 ) - Access Point - Storage Class + S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier + Snow Family - Storage Gateway - Backup 10/03/2025 10/03/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Overview of Amazon S3:\nS3 Bucket and S3 Object Main functions of S3 Common applications of S3 Understand the role of Amazon S3 in AWS architecture. Understand the mechanism of hosting static web on S3.\nKnow how to combine S3 + CloudFront to increase speed and security.\nUnderstand the mechanism of version control in S3.\nKnow how to restore data when files are overwritten or deleted.\nUnderstand the mechanism of automatic multi-region data synchronization.\nOverview of the entire AWS storage service group\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AWS Backup - Practice: + Create S3 bucket and Deploy infrastructure + Create Backup plan + Set up notifications + Check operation + Clean up resources 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice: Prepare VMware Workstation virtual machine + Export virtual machine from On-premise + Upload virtual machine to AWS + Import virtual machine to AWS + Deploy EC2 Instance from AMI + Set up ACL for S3 Bucket + Export EC2 Instance from AWS + Export virtual machine from AMI + Clean up resources 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Practice: create S3 bucket and create EC2 Instance using AMI Storage Gateway + Create S3 bucket and Create EC2 for Storage Gateway + Create Storage Gateway + Create File Shares + Connect File shares on On-premise machine + Clean up resources 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Translate Blog: + AWS Lambda introduces tiered pricing for Amazon CloudWatch Logs and additional logging targets + AWS DMS Implementation Guide: Build resilient database migrations through testing, monitoring, and SOP + AWS KMS metrics in CloudWatch help you track and better understand KMS key usage 10/09/2025 10/09/2025 https://docs.google.com/document/d/1s2iY6wbT6nwUDZbXcRchLlcCgJqimc1dLxjrLUX2sGA/edit?usp=sharing https://docs.google.com/document/d/1ZRVDcmVuPFlyDtdTLLiKmqOcVoFf8E2cah8cqb4SjkU/edit?usp=sharing https://docs.google.com/document/d/1GPZh5zNGKg0pY4hYCeKLoZHnl4lnhSgyP6o_zgTcQPI/edit?usp=sharing 6 - Practice: Deploy FSx on Windows Create a practice environment + Create an SSD Multi-AZ file system + Create a HDD Multi-AZ file system + Create a file share + Test performance + Monitor performance + Enable deduplication + Manage user sessions and open files + Enable user storage quotas + Enable Continuous Access sharing + Expand throughput + Expand storage capacity + Clean up resources 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Understand how AWS Backup automatically backs up data.\nUnderstand the process of migrating On-premise systems to AWS.\nKnow how to export/import virtual machines using VM Import/Export service.\nUnderstand Storage Gateway as a bridge to Hybrid Cloud.\nUnderstand the mechanism of file caching and cloud storage.\nUnderstand FSx used in enterprises for Windows file sharing.\nUnderstand how to scale performance and storage.\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Security Services on AWS + Share Responsibility Model + Amazon Identity and access management + Amazon Cognito + AWS Organization + AWS Identity Center + Amazon Key Management Service + AWS Security Hub + Hands-on and Additional research 10/13/2025 10/13/2025 3 - Learn Security Standards - Practice: + Enable Security Hub + Score each set of standards + Clean up resources - Practice: Optimizing EC2 costs with Lambda + Preparation steps: 1. Create VPC 2. Create Security group 3. Create EC2 4. Install Web-Hooks to Slack + Create Tag for Instance + Create Role for Lambda + Create Lambda Function 1. Create Lambda Function to perform Stop instances 2. Create Lambda Function to perform Start instances + Check results + Clean up resources 10/14/2025 10/14/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn AWS Resource Groups - Practice: + Create EC2 Instance with tag + Manage Tags in AWS Resources + Filter resources by tag + Use tag with CLI + Create a Resource Group + Clean up resources 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Create IAM User + Create IAM Policy + Create IAM Role + Transfer Role + Access EC2 console in AWS Region - Tokyo + Access EC2 console in AWS Region - North Virginia + Create EC2 instance without and with qualified Tags + Edit Resource Tag on EC2 Instance + Check policy + Clean up resources 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn what IAM Permission Boundary is - Practice: Limit user rights with IAM Permission Boundary + Create Limit Policy + Create IAM User Limit + Check IAM User Limit + Limit Resource Cleanup 10/17/2025 10/17/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Understand security services on AWS\nShare Responsibility Model\nAmazon Identity and access management\nAmazon Cognito\nAWS Organization\n\u0026hellip;\nUnderstand the concept of Authentication/Authorization through Cognito.\nUnderstand IAM, Security Hub, Organizations.\nUse tagging and resource groups to manage resources.\nOptimize costs by automation stop/start EC2.\nKnow how to use IAM Permission Boundary to limit User permissions.\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about IAM - Practice: + Create IAM Group + Create IAM User + Check permissions + Create Admin IAM Role + Configure Switch role + Limit access by IP + Limit by time + Clean up resources 10/20/2025 10/20/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice: Grant permissions to applications with IAM Role + Create EC2 Instance + Create S3 bucket + Create IAM user and access key + Use access key + Create IAM role + Use IAM role + Clean up resources - Learn about Database Concepts review 10/21/2025 10/21/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about: + Amazon RDS \u0026amp; Amazon Aurora + Redshift - Elasticache 10/22/2025 10/22/2025 https://cloudjourney.awsstudygroup.com/ 5 - Midterm Review 10/23/2025 10/23/2025 https://cloudjourney.awsstudygroup.com/ 6 - Midterm Review 10/24/2025 10/24/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Understand the knowledge of IAM.\nUse IAM Role Create EC2 instance Create S3 bucket to store data Create IAM User + Access Key, used to test manual permissions Create IAM Role and assign to EC2 Compare IAM User vs IAM Role in granting permissions Clean up all resources Get familiar with AWS Management Console and know how to find, access, and use services from the web interface.\nLearn about important AWS database services: Amazon RDS – traditional, fully managed database Amazon Aurora – high-performance database compatible with MySQL/PostgreSQL Amazon Redshift – data warehouse for big analytics Amazon ElastiCache – in-memory caching (Redis/Memcached) Review knowledge to prepare for midterms\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Midterm Review 10/27/2025 10/27/2025 3 - Midterm Review 10/28/2025 10/28/2025 4 - Midterm Review 10/29/2025 10/29/2025 5 - Midterm Review 10/30/2025 08/30/2025 6 - Midterm Test 10/31/2025 10/31/2025 Week 8 Achievements: Review knowledge to prepare for midterm "},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Learn about Amazon Bedrock 11/03/2025 11/03/2025 https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html 3 - Learn about RAG 11/04/2025 11/04/2025 https://aws.amazon.com/what-is/retrieval-augmented-generation/ 4 - Learn about the Knowledge Base 11/05/2025 11/05/2025 https://www.atlassian.com/itsm/knowledge-management/what-is-a-knowledge-base 5 - Learn and supplement knowledge about React, Javascript 11/06/2025 11/06/2025 6 - Learn and supplement knowledge of React, Javascript 11/07/2025 11/07/2025 ### Week 9 Achievements: Understand Amazon Bedrock, RAG, Knowledge Base\nKnow the basic syntax and necessary knowledge of React, Javascript\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.2-preparation/","title":"Preparation","tags":[],"description":"","content":"Prerequisites Requirements needed to complete this workshop:\nAWS Client Machine: Configured with access to necessary AWS services Development Environment: Windows, macOS, or Linux with basic development tools Basic Knowledge: Understanding of AWS, Python, JavaScript, and Docker GitHub Account: To clone source code and track changes AWS Budget: Approximately $65/month for resources (EC2, Bedrock, NAT Gateway) Tool Installation 1. AWS CLI AWS Command Line Interface (AWS CLI) is a tool to interact with AWS services.\nWindows:\n# Download and install MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi macOS:\nbrew install awscli Linux:\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Verify installation:\naws --version # aws-cli/2.x.x Python/3.x.x 2. Terraform Terraform is an Infrastructure as Code tool to provision AWS resources.\nWindows:\nchoco install terraform macOS:\nbrew tap hashicorp/tap brew install hashicorp/tap/terraform Linux:\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update \u0026amp;\u0026amp; sudo apt install terraform Verify:\nterraform --version 3. Docker Docker to run Qdrant vector database locally and on EC2.\nWindows/macOS: Download Docker Desktop\nLinux:\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -aG docker $USER Verify:\ndocker --version docker run hello-world 4. Node.js (\u0026gt;= 18) Node.js for frontend development with React + Vite.\nUsing nvm (recommended):\n# Install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash # Install Node.js 18 nvm install 18 nvm use 18 Verify:\nnode --version # v18.x.x npm --version # 9.x.x 5. Python (\u0026gt;= 3.11) Python for backend FastAPI application.\nWindows: Download from python.org (select \u0026ldquo;Add to PATH\u0026rdquo;)\nmacOS:\nbrew install python@3.11 Linux:\nsudo apt update sudo apt install python3.11 python3.11-venv python3-pip Verify:\npython --version # Python 3.11.x pip --version 6. Git Git for version control.\nWindows: Download from git-scm.com\nmacOS:\nbrew install git Linux:\nsudo apt install git Verify:\ngit --version Clone Repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Configure AWS Credentials Create IAM User Login to AWS Console Navigate to IAM → Users → Create user User name: arc-workshop-user Attach policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Create access key → Download credentials Configure AWS CLI aws configure Enter information:\nAWS Access Key ID: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name: ap-southeast-1 Default output format: json Verify:\naws sts get-caller-identity Activate Amazon Bedrock Models Request Model Access AWS Console → Amazon Bedrock → Model access Click Manage model access Select models: Anthropic - Claude 3.5 Sonnet (anthropic.claude-3-5-sonnet-20241022-v2:0) Cohere - Embed Multilingual v3 (cohere.embed-multilingual-v3) Click Request model access → Accept Terms → Submit Verify Access # Test Claude aws bedrock get-foundation-model \\ --model-identifier anthropic.claude-3-5-sonnet-20241022-v2:0 \\ --region ap-southeast-1 # Test Cohere aws bedrock get-foundation-model \\ --model-identifier cohere.embed-multilingual-v3 \\ --region ap-southeast-1 Expected: Status Access granted\nPrepare Sample Documents Project comes with sample PDFs in samples/:\nls samples/ # data-structures-sample.pdf # test-sample.pdf Document Requirements Limit Value Format PDF (text-based or scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Checklist Before proceeding, ensure:\nAWS CLI installed and configured Terraform installed Docker installed and running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created with proper permissions Bedrock models approved (Claude + Cohere) Sample documents ready "},{"uri":"https://tranvanan5.github.io/internship-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Academic Research Chatbot AWS RAG-based solutions support smart learning and academic research 1. Executive Summary Academic Research Chatbot is an AI assistant that supports academic research, helping students and lecturers to look up, summarize and analyze scientific documents (PDFs, articles) through natural conversations with accurate source citations.\nSolution Highlights:\nCore Technology: Combining IDP (Amazon Textract) to process documents (including scans) and RAG ​​(Amazon Bedrock - Claude 3.5 Sonnet) to generate intelligent answers.\nOptimized Architecture: Hybrid model using 1 EC2 t3.small combined with Serverless services (Amplify, Cognito, S3, DynamoDB) to balance performance and cost.\nFeasibility: Serving ~50 internal users with operating costs of ~60 USD/month, fast deployment time (20 days) and making the most of AWS Free Tier.\n2. Problem Statement What’s the Problem? Students and researchers have to work with a large number of academic documents (conference papers, journals, theses, technical reports). Many documents are old PDF scans (before 2000), without text layers, making searching for content, figures, and tables very time-consuming. Public AI tools (ChatGPT, Perplexity, NotebookLM, etc.) are not directly connected to the internal document repository of the school/faculty, making it difficult to ensure security and access rights by subject or research group. The current infrastructure does not have a unified access point to:\nManage research documents by subject/topic. Allow researchers to ask questions directly on their own papers. Ensure answers have clear citations (paper, page, table, section). Consequences: researchers have to read manually, take notes by hand, copy figures from many papers; lecturers have difficulty quickly synthesizing information when preparing lectures or topics; Academic data is distributed across many personal machines, making it difficult to standardize and reuse. The Solution Dev/Admin loads research document repository: Upload PDF to Amazon S3, metadata is stored in Amazon DynamoDB. An EC2 worker consumes Amazon SQS queue, calls Amazon Textract to OCR, extracts text, tables, forms, including scanned documents. Worker normalizes/chunks content, sends to Amazon Bedrock Titan Text Embeddings v2 to generate embedding, and indexes into Qdrant on EC2. Researchers ask questions via web interface (Amplify + CloudFront): Question is embedded, query Qdrant to get the most relevant paragraphs (Retrieval). These paragraphs are passed to Claude 3.5 Sonnet on Amazon Bedrock to generate answers with correct citation (paper, page, section, table) and explanation in academic context. All access is protected by Amazon Cognito (researcher vs admin authorization), logs \u0026amp; metrics are monitored via Amazon CloudWatch + SNS (alerts for worker errors, queue backlog, high EC2 CPU). Benefits and Return on Investment Reduce 40–60% of the time researchers have to spend to find data, F1-score, p-value, sample size, experimental equipment or method description from many different papers.\nReduce errors when citing due to forgetting pages/tables, because the chatbot always returns the source and location.\nInternal knowledge management:\nResearch documents are centralized in an S3 + DynamoDB repository, easy to backup, decentralize, and expand.\nCan be reused for many different courses, topics and labs without having to build a new system.\nLow infrastructure cost \u0026amp; easy to control:\nThe hybrid model of 1 EC2 + managed AI services helps keep operating costs for 50 internal users at around \u0026lt;50 USD/month, mainly paying for EC2, 2–3 VPC endpoint interfaces and Bedrock/Textract usage.\nThe system is designed to be deployed in about 20 days by a team of 4 people, suitable for research/internship projects but still has product architecture quality.\nLong-term value:\nCreate a foundation to later integrate a learning behavior analysis dashboard, a recommendation paper module, or expand to a multilingual and multidisciplinary learning assistant. 3. Solution Architecture Academic Research Chatbot applies the AWS Hybrid RAG Architecture model with IDP (Intelligent Document Processing), combining a single EC2 (FastAPI + Qdrant + Worker) with managed AI services (Textract, Bedrock) to optimize costs while ensuring performance for about 50 internal users.\nData processing and conversation flow\nAWS services used\nFrontend: Route 53, CloudFront, Amplify (DNS, CDN, Host React App).\nAuth: Cognito (Authentication \u0026amp; authorization of researcher/admin).\nCompute: EC2 t3.small (FastAPI + Qdrant + Worker).\nAI/ML: Bedrock (Claude 3.5 Sonnet, Titan Embeddings v2).\nIDP: Textract (OCR for PDF scans).\nStorage: S3, DynamoDB (Original PDF file + Metadata/Status).\nQueue: SQS (Document processing queue).\nNetwork: VPC, ALB, VPC Endpoints (Security, routing, AWS Services connection).\nMonitoring: CloudWatch, SNS (Logs, Metrics, Alerts).\nCI/CD: CodePipeline, CodeBuild (Auto deploy backend).\nComponent design\nUsers: Researchers: Q\u0026amp;A, academic content lookup. Dev/Admin: upload, manage and re-index documents. Document processing (IDP): PDF is uploaded to S3 by Dev/Admin. Worker on EC2 calls Textract to OCR and extract text/tables. Indexing \u0026amp; Vector DB: Worker normalizes, chunks content. Call Bedrock Titan Embeddings v2 to create embedding. Save embedding + metadata to Qdrant on EC2. AI Conversation (RAG): FastAPI embeds question, queries Qdrant to get top-k related segments. Send context + question to Claude 3.5 Sonnet (Bedrock) to generate answers with citations. User Management: Cognito authenticates and authorizes researcher / admin. Storage \u0026amp; State: DynamoDB stores document metadata (doc_id, status, owner, …) and (optionally) chat history. 4. Technical Implementation Implementation Phases The project consists of 2 parts — setting up the edge weather station and building the weather platform — each part goes through 4 phases:\nResearch \u0026amp; finalize the architecture: Review requirements (50 researchers, 1 EC2, IDP + RAG). Finalize the architecture of VPC, EC2 (FastAPI + Qdrant + Worker), Amplify, Cognito, S3, SQS, DynamoDB, Textract, Bedrock. POC \u0026amp; test connectivity: Create EC2, VPC endpoints, test calling Textract, Titan Embeddings, Claude 3.5 Sonnet. Run simple Qdrant on EC2, test insert/search vector. Create FastAPI skeleton + a minimalist Chat UI screen on Amplify. Complete main features: Build /api/chat (FastAPI) + RAG pipeline: embed query → Qdrant → Claude + citation. Build /api/admin/: upload PDF, save S3 + DynamoDB, put message into SQS. Write Worker on EC2: SQS → Textract → normalize/chunk → Titan → Qdrant → update DynamoDB. Complete Chat UI and Admin UI (upload + view document status). Test, optimize, deploy internal demo: End-to-end test with a set of ~50–100 papers. Add CloudWatch Logs/Alarms, SNS notify when error or queue backlog. Adjust EC2 configuration, Qdrant, batch size to optimize time and cost. Prepare user manual and demo for group of 50 researchers. Technical requirements\nFrontend \u0026amp; Auth:\nReact/Next.js hosted on AWS Amplify, CDN CloudFront, DNS Route 53. Amazon Cognito for identity and authorization management (Researcher/Admin). Backend \u0026amp; Compute:\nEC2 t3.small (Private Subnet) running All-in-one: FastAPI, Qdrant Vector DB and Worker. Asynchronous processing: Worker reads SQS, triggers Textract and Bedrock to index data. IDP \u0026amp; RAG:\nStorage: S3 (Original file), DynamoDB (Metadata \u0026amp; State). AI Core: Textract (OCR scanned documents), Bedrock Titan (Embedding), Claude 3.5 Sonnet (Question Answering). Network \u0026amp; Observability:\nNetwork: VPC Private Subnet, VPC Endpoints for secure connection to AWS Services. Monitoring: CloudWatch Logs/Metrics + SNS incident alert (high CPU, Worker error). 5. Timeline \u0026amp; Milestones *The project was implemented in about 6 weeks with specific stages:\nWeek 1-2 (Days 1-10): Research \u0026amp; Design Design detailed architecture, determine scope, services used. Plan to optimize operating and deployment costs. Week 3 (Days 11-15): Set up AWS infrastructure Configure VPC, Subnets, Security Groups, IAM Roles. Deploy EC2 t3.small, S3 bucket, DynamoDB tables. Set up VPC Endpoints (Gateway + Interface). Week 4 (Days 16-20): Backend APIs \u0026amp; IDP Pipeline Build FastAPI endpoints (/api/chat, /api/admin/upload). Integrate IDP pipeline: SQS → Worker → Textract → Embeddings → Qdrant. Bedrock Connectivity (Titan Embeddings + Claude 3.5 Sonnet). Week 5 (Days 21-25): Testing \u0026amp; Error Handling End-to-end testing with ~50-100 papers. Handling edge cases, retry logic, error handling. Optimizing chunking strategy and retrieval accuracy. Week 6 (Days 26-30): Deployment \u0026amp; Documentation Finalizing UI/UX for Admin and Researcher. Setting up CloudWatch Alarms + SNS notifications. Preparing documentation and demo for a group of 50 researchers. 6. Budget Estimation Infrastructure costs (estimated by month)\nCompute \u0026amp; Storage: EC2 t3.small: $10.08 (720h). EBS gp3: $2.40 (30GB). Network: NAT Gateway: $21.60. VPC Interface Endpoints: $14.60 (2 endpoints for Textract, Bedrock). VPC Gateway Endpoints: FREE (S3, DynamoDB). AI \u0026amp; Operations: Bedrock Claude 3.5 Sonnet: $25.00 (50 users). Bedrock Titan Embeddings: $0.75 (750 papers). CloudWatch + Data Transfer: $1.90. Free Tier (first 12 months)\nWeb \u0026amp; Auth: S3, CloudFront, Cognito, Amplify (FREE).\nServerless: DynamoDB, SQS, SNS (Always FREE).\nIDP: Textract AnalyzeDocument (100 pages/month for the first 3 months).\nTotal: ~$60-76/month (depending on Bedrock usage).\n7. Risk Assessment Risk Matrix\nHallucination (fabricated AI): High impact, medium probability. Budget Overrun (AI Services): Medium impact, medium probability. Infrastructure Incident (EC2/Qdrant): High impact, low probability. Mitigation Strategy\nAI Quality: Required citation, limited input context from Qdrant. Cost: Set up AWS Budgets/Alarms, control the number of ingest documents. Infrastructure \u0026amp; Security: Regular EBS backup, data encryption (S3/DynamoDB), strict authorization via Cognito/IAM. Contingency Plan\nSystem Incident: Restore from Snapshot, pause ingestion (buffer via SQS). Cost overrun: Temporarily lock new upload feature, limit daily query quota. 8. Expected Outcomes Technical improvements:\nConverting discrete document repositories (PDF/Scan) into digital knowledge that can be automatically queried and cited. Significantly reducing manual search time thanks to RAG + IDP technology. Long-term value Building a digital research platform for 50+ researchers, easy to scale. Creating the premise for developing advanced features: Suggesting documents, analyzing research trends and supporting literature reviews. "},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Amazon S3 + Access Point + Storage Class - Learn about AWS Security Hub 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Amazon DynamoDB - Find and review AWS Backup 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Practice: + Install Bedrock + Claude 3.5 Sonnet + Prompt writing techniques\n+ Handling rate limits 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn the concepts: + Amazon Simple Queue Service (Amazon SQS) + Amazon Simple Notification Service (SNS)\n+ AWS Organizations + Amazon Macie + AWS Direct Connect 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn how to set up amplify Amazon Cognito Review 11/14/2025 11/14/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Master knowledge of Amazon S3\nHow S3 – object storage works. Types of Storage Classes and when to use them (Standard, IA, Glacier…). S3 Access Point to manage access by application. Know Security Hub to aggregate security alerts from multiple AWS services.\nUnderstand standards such as CIS, PCI DSS in Security Hub.\nUnderstand NoSQL on AWS.\nServerless architecture, partition key, sort key, RCU/WCU.\nBackup plan concept.\nHow to automate backups.\nResources to support backups.\nLearn services to support communication \u0026amp; security\nAmazon SQS – message queue. Amazon SNS – pub/sub service. AWS Organizations – multi-account management. Amazon Macie – protect sensitive data. AWS Direct Connect – dedicated network connection. Learn how to set up AWS Amplify to build frontend + backend.\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Join AWS Mastery Series #2 11/17/2025 11/17/2025 3 - Learn more about Infrastructure as Code (IaC) Learn about CI/CD 11/18/2025 11/18/2025 4 - Practice: setup Amplify with React + React app created with Vite + Amplify CLI configured + Basic routing configured 11/19/2025 11/19/2025 5 - Brainstorm and build a Template for the project 11/20/2025 11/20/2025 6 - Collect and prepare Documents for the project 11/21/2025 11/21/2025 Week 11 Achievements: Complete project setup with Amplify + React\nImprove AWS \u0026amp; DevOps knowledge (IaC, CI/CD)\nPrepare full documentation to start building the project\n"},{"uri":"https://tranvanan5.github.io/internship-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Build the interface for Login Page 11/24/2025 11/24/2025 3 - Build the interface for Chat Page 11/25/2025 11/25/2025 4 - Build the interface for Admin Page 11/26/2025 11/16/2025 5 - Edit the interface for the project 11/27/2025 11/27/2025 6 - Create a template for ppt and prepare for the presentation 11/28/2025 11/28/2025 Week 12 Achievements: Build the interface for the final project "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.3-bedrock-models/","title":"Activate Bedrock Models","tags":[],"description":"","content":"Before deploying the solution, you need to activate the necessary Amazon Bedrock models in your AWS account.\nSteps to Activate Models Search for Amazon Bedrock in AWS Console Access Model catalog from the left navigation menu Select the corresponding model names: Anthropic Claude 3.5 Sonnet Anthropic Claude 3 Sonnet Anthropic Claude 3 Haiku Cohere - Embed Multilingual v3 Select \u0026ldquo;Open in playground\u0026rdquo; and send a test message to activate each model Note: Make sure you activate all four models in the ap-southeast-1 (Singapore) region as the solution is deployed in this region.\n"},{"uri":"https://tranvanan5.github.io/internship-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - AWS Lambda Introduces Tiered Pricing for Amazon CloudWatch Logs and Additional Logging Targets This blog introduces AWS Lambda\u0026rsquo;s new logging updates, which help serverless applications optimize costs and gain more flexibility when managing logs. The post explains that Lambda now uses tiered pricing for CloudWatch Logs, which dramatically reduces logging costs as log volumes increase. In addition to CloudWatch Logs, Lambda also supports Amazon S3 and Amazon Data Firehose as new logging targets, providing flexibility in storage, analysis, and integration with other observability platforms. The blog also covers enhancements such as advanced logging controls, CloudWatch Logs Infrequent Access, and Live Tail for increased observability. Finally, the post explains how to configure log destinations and outlines best practices for optimizing logging costs in serverless environments.\nBlog 2 - AWS DMS Implementation Guide: Building Resilient Database Migrations Through Testing, Monitoring, and SOPs This blog is an AWS Database Migration Service (AWS DMS) implementation guide, focusing on how to build a resilient, secure, and repeatable database migration process. The content emphasizes testing before migration, monitoring during execution, and building SOPs (Standard Operating Procedures) to ensure smooth data migration, reduced risk, and easy maintenance in the AWS environment.\nBlog 3 - AWS KMS Metrics in CloudWatch to Help You Track and Understand KMS Key Usage This blog is about the new AWS KMS feature that allows you to track API usage at the per-key level via Amazon CloudWatch, making it easy for users to see which keys are being used the most, which keys are costing money, or which keys are at risk of exceeding quota. Previously, this was quite complicated and required building custom solutions using CloudTrail and Athena, but now CloudWatch provides detailed metrics for each key, making monitoring simpler and more intuitive. The article also illustrates how to use these new metrics to query and find the KMS keys that consume the most APIs, as well as set up automatic alerts based on anomaly detection to promptly handle sudden request spikes due to configuration errors or operational processes. As a result, users can improve operational efficiency, reduce security risks, and optimize costs when using AWS KMS.\n"},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.4-aws-cli/","title":"Configure AWS CLI","tags":[],"description":"","content":"To deploy and manage the solution, you need to configure the AWS Command Line Interface (AWS CLI) with your authentication information.\nSteps Step 1: Check if AWS CLI is installed aws --version If not installed:\n# Download and install MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Step 2: Create IAM User (if not already exists) Login to AWS Console Search \u0026ldquo;IAM\u0026rdquo; → Click IAM Left sidebar → Users → Create user User name: arc-workshop-user Click Next Attach policies directly, select policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Click Create user Step 3: Create Access Key Go to the created user → Tab Security credentials Scroll down Access keys → Click Create access key Select Command Line Interface (CLI) Tick \u0026ldquo;I understand\u0026hellip;\u0026rdquo; → Next Description: ARC Workshop CLI Click Create access key ⚠️ IMPORTANT: Copy or download .csv file\nAccess key ID: AKIAXXXXXXXXXXXXXXXX Secret access key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Step 4: Configure AWS CLI Open PowerShell and run:\naws configure Enter information:\nAWS Access Key ID [None]: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name [None]: ap-southeast-1 Default output format [None]: json Step 5: Verify Configuration Check identity:\naws sts get-caller-identity Expected output:\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/arc-workshop-user\u0026#34; } "},{"uri":"https://tranvanan5.github.io/internship-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: AWS Cloud Mastery Series #1 workshop\n**Date \u0026amp; Time: 11/15/2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: BUILDING AGENTIC AI\nDate \u0026amp; Time: 12/052025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.5-data-preparation/","title":"Data Preparation","tags":[],"description":"","content":"Data Preparation Before creating AWS resources, you need to download sample data to test the system.\nStep 1: Download Sample Data Access ARC Sample Data Download the data to your computer Extract the file, which will create a folder named DATA Document Requirements Limit Value Format PDF (text-based or scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Prepare AWS Resources Step 2: Create S3 Bucket S3 Bucket is used to store uploaded PDF documents.\nSearch for S3 in AWS Console Click Create bucket Configure bucket: Bucket name: arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; (replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your AWS Account ID) AWS Region: Asia Pacific (Singapore) ap-southeast-1 Keep other settings as default Click Create bucket 💡 Tip: To get your AWS Account ID, run:\naws sts get-caller-identity --query Account --output text Or create using CLI:\naws s3 mb s3://arc-documents-$(aws sts get-caller-identity --query Account --output text) --region ap-southeast-1 Step 3: Create DynamoDB Table DynamoDB Table is used to store document metadata.\nSearch for DynamoDB in AWS Console Click Create table Configure table: Table name: arc-documents Partition key: doc_id (String) Sort key: sk (String) Table settings: Default settings Click Create table Or create using CLI:\naws dynamodb create-table \\ --table-name arc-documents \\ --attribute-definitions \\ AttributeName=doc_id,AttributeType=S \\ AttributeName=sk,AttributeType=S \\ --key-schema \\ AttributeName=doc_id,KeyType=HASH \\ AttributeName=sk,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 Step 4: Create SQS Queue SQS Queue is used for IDP pipeline to process documents.\nSearch for SQS in AWS Console Click Create queue Configure queue: Type: Standard Name: arc-document-queue Keep other settings as default Click Create queue Or create using CLI:\naws sqs create-queue --queue-name arc-document-queue --region ap-southeast-1 Step 5: Verify Resources Check that all resources have been created:\n# S3 Bucket aws s3 ls | grep arc-documents # DynamoDB Table aws dynamodb describe-table --table-name arc-documents --region ap-southeast-1 --query \u0026#34;Table.TableName\u0026#34; # SQS Queue aws sqs get-queue-url --queue-name arc-document-queue --region ap-southeast-1 Step 6: Upload Data to S3 Upload PDF files from the DATA folder downloaded in Step 1:\n# Upload all files from DATA folder aws s3 cp DATA/ s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ --recursive # Or upload individual file aws s3 cp DATA/sample-document.pdf s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ 💡 Tip: Replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your AWS Account ID\nVerify upload:\naws s3 ls s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ Checklist Before proceeding, make sure:\nAWS CLI installed and configured Terraform installed Docker installed and running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created with sufficient permissions Bedrock models approved (Claude + Cohere) S3 Bucket created DynamoDB Table created SQS Queue created Sample documents uploaded to S3 "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AI-POWERED ACADEMIC RESEARCH CHATBOT ON AWS Overview In this workshop, we will build ARC (Academic Research Chatbot) - an intelligent chatbot system running on the AWS Serverless platform. This solution leverages Generative AI and RAG (Retrieval-Augmented Generation) to support academic research, document queries, and answer questions flexibly.\nInstead of answering questions based on fixed scripts (rule-based), the system uses the Claude 3.5 Sonnet model to understand natural language, query data from the vector database, and respond to users accurately.\nWorkshop Objectives After completing this workshop, you will:\nUnderstand RAG architecture and how to apply it in practice Deploy a complete chatbot system on AWS Use Amazon Bedrock (Claude 3.5 Sonnet + Cohere Embed) Build an IDP pipeline with Amazon Textract Implement vector search with Qdrant Deploy infrastructure with Terraform Integrate authentication with Amazon Cognito Content Introduction Preparation Activate Bedrock Models Configure AWS CLI Data Preparation Deploy Infrastructure Setup Backend API Setup IDP Pipeline Setup Frontend Using Chatbot Using Admin Dashboard Clean up Resources "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.6-infrastructure/","title":"Deploy Infrastructure","tags":[],"description":"","content":"In this section, we will clone the repository and deploy the entire AWS infrastructure for the ARC Chatbot system.\nStep 1: Clone Repository Clone the repository from GitHub:\ngit clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Step 2: Build Dashboard Before deploying the application, we need to build the frontend dashboard.\nNavigate to the frontend folder cd frontend Install dependencies Run the following command to install the necessary libraries:\nnpm install Build Dashboard After installation is complete, run the build command:\nnpm run build After the process is complete, a dist folder will be created. Verify the index.html file and the assets folder:\nls dist/ # index.html assets/ Return to the project root directory cd .. Step 3: Deploy Terraform Application Deploy the Terraform application. The process will take approximately 20-30 minutes to deploy all resources.\ncd terraform terraform init terraform apply --auto-approve ⚠️ Note: If you encounter an error at this step, make sure Docker is running on your computer.\n💡 Info: Replace \u0026lt;account_id\u0026gt; with your actual AWS Account ID.\nStep 4: Verify Deployment After completing all the steps above, your environment has been successfully deployed.\nYou can verify the deployment by checking:\nAWS Console: Check the resources that have been created (EC2, S3, Cognito, DynamoDB, etc.) Terraform State: Run terraform state list to see the list of resources S3 Buckets: Buckets for documents and frontend have been created EC2 Instance: Instance for backend has been launched Check Outputs terraform output Important outputs:\nOutput Description api_endpoint Backend API URL cognito_user_pool_id Cognito User Pool ID cognito_client_id Cognito App Client ID s3_bucket_name S3 bucket for documents cloudfront_url Frontend URL Next Steps Now you can proceed to:\nSet up Backend Set up IDP Pipeline Set up Frontend "},{"uri":"https://tranvanan5.github.io/internship-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from 09/08/2025 to 11/28/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in Internship at First Cloud Journey, through which I improved my skills in self-study, time management, office work, teamwork,\u0026hellip;.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ☐ ✅ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ☐ ✅ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.7-backend/","title":"Set up Backend API","tags":[],"description":"","content":"Set up Backend API In this section, you will configure the Backend API (FastAPI) and Qdrant vector database on EC2.\nBackend Architecture Internet → ALB (:80) → EC2 Private Subnet ├── FastAPI Container (:8000) ├── Qdrant Container (:6333) └── SQS Worker (background) 💡 Note: EC2 is located in a Private Subnet with no Public IP. Access via SSM Session Manager.\nStep 1: Access EC2 via Session Manager The EC2 instance was created in a Private Subnet with no Public IP. Use AWS Systems Manager Session Manager to access it.\nMethod 1: AWS Console Open AWS Console → EC2 → Instances Select instance arc-dev-app-server Click Connect → Session Manager → Connect Method 2: AWS CLI # Get Instance ID from Terraform output INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Connect via SSM aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 ⚠️ Requirement: Install Session Manager Plugin\nStep 2: Check Running Services EC2 has been automatically set up via user_data script when Terraform created the instance. Verify the services:\n# Switch to ec2-user sudo su - ec2-user # Check Docker containers docker ps You should see 2 containers running:\napp-fastapi-1 - FastAPI server (port 8000) app-qdrant-1 - Qdrant vector database (port 6333) # Check Qdrant curl http://localhost:6333/collections # Check FastAPI curl http://localhost:8000/health Step 3: Deploy Backend Code Backend code will be deployed via CI/CD Pipeline (CodePipeline → CodeBuild → CodeDeploy). However, for quick testing, you can deploy manually:\ncd /home/ec2-user # Clone repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project/backend # Stop old containers cd /home/ec2-user/app docker-compose down # Copy backend code cp -r /home/ec2-user/ARC-project/backend/* /home/ec2-user/app/ # Start with new code docker-compose up -d --build Step 4: Configure Environment Variables Create a .env file with values from Terraform outputs:\ncd /home/ec2-user/app # Get values from Terraform outputs (run on local machine) # terraform -chdir=terraform output cat \u0026gt; .env \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # AWS Configuration AWS_REGION=ap-southeast-1 # S3 S3_BUCKET_NAME=arc-documents-\u0026lt;account-id\u0026gt; # DynamoDB DYNAMODB_TABLE_NAME=arc-dev-documents # SQS SQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account-id\u0026gt;/arc-dev-document-queue # Qdrant (local container) QDRANT_HOST=qdrant QDRANT_PORT=6333 # Cognito COGNITO_USER_POOL_ID=ap-southeast-1_xxxxx COGNITO_CLIENT_ID=xxxxx # Bedrock BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0 EMBEDDING_MODEL_ID=amazon.titan-embed-text-v2:0 EOF 💡 Tip: Replace \u0026lt;account-id\u0026gt; and xxxxx values with actual outputs from Terraform.\nStep 5: Restart Services # Restart to load new .env docker-compose down docker-compose up -d # Check logs docker-compose logs -f fastapi Step 6: Verify via ALB Backend is exposed via Application Load Balancer. Verify from your local machine:\n# Get ALB DNS from Terraform output ALB_DNS=$(terraform -chdir=terraform output -raw alb_dns_name) # Test health endpoint curl http://$ALB_DNS/health # {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;} Step 7: Check Qdrant Collection # On EC2 curl http://localhost:6333/collections # Create collection for documents (if not exists) curl -X PUT http://localhost:6333/collections/documents \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;vectors\u0026#34;: { \u0026#34;size\u0026#34;: 1024, \u0026#34;distance\u0026#34;: \u0026#34;Cosine\u0026#34; } }\u0026#39; 💡 Note: Vector size 1024 corresponds to Amazon Titan Embeddings v2.\nChecklist Successfully accessed EC2 via Session Manager Docker containers running (fastapi, qdrant) .env file configured Health check via ALB successful Qdrant collection created Troubleshooting Unable to connect to Session Manager # Check SSM Agent on EC2 sudo systemctl status amazon-ssm-agent # Verify IAM Role has AmazonSSMManagedInstanceCore policy Container fails to start # View logs docker-compose logs # Check disk space df -h ALB health check fails # Verify Security Group allows port 8000 from ALB # Verify FastAPI is listening on 0.0.0.0:8000 docker-compose logs fastapi "},{"uri":"https://tranvanan5.github.io/internship-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better.\n2. Support from Mentor / Team Admin\nMentors provide enthusiastic and detailed guidance, always ready to answer questions and give suggestions so that students can develop best. The Admin Team always provides good support during the internship process and always shares good resources and knowledge.\n3. Relevance of Work to Academic Major\nThe knowledge and tasks I received are very suitable to the knowledge I learned at school, it helps me expand new skills, supporting my future studies and work. From there, I can consolidate my basic knowledge, improve my professional knowledge and increase my future job opportunities.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using AWS services in infrastructure management, web management, improving professional skills and learning working skills and corporate culture. Mentor and the company created opportunities for me to participate in many useful events and shared a lot of good knowledge to support my personal development.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive and fun. People are very friendly, always willing to help each other in work and study. But no less serious when carrying out tasks and responsibilities at work.\n6. Internship Policies / Benefits\nThe company always supports the internship in the most appropriate and proper way. Create conditions for interns to participate in useful events to improve professional knowledge, learn more useful knowledge and help update new technology.\nAdditional Questions What I am most satisfied with during the internship is working in a professional environment and meeting enthusiastic Mentors. Along with that, I get to participate in many large-scale events such as Vietnam Cloud Day 2025. What I think the company needs to improve is managing the number of interns participating and registering at the office to improve the quality of the internship program. I will introduce it to my friends and advise them to intern here. Because this is a good internship program, helping students to supplement their knowledge and access job trends in the current era of technology development. "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.8-idp-pipeline/","title":"Set up IDP Pipeline","tags":[],"description":"","content":"Set up IDP Pipeline In this section, you will setup SQS Worker to process documents through IDP (Intelligent Document Processing) pipeline.\nIDP Flow Upload → S3 → DynamoDB (UPLOADED) → SQS ↓ EC2 Worker ↓ PyPDF2 (digital) / Textract (scanned) ↓ Chunk Text (1000 tokens) ↓ Cohere Embed Multilingual v3 (Bedrock) ↓ Qdrant (store vectors) ↓ DynamoDB (EMBEDDING_DONE) 💡 Note: Worker uses PyPDF2 for digital PDF (text-based) and Textract for scanned PDF (image-based).\nDocument States Status Description UPLOADED File uploaded, waiting for processing IDP_RUNNING Worker is processing TEXTRACT_DONE OCR completed (scanned PDF only) EMBEDDING_DONE Completed, ready to use FAILED Error occurred Step 1: Access EC2 via Session Manager # Get Instance ID INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Connect aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 After connecting:\nsudo su - ec2-user cd /home/ec2-user/backend 💡 Note: EC2 has 2 folders:\napp/ - Boilerplate from user_data script backend/ - Actual code deployed via CI/CD (contains run_worker.py) Step 2: Check Worker Code Worker code is in backend/run_worker.py. Verify the file exists:\nls -la # Must have: run_worker.py, app/, requirements.txt Step 3: Configure Environment Ensure .env file has all variables (in backend/ folder):\ncd /home/ec2-user/backend cat .env Important variables for IDP:\nSQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue S3_BUCKET=arc-documents-\u0026lt;account\u0026gt; QDRANT_HOST=localhost QDRANT_PORT=6333 AWS_REGION=ap-southeast-1 Step 4: Start Worker Option A: Run directly (for debugging) # Activate virtual environment (if available) source venv/bin/activate # Run worker python run_worker.py Worker will display:\n============================================================ IDP Pipeline - SQS Worker ============================================================ Queue URL: https://sqs.ap-southeast-1.amazonaws.com/xxx/arc-dev-document-queue Bucket: arc-documents-xxx Region: ap-southeast-1 Qdrant: localhost:6333 ------------------------------------------------------------ Processing indefinitely (Ctrl+C to stop)... Option B: Run in background with nohup nohup python run_worker.py \u0026gt; worker.log 2\u0026gt;\u0026amp;1 \u0026amp; # Check process ps aux | grep run_worker # View logs tail -f worker.log Option C: Run in Docker (recommended) # Add worker to docker-compose.yml docker-compose up -d worker Step 5: Test IDP Pipeline 5.1 Upload test file to S3 # From local machine aws s3 cp test-sample.pdf s3://arc-documents-\u0026lt;account\u0026gt;/uploads/test-001.pdf 5.2 Create record in DynamoDB aws dynamodb put-item \\ --table-name arc-dev-documents \\ --item \u0026#39;{ \u0026#34;doc_id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-001\u0026#34;}, \u0026#34;sk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;METADATA\u0026#34;}, \u0026#34;filename\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-sample.pdf\u0026#34;}, \u0026#34;s3_key\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;}, \u0026#34;status\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;UPLOADED\u0026#34;}, \u0026#34;uploaded_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;\u0026#39;$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#39;\u0026#34;} }\u0026#39; 5.3 Send message to SQS aws sqs send-message \\ --queue-url https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue \\ --message-body \u0026#39;{ \u0026#34;doc_id\u0026#34;: \u0026#34;test-001\u0026#34;, \u0026#34;s3_key\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;, \u0026#34;filename\u0026#34;: \u0026#34;test-sample.pdf\u0026#34; }\u0026#39; Step 6: Monitor Processing View worker logs:\n# If running directly # Logs displayed on terminal # If running in background tail -f worker.log Successful logs will look like:\n2024-01-15 10:30:00 - INFO - Received message for doc_id: test-001 2024-01-15 10:30:01 - INFO - Downloading from S3: uploads/test-001.pdf 2024-01-15 10:30:02 - INFO - Extracting text with PyPDF2... 2024-01-15 10:30:03 - INFO - Created 8 chunks from document 2024-01-15 10:30:05 - INFO - Generating embeddings with Cohere... 2024-01-15 10:30:10 - INFO - Stored 8 vectors for test-001 2024-01-15 10:30:10 - INFO - Updated status: EMBEDDING_DONE 2024-01-15 10:30:10 - INFO - Document test-001 processed successfully Step 7: Verify Processing 7.1 Check DynamoDB aws dynamodb get-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --query \u0026#39;Item.{status:status.S,chunks:chunk_count.N}\u0026#39; Expected output:\n{ \u0026#34;status\u0026#34;: \u0026#34;EMBEDDING_DONE\u0026#34;, \u0026#34;chunks\u0026#34;: \u0026#34;8\u0026#34; } 7.2 Check Qdrant # On EC2 curl -s http://localhost:6333/collections/arc_documents/points/count | jq Expected output:\n{ \u0026#34;result\u0026#34;: { \u0026#34;count\u0026#34;: 8 } } Error Handling Issue Cause Solution Worker not receiving messages SQS URL incorrect Check .env Bedrock timeout Rate limit Increase retry delay Qdrant connection refused Container not started docker-compose up -d qdrant FAILED status Check error_message in DynamoDB Fix and retry Retry Failed Document # Update status back to UPLOADED to retry aws dynamodb update-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --update-expression \u0026#34;SET #s = :s\u0026#34; \\ --expression-attribute-names \u0026#39;{\u0026#34;#s\u0026#34;:\u0026#34;status\u0026#34;}\u0026#39; \\ --expression-attribute-values \u0026#39;{\u0026#34;:s\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;UPLOADED\u0026#34;}}\u0026#39; # Send message to SQS again aws sqs send-message \\ --queue-url $SQS_QUEUE_URL \\ --message-body \u0026#39;{\u0026#34;doc_id\u0026#34;:\u0026#34;test-001\u0026#34;,\u0026#34;s3_key\u0026#34;:\u0026#34;uploads/test-001.pdf\u0026#34;,\u0026#34;filename\u0026#34;:\u0026#34;test-sample.pdf\u0026#34;}\u0026#39; Checklist Access EC2 via Session Manager Worker code is available Environment variables configured Worker is running Test document uploaded to S3 SQS message sent Worker processed document (logs) Status = EMBEDDING_DONE in DynamoDB Vectors stored in Qdrant "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.9-frontend/","title":"Setup Frontend","tags":[],"description":"","content":"Setup Frontend Configure and deploy Frontend React application with AWS Amplify.\nStep 1: Get Terraform Outputs cd terraform terraform output Note: cognito_user_pool_id, cognito_client_id, alb_dns_name\nStep 2: Configure Environment cd ARC-project cp .env.example .env Edit .env:\nVITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_POOL_ID=ap-southeast-1_xxxxxxx VITE_COGNITO_CLIENT_ID=xxxxxxxxxxxxxxxxxxxxxxxxxx VITE_API_URL=http://arc-chatbot-dev-alb-xxxxx.ap-southeast-1.elb.amazonaws.com Step 3: Install \u0026amp; Test Local npm install npm run dev Open http://localhost:5173\nStep 4: Build \u0026amp; Deploy npm run build Push code to GitHub, Amplify will deploy automatically:\ngit add . git commit -m \u0026#34;Update frontend config\u0026#34; git push origin main 💡 Amplify app was created via Terraform and connected with GitHub.\nStep 5: Update Cognito Callback URLs After getting Amplify URL:\naws cognito-idp update-user-pool-client \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --client-id xxxxxxxxxx \\ --callback-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; \\ --logout-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; Step 6: Create Test Users # Admin user aws cognito-idp admin-create-user \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --user-attributes Name=email,Value=admin@example.com \\ --temporary-password \u0026#34;TempPass123!\u0026#34; aws cognito-idp admin-add-user-to-group \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --group-name admin Error Handling Error Solution CORS error Check FastAPI CORS config \u0026ldquo;User pool does not exist\u0026rdquo; Check VITE_COGNITO_POOL_ID Build failed Check Amplify environment variables Checklist .env configured Local dev server running Amplify deploy successful Cognito callback URLs updated Login/Register working "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.10-using-chatbot/","title":"Using Chatbot","tags":[],"description":"","content":"Guide to using ARC Chatbot to search for information from research documents.\nAccess Local: http://localhost:5173 Production: Amplify URL from previous step Step 1: Login Enter email and password Click Login Redirect to Chat page Step 2: Chat Interface After logging in, you will see:\nSidebar with Chat, History menus Header with user info and dark mode toggle Chat area with welcome message Step 3: Ask Questions Enter a question in the input box and press Enter or click Send.\nGood Questions Type Example Definition \u0026ldquo;What is a stack data structure?\u0026rdquo; Comparison \u0026ldquo;Compare stack and queue\u0026rdquo; Explanation \u0026ldquo;Explain binary search algorithm\u0026rdquo; Avoid ❌ Too general: \u0026ldquo;Tell me about programming\u0026rdquo; ❌ Outside documents: \u0026ldquo;What\u0026rsquo;s the weather today?\u0026rdquo; Step 4: Citations Each answer has citations showing document sources:\n📚 Sources: [1] data-structures.pdf - Page 12 - Score: 85% [2] algorithms.pdf - Page 45 - Score: 72% Click on citation to view document details.\nField Description [1], [2] Citation number Filename PDF filename Page Page number Score Relevance (%) Step 5: Conversation History Click History in sidebar View list of previous conversations Click conversation to reload it Click trash icon to delete Step 6: New Chat Click New Chat in sidebar to start a new conversation.\nFeatures Feature Description Streaming Response displays in parts Markdown Supports code blocks, lists, headers Dark Mode Toggle in header History Save and reload conversations Checklist Successfully logged in Sent query and received response Citations displayed correctly Click citation to view document History working New Chat working "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.11-admin-dashboard/","title":"Using Admin Dashboard","tags":[],"description":"","content":"Using Admin Dashboard Guide to using Admin Dashboard to manage documents.\nAccess URL: http://localhost:5173/admin (or Amplify URL) Requirement: Account in admin group Step 1: Login Admin Log in with admin account (created in previous step).\nStep 2: Dashboard Overview After logging in, you will see:\nUpload section (drag \u0026amp; drop) Documents table with pagination Status filter and auto-refresh Step 3: Upload Documents 3.1. Select files Drag \u0026amp; drop PDF files to upload area Or click Browse Files to select 3.2. Upload Progress Each file displays progress bar and status:\nuploading - Uploading success - Upload successful error - Upload failed Step 4: Document Status After upload, document will be processed through IDP pipeline:\nStatus Description Time UPLOADED Waiting for processing - IDP_RUNNING Processing 1-5 min EMBEDDING_DONE Ready - FAILED Error - 💡 Tip: Enable Auto-refresh (5s) to automatically update status.\nStep 5: Manage Documents Filter by Status Use Status dropdown to filter:\nAll Uploaded Processing Done Failed Pagination Documents are paginated (5 items/page). Use pagination controls at footer.\nView Document Click 👁️ icon to view document details.\nDelete Document Click 🗑️ icon to delete document.\n⚠️ Warning: Deleting document will delete from S3, DynamoDB, and Qdrant.\nStep 6: Processing History Click Processing History link to view document processing history.\nError Handling Issue Solution Upload failed Check file size (\u0026lt;50MB), format (PDF only) Document stuck in IDP_RUNNING Check worker logs on EC2 Document FAILED See error message in Processing History Checklist Logged in to admin dashboard Upload document successful Document processed (EMBEDDING_DONE) Filter/pagination working Auto-refresh working "},{"uri":"https://tranvanan5.github.io/internship-report/5-workshop/5.12-cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Cleanup Resources After completing the workshop, clean up AWS resources to avoid incurring charges.\n⚠️ Warning: These steps will PERMANENTLY DELETE all data and resources!\nCleanup Order Stop services on EC2 Empty S3 buckets Terraform destroy Verify cleanup Step 1: Stop Services on EC2 Connect EC2 via Session Manager:\nINSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 Stop Docker containers:\nsudo su - ec2-user cd /home/ec2-user/app # Stop containers docker-compose down # Remove volumes docker volume rm app_qdrant_storage Step 2: Empty S3 Buckets S3 buckets must be empty before Terraform destroy:\n# Get bucket name from Terraform output BUCKET=$(terraform -chdir=terraform output -raw s3_bucket_name) # Empty bucket aws s3 rm s3://$BUCKET --recursive # Or force delete aws s3 rb s3://$BUCKET --force Step 3: Terraform Destroy cd terraform terraform plan -destroy terraform destroy Enter yes when prompted. This process takes about 10-15 minutes.\nStep 4: Manual Cleanup (if needed) If there are still resources not deleted:\n# CloudWatch Log Groups aws logs describe-log-groups --log-group-name-prefix /aws/arc | jq -r \u0026#39;.logGroups[].logGroupName\u0026#39; | xargs -I {} aws logs delete-log-group --log-group-name {} # EC2 Key Pair (if created manually) aws ec2 delete-key-pair --key-name arc-keypair # Amplify App (if created manually) aws amplify list-apps | jq -r \u0026#39;.apps[] | select(.name | contains(\u0026#34;arc\u0026#34;)) | .appId\u0026#39; | xargs -I {} aws amplify delete-app --app-id {} Step 5: Verify Cleanup # Check EC2 aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=*arc*\u0026#34; --query \u0026#39;Reservations[].Instances[].InstanceId\u0026#39; # Check S3 aws s3 ls | grep arc # Check RDS/DynamoDB aws dynamodb list-tables --query \u0026#39;TableNames[?contains(@, `arc`)]\u0026#39; # Check Lambda aws lambda list-functions --query \u0026#39;Functions[?contains(FunctionName, `arc`)].FunctionName\u0026#39; # Check ECR aws ecr describe-repositories --query \u0026#39;repositories[?contains(repositoryName, `arc`)].repositoryName\u0026#39; Expected output: All empty.\nCost Estimation Before cleanup, check estimated costs:\n# CloudWatch - Check billing dashboard for actual charges # https://console.aws.amazon.com/billing/ Checklist EC2 services stopped S3 buckets emptied Terraform destroy completed Manual cleanup verified All resources deleted Billing dashboard checked "},{"uri":"https://tranvanan5.github.io/internship-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://tranvanan5.github.io/internship-report/tags/","title":"Tags","tags":[],"description":"","content":""}]