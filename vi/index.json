[{"uri":"https://tranvanan5.github.io/internship-report/vi/","title":"Báo cáo thực tập","tags":[],"description":"","content":"Báo cáo thực tập Thông tin sinh viên: Họ và tên: Trần Văn An\nSố điện thoại: 0378374762\nEmail: antvse181691@fpt.edu.vn\nTrường: Trường Đại học FPT phân hiệu Hồ Chí Minh\nNgành: Kỹ Thuật Phần Mềm\nLớp: AWS082025\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 08/09/2025 đến ngày 28/11/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "},{"uri":"https://tranvanan5.github.io/internship-report/vi/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS Lambda giới thiệu mức giá theo tầng cho nhật ký Amazon CloudWatch và các đích ghi nhật ký bổ sung bởi Shridhar Pandey và Matthew Barker | 01 tháng 5 năm 2025 | [Thông báo], [AWS Lambda], [Intermediate (200)], [Serverless]\nViệc ghi nhật ký hiệu quả là một phần quan trọng của chiến lược quan sát khi xây dựng các ứng dụng không có máy chủ bằng AWS Lambda.\nLambda tự động thu thập và gửi nhật ký đến Amazon CloudWatch Logs.Điều này cho phép bạn tập trung vào việc xây dựng logic ứng dụng thay vì thiết lập cơ sở hạ tầng ghi nhật ký và cho phép người vận hành khắc phục sự cố lỗi và hiệu suất dễ dàng hơn.\nVào ngày 1 tháng 5 năm 2025, AWS đã công bố những thay đổi đối với tính năng ghi nhật ký Lambda, giúp giảm chi phí ghi nhật ký Lambda CloudWatch và giúp việc sử dụng nhiều công cụ giám sát dễ dàng và tiết kiệm chi phí hơn. Nhật ký Lambda hiện có sẵn với mức giá theo tầng dựa trên khối lượng khi sử dụng các lớp nhật ký CloudWatch Logs Standard và Infrequent Access. Khi tạo nhật ký Lambda ở quy mô lớn, bạn có thể mong đợi chi phí giảm ngay lập tức theo mô hình định giá mới này. Ngoài Nhật ký CloudWatch, Lambda hiện cũng hỗ trợ Amazon S3 và Amazon Data Firehose làm đích bổ sung cho nhật ký Lambda. Nhật ký Lambda được gửi đến S3 và Firehose cũng có sẵn với mức giá theo tầng dựa trên khối lượng.\nBài đăng trên blog này đề cập đến một số cải tiến ghi nhật ký Lambda gần đây và mô tả cách thay đổi này mang lại trải nghiệm ghi nhật ký đơn giản hơn, tiết kiệm chi phí hơn cho Lambda.\nTổng quan Việc ghi nhật ký cung cấp cho các nhà phát triển và vận hành viên dữ liệu giá trị để gỡ lỗi và xử lý sự cố hành vi ứng dụng, các vấn đề về hiệu suất và các lỗi tiềm ẩn. Điều này càng trở nên quan trọng hơn đối với các ứng dụng không máy chủ được xây dựng bằng Lambda do tính chất tạm thời và không trạng thái của Lambda execution environment. Tích hợp sẵn của Lambda với CloudWatch Logs đảm bảo rằng nhật ký cho mọi lệnh gọi hàm đều có sẵn để phân tích. Dữ liệu nhật ký được thu thập bao gồm nhật ký ứng dụng do mã hàm Lambda của bạn tạo ra và nhật ký hệ thống do dịch vụ Lambda tạo ra khi chạy mã hàm. CloudWatch Logs cho phép bạn tìm kiếm, lọc và phân tích dữ liệu nhật ký để khắc phục sự cố, theo dõi số liệu và thiết lập cảnh báo.\nYêu cầu ghi nhật ký ngày càng phát triển khi các ứng dụng không máy chủ ngày càng phức tạp và mở rộng quy mô, đôi khi bao gồm hàng trăm hoặc hàng nghìn hàm Lambda, tạo ra khối lượng nhật ký đáng kể. Các tổ chức cần các giải pháp ghi nhật ký tinh vi có thể xử lý quy mô này mà vẫn tiết kiệm chi phí. Một số tình huống, chẳng hạn như giám sát các giao dịch kinh doanh quan trọng, đòi hỏi phân tích nhật ký theo thời gian thực, trong khi những tình huống khác tập trung vào phân tích pháp y sau sự kiện. Nhật ký gỡ lỗi từ môi trường phát triển và môi trường dàn dựng thường cần độ chi tiết cao, trong khi bạn có thể muốn độ chi tiết thấp hơn trong nhật ký sản xuất để cải thiện tỷ lệ tín hiệu trên nhiễu.\nCác cải tiến gần đây của Lambda logging Trong những năm gần đây, Lambda và CloudWatch Logs đã mở rộng khả năng ghi nhật ký của Lambda để đáp ứng nhu cầu ngày càng tăng của các ứng dụng không máy chủ. Các khả năng này cung cấp thông tin chi tiết sâu hơn, khả năng kiểm soát tốt hơn và các giải pháp tiết kiệm chi phí hơn để thu thập, xử lý và sử dụng nhật ký, từ đó nâng cao trải nghiệm quan sát không máy chủ. Các kiểm soát ghi nhật ký nâng cao của Lambda (Lambda advanced logging controls)cho phép các nhà phát triển kiểm soát việc tạo và nội dung nhật ký. Các điều khiển này cho phép bạn thu thập nhật ký Lambda ở định dạng có cấu trúc JSON. Bạn không cần phải sử dụng thư viện ghi nhật ký và tùy chỉnh các mức nhật ký (INFO, DEBUG, WARN, ERROR) riêng biệt cho nhật ký ứng dụng và nhật ký hệ thống. Điều này giúp giảm chi phí ghi nhật ký bằng cách đảm bảo chỉ tạo các nhật ký cần thiết trong khi vẫn duy trì khả năng hiển thị phù hợp trên các môi trường khác nhau. Ví dụ: bạn có thể thiết lập mức ghi nhật ký DEBUG chi tiết trong môi trường phát triển đồng thời giới hạn mức ghi nhật ký sản xuất ở mức ERROR để cải thiện tỷ lệ tín hiệu trên nhiễu và kiểm soát chi phí.\nInfrequent Access log class dành cho CloudWatch Logs đã giới thiệu một giải pháp tiết kiệm chi phí cho các nhật ký cần lưu giữ nhưng ít được truy cập. Lớp Infrequent Access có giá nhập dữ liệu (ingestion) trên mỗi GB thấp hơn 50% so với lớp nhật ký Standard. Bộ tính năng được thiết kế riêng này cho phép bạn giảm chi phí ghi nhật ký trong khi vẫn duy trì quyền truy cập vào dữ liệu lịch sử cho mục đích tuân thủ, kiểm toán hoặc phân tích pháp y.\nCloudWatch Logs Live Tail là một tính năng phân tích và truyền phát nhật ký tương tác theo thời gian thực. Live Tail đơn giản hóa quy trình gỡ lỗi và giám sát; cho phép bạn quan sát đầu ra nhật ký khi các hàm thực thi mà không cần rời khỏi bảng điều khiển Lambda. Điều này giúp dễ dàng xác định và chẩn đoán sự cố trong quá trình phát triển và xử lý sự cố. Logs Live Tail cũng có sẵn trong Visual Code IDE.\nMức giá theo bậc cho Lambda logging trong CloudWatch Logs Bắt đầu từ hôm nay, nhật ký Lambda được gửi đến CloudWatch Logs sẽ được phân loại là Vended Logs (Nhật ký do dịch vụ cung cấp), tức là nhật ký từ các dịch vụ AWS cụ thể có sẵn theo mức giá theo khối lượng. Điều này thay thế mô hình giá cố định trước đây khi sử dụng lớp nhật ký Chuẩn của CloudWatch Logs. Ví dụ: tại Khu vực AWS Đông Hoa Kỳ (Bắc Virginia), bạn bị tính phí 0,50 đô la cho mỗi GB khi sử dụng lớp nhật ký Chuẩn cho nhật ký Lambda của mình. Theo mô hình giá mới, bạn bị tính phí khi gửi nhật ký Lambda đến CloudWatch Logs bắt đầu từ 0,50 đô la cho mỗi GB cho lần sử dụng đầu tiên. Khi khối lượng nhật ký tăng lên, giá cho mỗi GB sẽ tự động giảm qua nhiều tầng, xuống mức thấp nhất là 0,05 đô la cho mỗi GB ở tầng thấp nhất. Thay đổi giá này được tự động áp dụng cho tất cả nhật ký Lambda được gửi đến CloudWatch Logs, không yêu cầu bạn phải thay đổi mã hoặc cấu hình.\nDữ liệu đã được nhập Nhật ký CloudWatch Tiêu chuẩn CloudWatch Logs Infrequent Access 10 TB đầu tiên/tháng $0.50/GB $0.25/GB 20 TB tiếp theo/tháng $0.25/GB $0.15/GB 20 TB tiếp theo/tháng $0.10/GB $0.075/GB hơn 50 TB/tháng $0.05/GB $0.05/GB *Bảng 1: Giá theo bậc cho nhật ký Lambda trong CloudWatch Logs tại Khu vực Đông Hoa Kỳ (Bắc Virginia) *\nKhi tạo nhật ký Lambda ở quy mô lớn, bạn sẽ thấy chi phí giảm ngay lập tức theo mô hình giá mới này. Ví dụ: nếu bạn tạo 60 TB nhật ký Lambda hàng tháng trong CloudWatch Logs, chi phí sẽ giảm 58% (từ 30.000 đô la xuống còn 12.500 đô la). Các bậc giá sẽ tăng theo khối lượng ghi nhật ký của bạn, đảm bảo lợi ích về chi phí sẽ tăng lên khi ứng dụng của bạn phát triển. Điều này cho phép bạn duy trì các hoạt động ghi nhật ký toàn diện mà trước đây có thể rất tốn kém. Định giá theo bậc của Vended logs được áp dụng cho tất cả Vended logs được nạp vào CloudWatch chứ không phải theo bậc cho mỗi dịch vụ. Khi thu thập các nhật ký bán sẵn khác, chẳng hạn như Amazon Virtual Private Cloud flow logs và Amazon Route 53 resolver query logs, bạn sẽ thấy mức chiết khấu lớn hơn khi phân tầng được áp dụng ở khối lượng thu thập nhật ký hợp nhất.\nĐiểm đến ghi nhật ký Lambda mới: Amazon S3 và Amazon Data Firehose Bắt đầu từ hôm nay, ngoài CloudWatch Logs, Lambda cũng hỗ trợ Amazon S3 và Amazon Data Firehose làm đích cho nhật ký Lambda. Khi sử dụng S3 hoặc Firehose làm đích, chi phí ghi nhật ký bắt đầu từ 0,25 đô la Mỹ/GB. Mức giá theo bậc cũng được áp dụng, với mức giá giảm xuống còn 0,05 đô la Mỹ/GB ở bậc thấp nhất. Mức giá theo bậc này cũng được áp dụng cho khối lượng nhật ký được hợp nhất.\nDữ liệu đã được nhập Nhật ký CloudWatch Tiêu chuẩn CloudWatch ghi lại các lần truy cập không thường xuyên 10 TB đầu tiên/tháng $0.50/GB $0.25/GB 20 TB tiếp theo/tháng $0.25/GB $0.15/GB 20 TB tiếp theo/tháng $0.10/GB $0.075/GB hơn 50 TB/tháng $0.05/GB $0.05/GB *Bảng 2: Giá theo từng bậc cho dịch vụ giao nhật ký Lambda tới Amazon S3 và Amazon Data Firehose tại Khu vực Đông Hoa Kỳ (Bắc Virginia) *\nViệc phân phối trực tiếp nhật ký Lambda đến S3 mang lại sự linh hoạt hơn trong việc quản lý nhật ký. Hỗ trợ Firehose giúp đơn giản hóa việc phân phối nhật ký Lambda đến các đích bổ sung như Amazon OpenSearch Service, HTTP endpoints, và các nhà cung cấp khả năng quan sát (observability providers) bên thứ ba. Điều này phù hợp với mô hình phân phối nhật ký đã được thiết lập, được sử dụng với các dịch vụ điện toán AWS khác như Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Compute Cloud (Amazon EC2).\nKhả năng mới này mang lại lợi ích đáng kể về chi phí và hợp lý hóa việc phân phối nhật ký đến các đích ghi nhật ký bổ sung, giúp sử dụng dễ dàng hơn nhiều công cụ giám sát (bao gồm cả CloudWatch) khi xây dựng các ứng dụng không có máy chủ bằng Lambda.\nThiết lập đích ghi nhật ký Lambda mới [cite_start]Tất cả các hàm Lambda mới và hiện có đều có CloudWatch Logs làm đích ghi nhật ký mặc định, với S3 và Firehose là các lựa chọn thay thế[cite: 1]. [cite_start]Khi bạn chọn S3 hoặc Firehose, Lambda sẽ gửi nhật ký thông qua một lớp nhật ký phân phối (delivery log class) mới của CloudWatch Logs[cite: 1].\nCác bước thiết lập cơ bản trong bảng điều khiển Lambda:\nTất cả các hàm Lambda mới và hiện có đều có CloudWatch Logs làm đích ghi nhật ký mặc định, với S3 và Firehose là các lựa chọn thay thế. Khi bạn chọn S3 hoặc Firehose làm đích ghi nhật ký,Lambda sẽ gửi nhật ký đến đích đã chọn thông qua (via) một lớp nhật ký phân phối (delivery log class) mới của CloudWatch Logs. Lớp nhật ký này cho phép định tuyến hiệu quả nhưng không hỗ trợ các tính năng lớp nhật ký CloudWatch Logs Standard, chẳng hạn như Logs Insights và Live Tail.\nĐể thiết lập S3 hoặc Firehose làm đích cho nhật ký Lambda của bạn trong bảng điều khiển Lambda:\nĐiều hướng đến Lambda console,và chọn hoặc tạo một hàm để thiết lập đích ghi nhật ký S3 hoặc Firehose. Trong tab Configuration, chọn Monitoring and operations tools ở ngăn bên trái. Chọn Edit trong Logging configuration. Thao tác này sẽ mở trang Edit logging configuration Trong phần \u0026ldquo;Log destination\u0026rdquo;, chọn Amazon S3 hoặc Amazon Data Firehose. Amazon CloudWatch Logs là lựa chọn mặc định. Trong CloudWatch delivery log group, chọn Create new log group hoặc Existing log group. Để tạo nhóm nhật ký phân phối mới để gửi nhật ký đến S3, hãy nhập tên nhóm nhật ký và chỉ định vùng lưu trữ S3 đích. Cung cấp vai trò AWS Identity and Access Management (IAM)cho CloudWatch Logs để phân phối nhật ký đến S3. Để sử dụng nhóm nhật ký phân phối hiện có, hãy chọn một nhóm từ nhóm Delivery log group. Delivery log group được chọn phải có đích được cấu hình (S3 hoặc Firehose) và khớp với đích bạn đã chọn. Các điều khiển ghi nhật ký nâng cao cũng khả dụng cho các đích S3 và Firehose. Các điều khiển này bao gồm lựa chọn định dạng có cấu trúc JSON và bộ lọc cấp độ nhật ký cho cả nhật ký ứng dụng và nhật ký hệ thống. Điều này cung cấp cho bạn các điều khiển quản lý nhật ký nâng cao để tìm kiếm, lọc và phân tích dễ dàng hơn. Bạn cũng có thể sử dụng AWS Command Line Interface (AWS CLI) và infrastructure as code (IaC) tools như AWS CloudFormation and AWS Cloud Development Kit (AWS CDK) để thiết lập việc phân phối nhật ký Lambda đến S3 và Firehose.\nThực hành tốt nhất Để tận dụng tối đa những thay đổi được công bố hôm nay, hãy đảm bảo chiến lược ghi nhật ký của bạn phù hợp chặt chẽ với yêu cầu khối lượng công việc. Ví dụ: hãy cân nhắc gửi các nhật ký sản xuất quan trọng đến CloudWatch Logs để tận dụng các tính năng phân tích và cảnh báo theo thời gian thực tiên tiến. Giờ đây, bạn sẽ tự động được hưởng chiết khấu theo khối lượng thông qua giá theo bậc trong CloudWatch Logs cho các tình huống ghi nhật ký khối lượng lớn. Đối với các nhật ký cần lưu giữ lâu dài để phân tích lịch sử, bạn có thể sử dụng các lớp lưu trữ của S3 để giảm chi phí hơn nữa. Khi sử dụng các công cụ giám sát hiện có hoặc của bên thứ ba, tích hợp trực tiếp thông qua Firehose giúp loại bỏ nhu cầu về các giải pháp chuyển tiếp tùy chỉnh và các chi phí liên quan.\nTối ưu hóa chi phí ghi nhật ký không chỉ giới hạn ở việc lựa chọn đích đến. Thường xuyên theo dõi khối lượng nhật ký để hiểu tác động của các bậc giá. Triển khai các chính sách lưu giữ phù hợp để ngăn chặn việc lưu trữ nhật ký cũ không cần thiết và việc lấy mẫu nhật ký cho các nhật ký gỡ lỗi khối lượng lớn. Cân nhắc sử dụng các chiến lược ghi nhật ký khác nhau trên các môi trường phát triển, dàn dựng và sản xuất để cân bằng nhu cầu quan sát với hiệu quả chi phí.\nKết luận Mức giá theo từng bậc cho nhật ký Lambda trong CloudWatch Logs và hỗ trợ S3 và Firehose khi có thêm các đích ghi nhật ký, giúp cải thiện khả năng quan sát ứng dụng Lambda. Giờ đây, bạn có thể quản lý chi phí ghi nhật ký ở quy mô lớn và mở rộng các giải pháp giám sát Lambda thông qua các tích hợp tiết kiệm chi phí và dễ cấu hình. Cho dù bạn đang xây dựng các ứng dụng không máy chủ mới hay tối ưu hóa các ứng dụng hiện có, những cải tiến này giúp bạn triển khai các chiến lược ghi nhật ký toàn diện, có khả năng mở rộng hiệu quả về chi phí theo khối lượng công việc của bạn.\nCác tính năng mới được công bố hôm nay có sẵn tại tất cả các AWS Regions Hỗ trợ cấu hình phân phối nhật ký đến S3 và Firehose trong bảng điều khiển Lambda hiện có sẵn tại các Khu vực Đông Hoa Kỳ (Ohio), Đông Hoa Kỳ (Bắc Virginia), Tây Hoa Kỳ (Oregon) và Châu Âu (Ireland), cùng các Khu vực bổ sung sẽ sớm được ra mắt. Xem lại Lambda documentation và CloudWatch Logs documentation để tìm hiểu thêm về các tính năng này và cách sử dụng chúng. Xem lại CloudWatch pricing page để tìm hiểu thêm về cách tính giá của các tính năng này. Để biết thêm tài nguyên học tập về không máy chủ, hãy truy cập Serverless Land.\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Hướng dẫn triển khai AWS DMS: Xây dựng khả năng di chuyển cơ sở dữ liệu linh hoạt thông qua thử nghiệm, giám sát và SOP Bởi Sushant Deshmukh, Alex Anto Kizhakeyyepunnil Joy, and Sanyam Jain | Ngày 5 tháng 5 năm 2025 | trong Nâng cao (300), Dịch vụ di chuyển cơ sở dữ liệu AWS (AWS DMS), AWS Well-Architected,Thực hành tốt nhất |\nDịch vụ di chuyển cơ sở dữ liệu AWS (AWS DMS) đơn giản hóa việc di chuyển và sao chép cơ sở dữ liệu, cung cấp giải pháp được quản lý cho khách hàng. Quan sát của chúng tôi qua nhiều lần triển khai doanh nghiệp cho thấy việc đầu tư thời gian vào việc lập kế hoạch di chuyển cơ sở dữ liệu chủ động mang lại lợi ích đáng kể. Các tổ chức áp dụng chiến lược thiết lập toàn diện luôn gặp ít gián đoạn hơn và đạt được kết quả di chuyển tốt hơn.\nTrong bài viết này, chúng tôi trình bày các biện pháp chủ động để tối ưu hóa việc triển khai AWS DMS ngay từ giai đoạn thiết lập ban đầu. Bằng cách sử dụng kế hoạch chiến lược và tầm nhìn kiến trúc, các tổ chức có thể nâng cao độ tin cậy của hệ thống sao chép, cải thiện hiệu suất và tránh những cạm bẫy thường gặp.\nChúng tôi khám phá các chiến lược và phương pháp hay nhất trong các lĩnh vực chính sau:\nLập kế hoạch và chạy thử nghiệm chứng minh khái niệm (PoC) Triển khai kiểm tra lỗi hệ thống Xây dựng quy trình vận hành tiêu chuẩn (SOP) Giám sát và cảnh báo Áp dụng các nguyên tắc của AWS Well-Architected Framework Lập kế hoạch và thực hiện PoC Việc thực hiện PoC giúp phát hiện và khắc phục sớm các vấn đề về môi trường. Nó cũng giúp tạo ra thông tin mà bạn có thể sử dụng để ước tính tổng thời gian di chuyển và cam kết tài nguyên của mình.\nSau đây là các bước cơ bản để thực hiện PoC thành công:\nLên kế hoạch và triển khai môi trường thử nghiệm với các instances sao chép, tasks và endpoints AWS DMS phù hợp. Để biết thêm thông tin về việc lập kế hoạch và cung cấp tài nguyên, bạn có thể tham khảo Thực hành tốt nhất cho Dịch vụ di chuyển cơ sở dữ liệu AWS (AWS DMS). Sử dụng khối lượng công việc tương tự như trong môi trường sản xuất. Điều bắt buộc là phải mô phỏng môi trường sản xuất của bạn càng sát càng tốt để tăng khả năng gặp phải nhiều vấn đề khác nhau. Thực hiện thử nghiệm lỗi dựa trên các tình huống được thảo luận trong bảng ở phần tiếp theo. Theo dõi việc sử dụng tài nguyên và các điểm nghẽn xảy ra trong PoC và xem xét lại kế hoạch và triển khai tài nguyên cho phù hợp. Ghi lại các quan sát của bạn và thực hiện đánh giá di chuyển bằng cách so sánh với kết quả kinh doanh. Việc này bao gồm đánh giá thời gian phục hồi sau di chuyển và Thỏa thuận Mức Dịch vụ (SLA) của ứng dụng cho cả hoạt động di chuyển và hoạt động kinh doanh đang diễn ra. Nếu các yêu cầu di chuyển và vận hành này không được đáp ứng, hãy xem xét lại giai đoạn lập kế hoạch để đảm bảo phù hợp với nhu cầu kinh doanh của bạn. Thực hiện thử nghiệm lỗi có hệ thống Tất cả các hệ thống, bất kể độ bền bỉ ra sao, đều có thể gặp sự cố và thời gian ngừng hoạt động. Đối với các tổ chức vận hành khối lượng công việc quan trọng, việc lập kế hoạch chủ động là điều cần thiết để duy trì tính liên tục của hoạt động kinh doanh và đáp ứng các Thỏa thuận Dịch vụ (SLA). Phần này cung cấp một khuôn khổ chiến lược để phát triển các Quy trình Vận hành Chuẩn (SOP) thiết lập các giao thức phục hồi rõ ràng và giảm thiểu tác động vận hành trong trường hợp gián đoạn hệ thống.\nKhi triển khai AWS DMS, việc hiểu rõ các điểm lỗi tiềm ẩn trở nên vô cùng quan trọng để xây dựng các hệ thống linh hoạt. Bảng sau đây phác thảo các tình huống lỗi thường gặp trong các hệ thống sao chép AWS DMS, làm nền tảng cho chiến lược kiểm thử của bạn. Mặc dù bảng này khá toàn diện, chúng tôi khuyến khích bạn mở rộng các tình huống này dựa trên kiến trúc, yêu cầu tuân thủ và mục tiêu kinh doanh cụ thể của mình để có thể bao quát toàn diện các chế độ lỗi tiềm ẩn trong môi trường của bạn.\nĐiểm thất bại Các tình huống có khả năng ngừng hoạt động Kiểm tra Chiến lược giảm thiểu tiềm năng Cơ sở dữ liệu nguồn và đích Nút thắt hiệu suất trên máy chủ cơ sở dữ liệu như CPU cao, hạn chế bộ nhớ. Tạo áp lực tải lớn lên hệ thống bằng công cụ đánh giá chuẩn như sysbench để mô phỏng tải trọng cao trên máy chủ cơ sở dữ liệu. Bạn có thể cung cấp một nút cơ sở dữ liệu chỉ đọc cho các công cụ mà AWS DMS hỗ trợ, sử dụng bản sao đọc làm nguồn. Để biết thêm thông tin, hãy tham khảo mục Nguồn di chuyển dữ liệu. Bạn cũng có thể mở rộng tài nguyên cơ sở dữ liệu và tối ưu hóa các tham số cơ sở dữ liệu. Các vấn đề truy cập dữ liệu do không đủ quyền Dùng tài khoản database cho AWS DMS nhưng tài khoản này không được cấp đủ quyền. Tạo người dùng cơ sở dữ liệu theo nguyên tắc least-privilege. Tham khảo tài liệu điểm cuối nguồn AWS DMS tương ứng để biết các đặc quyền bắt buộc của DMS cho từng công cụ cơ sở dữ liệu. Failover cơ sở dữ liệu (nếu sử dụng thiết lập chính hoặc dự phòng) Thực hiện chuyển đổi dự phòng cơ sở dữ liệu từ node chính sang node phụ. Trong trường hợp AWS DMS có thể cố kết nối lại vào primary cũ sau quá trình failover, hành vi này phụ thuộc vào Thời gian Sống (TTL). Bạn sẽ cần khởi động lại task sau khi TTL được làm mới. Tham khảo Tại sao kết nối của tôi lại được chuyển hướng đến phiên bản trình đọc khi tôi cố kết nối với điểm cuối trình ghi Amazon Aurora của mình? Tắt cơ sở dữ liệu HOẶC xảy ra sự cố Dừng cơ sở dữ liệu đang sao chép DMS. Ghi lại toàn bộ hành vi của task DMS nhằm phục vụ việc xây dựng SOP, sau đó tiếp tục task khi sự cố cơ sở dữ liệu đã được khắc phục. Không có sẵn nhật ký giao dịch Xóa nhật ký có thời gian lưu giữ ngắn hơn khi task ngoại tuyến hoặc đang chạy chậm. Ghi lại các quan sát của task DMS để tạo SOP và tiếp tục task sau khi tạo nhật ký giao dịch hoặc thực hiện tải đầy đủ mới nếu nhật ký không khả dụng. Thực hiện các thay đổi về cấu trúc như lược đồ, bảng, chỉ mục, phân vùng và kiểu dữ liệu Chạy các câu lệnh ngôn ngữ định nghĩa dữ liệu (DDL) khác nhau để sửa đổi bảng có liên quan. Xem danh sách các câu lệnh DDL được hỗ trợ and cài đặt task. Lỗi mạng (áp dụng cho nguồn và đích) Các vấn đề về kết nối bao gồm lỗi mạng, DNS và SSL Xóa IP nguồn khỏi nhóm bảo mật AWS DMS HOẶC sửa đổi iptables; Xóa chứng chỉ khỏi điểm cuối DMS; Sửa đổi giá trị MTU (đơn vị truyền tối đa). Tham khảo cách khắc phục sự cố kết nối điểm cuối AWS DMS và sự cố khi kết nối với Amazon RDS. Mất gói tin Sử dụng lệnh kiểm soát lưu lượng (tc) trên hệ thống Linux HOẶC sử dụng AWS Fault Injection Simulator (FIS). Tham khảo phần Khắc phục sự cố mạng trong quá trình di chuyển cơ sở dữ liệu bằng AMI hỗ trợ chẩn đoán AWS DMS và Làm việc với AMI hỗ trợ chẩn đoán AWS DMS Lỗi AWS DMS lỗi Khởi động lại phiên bản sao chép Single-AZ Khởi động lại phiên bản sao chép AWS DMS. DMS sẽ tự động tiếp tục các task sau khi khởi động lại phiên bản sao chép. Khởi động lại phiên bản Multi-AZ Replication với khả năng chuyển đổi dự phòng trong quá trình sao chép đang diễn ra Khởi động lại phiên bản sao chép AWS DMS, chọn tùy chọn “Khởi động lại với chế độ dự phòng đã lên kế hoạch?”. DMS sẽ tự động tiếp tục các task sau khi chuyển đổi dự phòng Multi-AZ của phiên bản sao chép. Bộ nhớ EBS đầy Cho phép ghi nhật ký gỡ lỗi chi tiết cho nhiều thành phần nhật ký dẫn đến đầy bộ nhớ do AWS DMS logs. Thiết lập cảnh báo khi dung lượng lưu trữ đạt 80% và mở rộng dung lượng lưu trữ được liên kết với phiên bản sao chép DMS. Để biết thêm thông tin, hãy tham khảo Tại sao phiên bản DB sao chép AWS DMS của tôi lại ở trạng thái đầy dung lượng lưu trữ? Áp dụng các thay đổi trong cửa sổ bảo trì Sửa đổi cấu hình cho phiên bản sao chép DMS của bạn dẫn đến thời gian ngừng hoạt động và chọn tùy chọn “Áp dụng trong cửa sổ bảo trì theo lịch trình tiếp theo”. DMS tự động tiếp tục các task sau khi bảo trì. Tranh chấp tài nguyên trên phiên bản sao chép (CPU cao, tranh chấp bộ nhớ, IOPS cao hơn mức cơ bản) Tạo nhiều task DMS có giá trị cao cho MaxFullLoadSubTasks trên một phiên bản sao chép DMS nhỏ. Thiết lập giám sát và cảnh báo trên các số liệu quan trọng của CloudWatch, như đã thảo luận trong phần Giám sát và cảnh báo. Mở rộng lớp phiên bản hoặc bạn có thể di chuyển các task sang một phiên bản sao chép mới. Nâng cấp phiên bản sao chép DMS Nâng cấp phiên bản sao lưu DMS. DMS không còn hỗ trợ các phiên bản DMS cũ, do đó người dùng cần nâng cấp phiên bản sao lưu. Để biết thêm thông tin, vui lòng tham khảo Ghi chú phát hành AWS DMS. Để giảm thiểu thời gian ngừng hoạt động liên quan đến hoạt động này, chúng tôi khuyên bạn nên thực hiện một PoC kỹ lưỡng. Sau khi kiểm tra PoC, bạn có thể lên kế hoạch tạo các phiên bản sao chép mới chạy trên phiên bản DMS mới nhất và di chuyển tất cả các task của mình vào các giờ cao điểm thấp khi độ trễ thu thập dữ liệu thay đổi (CDC) bằng 0 hoặc tối thiểu. Để biết thêm thông tin, hãy tham khảo Di chuyển một nhiệm vụ. Bạn cũng có thể tham khảo Thực hiện nâng cấp song song trong AWS DMS bằng cách di chuyển các task để giảm thiểu tác động đến hoạt động kinh doanh. Các vấn đề về dữ liệu Sao chép dữ liệu Chạy một task chỉ tải đầy đủ hai lần, lần đầu tiên dừng task ở giữa và lần thứ hai chạy task với cấu hình \u0026ldquo;DO NOTHING\u0026rdquo; cho chế độ chuẩn bị bảng Mục tiêu. Sử dụng xác thực DMS cho các công cụ cơ sở dữ liệu được hỗ trợ. Nếu xác thực báo cáo bất kỳ sự không khớp nào, bạn cần điều tra dựa trên lỗi chính xác. Để giảm thiểu sự cố, bạn có thể thực hiện backfill bằng cách tạo task tải đầy đủ hoặc tải lại bảng (nếu có) cho các bảng cụ thể, sau đó tạo task sao chép liên tục. Mất dữ liệu Tạo các kích hoạt trên mục tiêu để xóa hoặc cắt bớt các bản ghi ngẫu nhiên. Chúng tôi khuyên bạn nên sử dụng xác thực DMS để tránh những vấn đề này. Bạn có thể thực hiện tải lại bảng hoặc task, hoặc tạo một task tải đầy đủ mới và thay đổi task thu thập dữ liệu để thực hiện tải dữ liệu mới cho (các) bảng bị ảnh hưởng.. Lỗi bảng Có được khóa truy cập độc quyền trên các bảng trước khi bắt đầu task DMS HOẶC sử dụng các kiểu dữ liệu không được hỗ trợ. Sự cố này có thể do tính năng hoặc cấu hình không được hỗ trợ của DMS. Cần phải điều tra dựa trên lỗi chính xác. Để biết thêm thông tin, vui lòng tham khảo Tại sao task AWS DMS của tôi lại ở trạng thái lỗi? Các vấn đề về độ trễ Tích lũy tệp hoán đổi trên phiên bản sao chép Bắt đầu các giao dịch dài hạn với số lượng thay đổi lớn và theo dõi. số liệu CloudWatch CDCChangesDiskSource. Theo dõi các số liệu CDCChangesDiskSource và CDCChangesDiskTarget. Tham khảo bài viết trong Trung tâm Kiến thức này để biết cách tạo SOP: Tệp hoán đổi là gì và tại sao các tệp này lại chiếm dung lượng trên phiên bản AWS DMS của tôi? Lỗi BatchApply Xóa một bản ghi trên mục tiêu và cập nhật bản ghi đó trên nguồn bằng cách sử dụng BatchApply trên task. Thiết lập cảnh báo trên nhật ký DMS CloudWatch cho thao tác áp dụng hàng loạt không thành công, vui lòng tham khảo phần Giám sát và cảnh báo để biết hướng dẫn chi tiết. Để khắc phục sự cố và tạo SOP, vui lòng tham khảo bài viết này trong Trung tâm Kiến thức: Làm thế nào để tôi có thể khắc phục sự cố tại sao Amazon Redshift chuyển sang chế độ từng cái một? Các vấn đề xác thực dữ liệu Nguồn bị thiếu Những điều này có thể được mô phỏng do thiếu dữ liệu về nguồn và mục tiêu. Xem lại trường hợp sử dụng được hỗ trợ và các hạn chế với Xác thực dữ liệu AWS DMS và tham khảo bài viết sau trong Trung tâm kiến thức để biết thêm thông tin: Tại sao quá trình xác thực task AWS DMS của tôi không thành công hoặc tại sao quá trình xác thực không tiến triển? Mục tiêu bị mất Những điều này có thể được mô phỏng do thiếu dữ liệu về nguồn và mục tiêu. Xem lại trường hợp sử dụng được hỗ trợ và các hạn chế với Xác thực dữ liệu AWS DMS và tham khảo bài viết sau trong Trung tâm kiến thức để biết thêm thông tin: Tại sao quá trình xác thực task AWS DMS của tôi không thành công hoặc tại sao quá trình xác thực không tiến triển? Sự khác biệt kỷ lục Bạn có thể tạo các lược đồ bảng khác nhau trong nguồn và đích để mô phỏng tình huống này. Xem lại trường hợp sử dụng được hỗ trợ và các hạn chế với Xác thực dữ liệu AWS DMS và tham khảo bài viết sau trong Trung tâm kiến thức để biết thêm thông tin: Tại sao quá trình xác thực task AWS DMS của tôi không thành công hoặc tại sao quá trình xác thực không tiến triển? Không tìm thấy khóa chính/khóa duy nhất đủ điều kiện Xác thực yêu cầu phải có khóa chính hoặc khóa duy nhất trong bảng. LOB và một số kiểu dữ liệu khác không được hỗ trợ với xác thực DMS. Để biết thêm chi tiết, hãy tham khảo hạn chế xác thực. Xem lại trường hợp sử dụng được hỗ trợ và các hạn chế với Xác thực dữ liệu AWS DMS và tham khảo bài viết sau trong Trung tâm kiến thức để biết thêm thông tin: Tại sao quá trình xác thực task AWS DMS của tôi không thành công hoặc tại sao quá trình xác thực không tiến triển? Bằng cách kiểm tra một cách có hệ thống các kịch bản này và ghi lại kết quả, các tổ chức có thể phát triển các quy trình phục hồi mạnh mẽ, giải quyết cả các chế độ lỗi phổ biến và lỗi đặc thù. Cách tiếp cận chủ động này không chỉ giúp duy trì độ tin cậy của hệ thống mà còn cung cấp cho các nhóm vận hành các giao thức rõ ràng để giải quyết các vấn đề khi chúng phát sinh.\nPhát triển các quy trình vận hành tiêu chuẩn (SOP) Trong các tình huống kiểm thử lỗi, hãy ghi chép cẩn thận tác động của từng sự cố lên hệ thống sao chép của bạn. Tài liệu này tạo nền tảng cho việc tạo ra các SOP tùy chỉnh mà nhóm của bạn có thể dựa vào khi quản lý việc triển khai AWS DMS. Các chiến lược giảm thiểu được nêu trong khuôn khổ kiểm thử lỗi của chúng tôi đóng vai trò là điểm khởi đầu tuyệt vời để phát triển các quy trình này.\nCác SOP ban đầu của bạn sẽ xuất hiện trong giai đoạn đầu của quá trình kiểm thử PoC. Các quy trình này nên được coi là tài liệu sống, đòi hỏi phải cập nhật và tinh chỉnh thường xuyên khi bạn có thêm kinh nghiệm vận hành và gặp phải các tình huống mới. Bản chất năng động của việc di chuyển cơ sở dữ liệu đồng nghĩa với việc các SOP của bạn sẽ phát triển cùng với sự hiểu biết của bạn về hành vi hệ thống và các thách thức mới nổi.\nĐể được hướng dẫn thêm về cách xử lý các tình huống di chuyển phức tạp, chúng tôi khuyên bạn nên xem lại loạt bài viết gồm ba phần trên blog của chúng tôi về gỡ lỗi di chuyển AWS DMS. Các tài nguyên này cung cấp những hiểu biết quý giá có thể giúp bạn phát triển các phương pháp tiếp cận có hệ thống để khắc phục sự cố, ngay cả đối với các tình huống không được đề cập trong các SOP hiện có của bạn. Bạn có thể tìm thấy các hướng dẫn chi tiết này tại:\nGỡ lỗi quá trình di chuyển AWS DMS của bạn: Cần làm gì khi có sự cố xảy ra (Phần 1) Gỡ lỗi quá trình di chuyển AWS DMS của bạn: Cần làm gì khi có sự cố xảy ra (Phần 2) Gỡ lỗi quá trình di chuyển AWS DMS của bạn: Cần làm gì khi có sự cố xảy ra (Phần 3) Bằng cách ghi chép và kiểm tra các quy trình này, các tổ chức có thể đo lường và xác thực chính xác khả năng đáp ứng SLA của hệ thống sao chép, đặc biệt là trong các trường hợp sự cố nghiêm trọng. Cách tiếp cận chủ động này giúp xác định các điểm nghẽn tiềm ẩn và các lĩnh vực cần cải thiện trong chiến lược phục hồi sau thảm họa, từ đó củng cố khả năng phục hồi và độ tin cậy của kiến trúc sao chép dữ liệu.\nKhi thiết kế chiến lược sao chép dữ liệu bằng AWS DMS, điều quan trọng là phải thiết lập các kế hoạch dự phòng toàn diện cho các tình huống liên quan đến việc dịch vụ không khả dụng hoặc sự khác biệt về dữ liệu. Việc đánh giá kỹ lưỡng RTO và RPO của doanh nghiệp sẽ thúc đẩy việc phát triển các SOP. Việc lập kế hoạch chiến lược này không chỉ thúc đẩy tính liên tục của hoạt động kinh doanh mà còn cung cấp những hiểu biết có giá trị về các chỉ số hiệu suất thực tế của hệ thống sao chép trong các tình huống sự cố.\nGiám sát và cảnh báo Để tối đa hóa hiệu quả của AWS DMS, cần có một phương pháp tiếp cận chiến lược đối với khả năng giám sát và báo cáo. Một khuôn khổ giám sát mạnh mẽ là điều cần thiết để duy trì hoạt động sao chép liền mạch và thúc đẩy tính toàn vẹn dữ liệu trong suốt quá trình di chuyển.\nViệc cấu hình các cảnh báo phù hợp trong quá trình thiết lập ban đầu sẽ cung cấp khả năng hiển thị theo thời gian thực đối với các task sao chép và cho phép phản hồi nhanh chóng với các bất thường. Các khả năng giám sát này hoạt động như một hệ thống cảnh báo sớm, giúp duy trì tình trạng hoạt động và hiệu quả của cơ sở hạ tầng di chuyển cơ sở dữ liệu của bạn.\nViệc triển khai giám sát và cảnh báo chủ động giúp nâng cao độ tin cậy vận hành, đồng thời cung cấp thông tin chi tiết về việc sử dụng tài nguyên và các mô hình hiệu suất. Phương pháp tiếp cận có hệ thống này cho phép đưa ra quyết định dựa trên dữ liệu và duy trì hiệu suất sao chép tối ưu trong suốt vòng đời di chuyển.\nAWS DMS cung cấp các tính năng giám sát sau:\nAmazon CloudWatch metrics – These metrics are automatically populated by AWS DMS for users to get insights into resource utilization and related metrics for individual task and at replication instance level. For a list of all the metrics available with AWS DMS, refer to Số liệu của Dịch vụ di chuyển cơ sở dữ liệu AWS. CloudWatch logs và Time Travel logs của AWS DMS – AWS DMS tạo nhật ký lỗi và điền vào chúng dựa trên mức ghi nhật ký do người dùng thiết lập cho từng thành phần. Để biết thêm thông tin, hãy tham khảo Xem và quản lý nhật ký task AWS DMS. Khi nhật ký CloudWatch được bật, AWS DMS theo mặc định sẽ bật ghi nhật ký ngữ cảnh. Ngoài ra, DMS còn có tính năng ghi nhật ký Du hành Thời gian để hỗ trợ gỡ lỗi các task sao chép. Để biết thêm thông tin về ghi nhật ký Du hành Thời gian, vui lòng tham khảo phần Cài đặt task Du hành Thời gian. Để biết các phương pháp hay nhất khi sử dụng nhật ký Du hành thời gian, hãy tham khảo Khắc phục sự cố task sao chép bằng Time Travel. Trạng thái task và bảng – AWS DMS cung cấp bảng thông tin gần như theo thời gian thực để báo cáo trạng thái của task và bảng. Để biết danh sách chi tiết về trạng thái task, vui lòng tham khảo Trạng thái nhiệm vụ. Để biết trạng thái bảng, hãy tham khảo Trạng thái bảng trong quá trình thực hiện nhiệm vụ. AWS CloudTrail logs– AWS DMS được tích hợp với AWS CloudTrail, một dịch vụ cung cấp bản ghi các hành động được thực hiện bởi người dùng, vai trò hoặc dịch vụ AWS trong AWS DMS. CloudTrail ghi lại tất cả các lệnh gọi API cho AWS DMS dưới dạng sự kiện, bao gồm các lệnh gọi từ bảng điều khiển AWS DMS và từ các lệnh gọi mã đến các hoạt động API của AWS DMS. Để biết thêm thông tin về cách thiết lập CloudTrail, vui lòng tham khảo Hướng dẫn sử dụng AWS CloudTrail. Monitoring dashboard – Bảng điều khiển giám sát nâng cao cung cấp khả năng hiển thị toàn diện các số liệu quan trọng liên quan đến task giám sát và phiên bản sao chép của bạn; lọc, tổng hợp và trực quan hóa các số liệu cho các tài nguyên cụ thể bạn muốn theo dõi. Bảng điều khiển trực tiếp xuất bản các số liệu CloudWatch hiện có để giám sát hiệu suất tài nguyên mà không thay đổi thời gian lấy mẫu điểm dữ liệu. Để biết thêm thông tin, vui lòng tham khảo Tổng quan về bảng điều khiển giám sát nâng cao. Chúng tôi khuyên bạn nên thiết lập cảnh báo CloudWatch trên các số liệu quan trọng và nhật ký sự kiện để chủ động xác định các vấn đề tiềm ẩn trước khi chúng leo thang thành gián đoạn toàn hệ thống. Mặc dù phương pháp giám sát cơ bản này chỉ là điểm khởi đầu, nhưng việc mở rộng chiến lược giám sát dựa trên các yêu cầu cụ thể về trường hợp sử dụng và mục tiêu kinh doanh của bạn là rất quan trọng.\nLoại số liệu Tên số liệu Biện pháp khắc phục Số liệu máy chủ Sử dụng CPU Bạn nên thiết lập cảnh báo dựa trên các số liệu này để cảnh báo người vận hành vì tranh chấp tài nguyên sẽ ảnh hưởng đến hiệu suất task DMS của bạn. Dựa trên giới hạn tài nguyên trên máy chủ, bạn cần nâng cấp lớp phiên bản DMS nếu có tranh chấp CPU và bộ nhớ, hoặc tăng dung lượng lưu trữ nếu dung lượng lưu trữ thấp hoặc IOPS cơ bản đang bị giới hạn. Để biết thêm thông tin về cách chọn phiên bản sao chép phù hợp, bạn có thể tham khảo bài viết Chọn kích thước tốt nhất cho phiên bản sao chép.. Bộ nhớ trống SwapUsage FreeStorageSpace WriteIOPS ReadIOPS Số liệu nhiệm vụ sao chép CDCLatencySource Dựa trên các yêu cầu SLA, bạn có thể thiết lập ngưỡng cảnh báo cho các số liệu về độ trễ. Trong DMS, độ trễ có thể do nhiều nguyên nhân gây ra. Để khắc phục sự cố và tạo SOP, bạn có thể tham khảo Khắc phục sự cố độ trễ trong Dịch vụ di chuyển cơ sở dữ liệu AWS. (AWS DMS) CDCLatencyTarget Sự kiện DMS cho phiên bản sao chép Thay đổi cấu hình Mỗi danh mục này là một danh mục với các sự kiện khác nhau được liên kết. Bạn có thể thiết lập thông báo về các sự kiện cụ thể dựa trên nhu cầu của mình. Để biết danh sách chi tiết và mô tả về các sự kiện này, vui lòng tham khảo danh mục sự kiện AWS DMS và thông báo sự kiện cho thông báo SNS.. Sáng tạo Xóa Bảo trì Lưu trữ thấp Chuyển đổi dự phòng Lỗi Sự kiện DMS cho các task sao chép Lỗi Mỗi danh mục này là một danh mục với các sự kiện khác nhau được liên kết. Bạn có thể thiết lập thông báo về các sự kiện cụ thể dựa trên nhu cầu của mình. Để biết danh sách chi tiết và mô tả về các sự kiện này, vui lòng tham khảo danh mục sự kiện AWS DMS và thông báo sự kiện cho thông báo SNS.. Thay đổi trạng thái Sáng tạo Xóa Để biết danh sách đầy đủ các số liệu có sẵn, bạn có thể tham khảo Hướng dẫn sử dụng AWS DMS về số liệu của AWS Database Migration Service.\nBạn có thể sử dụng Amazon EventBridge để cung cấp thông báo khi sự kiện AWS DMS xảy ra hoặc sử dụng Dịch vụ thông báo đơn giản của Amazon (Amazon SNS) để tạo cảnh báo cho các sự kiện quan trọng. Để biết thêm thông tin về sự kiện EventBridge trong DMS, hãy tham khảo Hướng dẫn sử dụng sự kiện EventBridge. Để biết thêm thông tin về cách sử dụng Amazon SNS với AWS DMS, hãy tham khảo Hướng dẫn sử dụng sự kiện Amazon SNS.\nNgoài việc thiết lập cảnh báo CloudWatch, bạn có thể tạo cảnh báo tùy chỉnh dựa trên nhật ký lỗi CloudWatch của AWS DMS bằng bộ lọc số liệu. Để biết hướng dẫn chi tiết từng bước về cách triển khai các cảnh báo tùy chỉnh này, hãy tham khảo bài đăng trên blog có tiêu đề \u0026quot;Gửi cảnh báo về lỗi AWS DMS tùy chỉnh từ Nhật ký Amazon CLoudWatch\u0026quot;. Tài nguyên này cung cấp hướng dẫn toàn diện để nâng cao khả năng giám sát lỗi tùy chỉnh của bạn.\nÁp dụng các nguyên tắc của AWS Well-Architected Framework Khung Kiến trúc Tốt của AWS giúp bạn hiểu rõ ưu và nhược điểm của các quyết định khi xây dựng hệ thống trên đám mây. Sáu trụ cột của khung hướng dẫn bạn các phương pháp kiến trúc tốt nhất để thiết kế và vận hành các hệ thống đáng tin cậy, an toàn, hiệu quả, tiết kiệm chi phí và bền vững.\nKhi sử dụng AWS Well-Architected Tool, có sẵn miễn phí trong AWS Management Console,bạn có thể xem xét khối lượng công việc của mình dựa trên các phương pháp hay nhất này bằng cách trả lời một bộ câu hỏi cho từng trụ cột.\nĐể biết thêm hướng dẫn chuyên môn và các phương pháp hay nhất cho kiến trúc đám mây của bạn—triển khai kiến trúc tham khảo, sơ đồ và sách trắng—hãy tham khảo AWS Architecture Center\nKết luận Trong bài viết này, chúng tôi đã trình bày một khuôn khổ toàn diện để xây dựng các triển khai AWS DMS linh hoạt. Hiệu quả của hướng dẫn này liên quan trực tiếp đến mức độ sâu sắc của việc triển khai và khả năng thích ứng với môi trường cụ thể của bạn. Chúng tôi đặc biệt khuyến khích các tổ chức xem xét kỹ lưỡng từng phần của hướng dẫn này và sử dụng nó làm nền tảng để phát triển một chiến lược di chuyển tùy chỉnh phù hợp với trường hợp sử dụng riêng của bạn.\nBằng cách đánh giá cẩn thận và kết hợp các khuyến nghị này vào quy trình lập kế hoạch di chuyển, bạn có thể phát triển một phương pháp tiếp cận toàn diện và đáng tin cậy để sử dụng AWS DMS, tạo điều kiện cho sự thành công lâu dài trong các chiến lược di chuyển dữ liệu của bạn.\nĐể được hỗ trợ và tài nguyên bổ sung, hãy truy cập AWS DMS documentation và tham gia với AWS Support.\nGiới thiệu về tác giả Sanyam Jain là Kỹ sư Cơ sở dữ liệu của nhóm Dịch vụ Di chuyển Cơ sở dữ liệu AWS (AWS DMS). Anh hợp tác chặt chẽ với khách hàng, cung cấp hướng dẫn kỹ thuật để di chuyển khối lượng công việc tại chỗ lên Đám mây AWS. Ngoài ra, anh còn đóng vai trò then chốt trong việc nâng cao chất lượng và chức năng của các sản phẩm di chuyển dữ liệu AWS.\nSushant Deshmukh là Kiến trúc sư Giải pháp Đối tác Cấp cao, làm việc với các Nhà tích hợp Hệ thống Toàn cầu. Anh đam mê thiết kế các kiến trúc có tính khả dụng cao, khả năng mở rộng, linh hoạt và bảo mật trên AWS. Anh hỗ trợ Khách hàng và Đối tác AWS di chuyển và hiện đại hóa thành công các ứng dụng của họ lên Đám mây AWS. Ngoài công việc, anh thích đi du lịch khám phá những địa điểm và nền ẩm thực mới, chơi bóng chuyền và dành thời gian chất lượng cho gia đình và bạn bè.\nAlex Anto là Kiến trúc sư Giải pháp Chuyên gia Di chuyển Dữ liệu thuộc nhóm Amazon Database Migration Accelerator tại Amazon Web Services. Anh làm việc với tư cách là Cố vấn Amazon DMA, hỗ trợ khách hàng AWS di chuyển dữ liệu tại chỗ sang các giải pháp cơ sở dữ liệu AWS Cloud.\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Các chỉ số AWS KMS trong CloudWatch giúp bạn theo dõi và hiểu rõ hơn về mức độ sử dụng khóa KMS Bởi Norman Li and Haiyu Zhen | ngày 17 tháng 3 năm 2025 | trong AWS Key Management Service, Best Practices, Intermediate (200), Security, Identity, \u0026amp; Compliance, Technical How-to |\nAWS Key Management Service (AWS KMS) hân hạnh ra mắt tính năng lọc cấp độ khóa cho việc sử dụng API AWS KMS trong số liệu của Amazon CloudWatch, cung cấp khả năng hiển thị nâng cao để giúp khách hàng cải thiện hiệu quả hoạt động và hỗ trợ quản lý rủi ro bảo mật và tuân thủ.\nAWS KMS hiện đang công bố số liệu sử dụng API AWS KMS ở cấp độ tài khoản với Amazon CloudWatch, cho phép bạn theo dõi và quản lý việc sử dụng API. Tuy nhiên, nếu bạn đang sử dụng nhiều khóa KMS, việc xác định chính xác khóa nào đang sử dụng nhiều nhất hạn ngạch về tần suất yêu cầu (request rate quota) hoặc gây ra chi phí API đáng kể sẽ trở nên khó khăn. Ví dụ: nếu bạn có hơn 10 khóa KMS đang hoạt động trong tài khoản, trước khi tính năng này ra mắt, bạn sẽ cần xây dựng một giải pháp tùy chỉnh dựa trên CloudTrail và Amazon Athena để xác định khóa nào đang chiếm phần lớn mức sử dụng và chi phí API. Với các chỉ số CloudWatch mới có trong không gian tên (namespace) AWS/KMS trong CloudWatch, bạn có thể theo dõi, tìm hiểu và đặt cảnh báo về mức sử dụng API chi tiết ở cấp độ khóa KMS riêng lẻ mà không cần xây dựng giải pháp tùy chỉnh tốn kém.\nBài viết trên blog này khám phá một số trường hợp sử dụng giúp bạn tận dụng tốt hơn các số liệu CloudWatch mới được giới thiệu này để quản lý việc sử dụng và chi phí API AWS KMS. Các trường hợp sử dụng bao gồm việc xem và hiểu rõ việc sử dụng API của bạn ở cấp độ chính, cũng như tạo cảnh báo CloudWatch để phát hiện việc sử dụng vượt mức ngoài ý muốn.\nTổng quan về số liệu CloudWatch mới cho khóa KMS Với số liệu CloudWatch cho khóa KMS, giờ đây bạn có thể thực hiện các thao tác sau:\nXem mức sử dụng API cho một khóa KMS cụ thể, được lọc theo từng thao tác API (ví dụ:Encrypt, Decrypt, or GenerateDataKey). Xem mức sử dụng tổng hợp trên các thao tác mã hóa cho một khóa KMS nhất định. Thiết lập cảnh báo nếu một khóa KMS cụ thể vượt quá ngưỡng quy định trên một thao tác API đơn lẻ hoặc một tập hợp các thao tác API. Phương pháp tiếp cận hợp lý này cho phép bạn nhanh chóng theo dõi, hiểu và khắc phục sự cố các mẫu sử dụng API của khóa KMS mà không cần phải thực hiện quy trình nhiều bước như trước đây. Hãy cùng tìm hiểu chi tiết cách sử dụng các số liệu sử dụng API cấp độ khóa này trong hai ví dụ thực tế.\nVí dụ 1: Cách xác định các khóa KMS tiêu thụ nhiều hạn ngạch sử dụng API nhất hoặc đóng góp nhiều phí API nhất Khi vượt quá hạn mức yêu cầu API KMS của AWS, bạn có thể xem mức sử dụng API KMS của AWS trong Service Quotas console. Tuy nhiên, bạn vẫn có thể gặp khó khăn khi xác định các khóa KMS chiếm nhiều hạn mức yêu cầu nhất. Khi nhận được phí API KMS của AWS vượt quá dự kiến, bạn có thể kiểm tra mức sử dụng chi tiết trong từng Khu vực AWS trong Cost Explorer, nhưng bạn không thể dễ dàng tìm thấy các khóa KMS có mức phí API cao nhất. Quá trình này càng trở nên khó khăn hơn khi bạn quản lý một số lượng lớn khóa KMS.\nVới các chỉ số CloudWatch về mức sử dụng API cấp độ khóa, bạn có thể sử dụng tùy chọn truy vấn chỉ số (metric query) nâng cao để truy vấn dữ liệu CloudWatch Metrics Insights bằng phương ngữ SQL thân thiện với người dùng để xác định các khóa KMS chiếm phần lớn hạn ngạch sử dụng API hoặc đóng góp nhiều phí API nhất.\nWalkthrough Để sử dụng Amazon CloudWatch Metrics Insights nhằm xác định 20 khóa KMS hàng đầu có mức sử dụng API mã hóa nhiều nhất trong 3 giờ qua, hãy hoàn thành các bước sau:\nMở CloudWatch console.\nTrong ngăn điều hướng, chọn Metrics (Số liệu), rồi chọn All metrics (Tất cả số liệu).\nChọn tab Multi source query (Truy vấn đa nguồn).\nĐối với nguồn dữ liệu, chọn CloudWatch Metrics Insights (Thông tin chi tiết về số liệu CloudWatch).\nBạn có thể nhập truy vấn ví dụ sau trong chế độ xem Editor:\nLưu ý: Trong chế độ xem Builder, các tùy chọn không gian tên số liệu, tên số liệu, lọc theo, nhóm theo, sắp xếp theo và giới hạn sẽ được hiển thị. Trong chế độ xem Editor, các tùy chọn tương tự như trong chế độ xem Builder được hiển thị ở định dạng truy vấn.\nSELECT SUM(SuccessfulRequest) FROM SCHEMA(\u0026quot;AWS/KMS\u0026quot;, KeyArn, Operation) GROUP BY KeyArn ORDER BY MAX () DESC LIMIT 20 Chọn Run (Chạy) trong chế độ xem Editor hoặc Graph query (Truy vấn đồ thị) trong chế độ xem Builder.\nVí dụ 2: Cách thiết lập cảnh báo chi tiết mới về việc sử dụng API AWS KMS ngoài ý muốn Việc chạy các quy trình xử lý dữ liệu lớn đọc các tệp Amazon Simple Storage Service (Amazon S3) được mã hóa bằng khóa KMS là một tình huống phổ biến đối với các dự án phân tích, báo cáo kinh doanh hoặc học máy. Thông thường, các quy trình này chỉ đọc một số lượng tệp giới hạn từ S3 trong mỗi lần gọi. Tuy nhiên, các quy trình được cấu hình sai có thể vô tình đọc một số lượng lớn tệp S3, dẫn đến việc vượt quá hạn ngạch tần suất yêu cầu (request rate quotas) API AWS KMS của bạn hoặc phát sinh các khoản phí không mong muốn do mức sử dụng API AWS KMS tăng vọt (spiky). Trước đây, để giải quyết vấn đề này, bạn sẽ phải xây dựng một hệ thống cảnh báo tùy chỉnh bằng cách làm theo các bước sau: 1) gửi các sự kiện AWS CloudTrail do AWS KMS tạo ra đến Amazon CloudWatch Logs; 2) viết các truy vấn trong Amazon CloudWatch Logs Insights để theo dõi mức sử dụng yêu cầu API của bạn; và 3) bật phát hiện bất thường trên biểu thức toán học CloudWatch Log Insights tương ứng.\nGiờ đây, với số liệu CloudWatch về mức sử dụng API ở cấp độ khóa, bạn có thể trực tiếp bật phát hiện bất thường trên các số liệu này để thiết lập cảnh báo cho các mẫu sử dụng API AWS KMS bất thường. Điều này cung cấp một phương pháp hợp lý và hiệu quả hơn để giám sát và phát hiện các quy trình làm việc tiềm ẩn rủi ro. Bằng cách sử dụng các số liệu CloudWatch và khả năng phát hiện bất thường này, bạn có thể chủ động xác định và xử lý các sự gia tăng ngoài ý muốn trong việc sử dụng API AWS KMS, giúp tránh các khoản phí bất ngờ hoặc gián đoạn dịch vụ trong quy trình phân tích, báo cáo hoặc học máy của bạn.\nWalkthrough Hãy xem xét một tình huống trong đó bạn có một quy trình phân tích chạy thường xuyên, sử dụng Decrypt AWS KMS API trên khóa KMS để giải mã và đọc dữ liệu từ S3. Bạn muốn bật tính năng phát hiện bất thường trên khóa KMS để kích hoạt báo động khi Decrypt gọi đến khóa KMS cụ thể phát hiện một xu hướng hoặc mẫu hình rõ ràng. Để thực hiện việc này, hãy hoàn thành các bước sau:\nMở CloudWatch console. Trong ngăn điều hướng, chọn Metrics (Số liệu), rồi chọn All metrics (Tất cả số liệu). Chọn KMS, rồi chọn KeyArn, Operation. Trong thanh tìm kiếm, nhập Tên tài nguyên Amazon (ARN) của khóa, rồi chọn Search (Tìm kiếm). Chọn số liệu CloudWatch mà bạn muốn bật phát hiện bất thường. Điều hướng đến Graphed metrics (Số liệu biểu đồ), và sử dụng danh sách thả xuống Statistic (Thống kê) và Period (Kỳ hạn), chọn số liệu thống kê và period (Kỳ hạn) mà bạn muốn theo dõi. Sau đó, bạn có thể bật phát hiện bất thường bằng cách chọn biểu tượng Pulse. Bạn có thể điều chỉnh phát hiện bất thường bằng cách đặt độ nhạy để điều chỉnh băng thông, nếu cần. Conclusion Bài đăng trên blog này đã nêu bật khả năng lọc cấp khóa mới được giới thiệu cho việc sử dụng API AWS KMS trong CloudWatch. Chúng tôi đã trình bày hai trường hợp sử dụng thực tế để minh họa cách bạn có thể sử dụng các số liệu CloudWatch mới. Các trường hợp sử dụng này bao gồm cải thiện khả năng hiển thị hoạt động, thiết lập cảnh báo chủ động về các bất thường trong mô hình sử dụng API KMS và có khả năng theo dõi việc sử dụng khóa chi tiết cho mục đích tuân thủ.\nNếu bạn có phản hồi về bài viết này, vui lòng gửi ý kiến trong phần Bình luận bên dưới. Nếu bạn có thắc mắc về bài viết này, vui lòng tạo một chủ đề mới trong AWS Key Management Service re:Post.\nNorman LiNorman là Giám đốc Phát triển Phần mềm cho AWS KMS. Trong vai trò này, Norman lãnh đạo việc phát triển các tính năng hiển thị (visibility features), cũng như các sáng kiến mở rộng quy mô nội bộ. Ngoài giờ làm việc, Norman thích dành thời gian ở những ngọn núi tuyệt đẹp vùng Tây Bắc Thái Bình Dương. Haiyu ZhenHaiyu là Kỹ sư Phát triển Phần mềm Cao cấp cho AWS KMS. Cô chuyên xây dựng các hệ thống phân tán quy mô lớn, bảo mật và đam mê tăng cường bảo mật ứng dụng cloud-native mà không làm ảnh hưởng đến hiệu suất. TAGS: AWS Key Management Service, AWS Key Management Service (KMS), Security Blog\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Bài thu hoạch “AWS Cloud Mastery Series #1 workshop” Mục Đích Của Sự Kiện Giới thiệu tổng quan về hệ sinh thái AWS AI/ML Services, đặc biệt là Amazon SageMaker.\nCung cấp kiến thức về quy trình xây dựng Machine Learning end-to-end từ chuẩn bị dữ liệu đến triển khai mô hình.\nTrình bày chuyên sâu về Generative AI với Amazon Bedrock và các Foundation Models nổi bật như Claude, Llama và Titan.\nHướng dẫn kỹ thuật Prompt Engineering, RAG architecture và Bedrock Agents cho các workflow đa bước.\nDanh Sách Diễn Giả AWS AI/ML Specialist Team – AWS Vietnam AWS Solutions Architect AI/ML Community Leader in Vietnam Nội Dung Nổi Bật AWS AI/ML Services Overview – Amazon SageMaker Amazon SageMaker được giới thiệu như một nền tảng ML toàn diện, hỗ trợ đầy đủ lifecycle của Machine Learning.\nData preparation \u0026amp; labeling: Giới thiệu SageMaker Data Wrangler, Ground Truth, giúp chuẩn hóa và gắn nhãn dữ liệu hiệu quả.\nModel training, tuning, deployment: Workflow huấn luyện mô hình, tối ưu (hyperparameter tuning) và triển khai trên endpoints.\nMLOps tích hợp: SageMaker Pipelines \u0026amp; Model Registry giúp tự động hóa CI/CD cho ML models.\nGenerative AI with Amazon Bedrock Foundation Models Overview\nSo sánh Claude, Llama và Titan theo các tiêu chí: context length, chất lượng reasoning, tốc độ suy luận, độ tùy biến trong fine-tuning và tính phù hợp với từng use case. Prompt Engineering Techniques\nỨng dụng Chain-of-Thought, few-shot prompting, cấu trúc prompt rõ ràng để tăng độ chính xác. RAG (Retrieval-Augmented Generation)\nGiải thích kiến trúc RAG và cách tích hợp Knowledge Base trong Amazon Bedrock để tối ưu câu trả lời dựa trên dữ liệu doanh nghiệp. Bedrock Agents\nGiới thiệu khả năng tạo agents để thực thi workflows đa bước và tích hợp với hệ thống bên ngoài. Những Gì Học Được Kiến Thức Về AI/ML Truyền Thống (Machine Learning) Hiểu rõ quy trình chuẩn của một dự án ML: thu thập dữ liệu, xử lý, huấn luyện, tối ưu, triển khai, MLOps.\nBiết cách sử dụng Amazon SageMaker như một nền tảng ML chuẩn hóa, phù hợp với doanh nghiệp.\nKiến Thức Về Generative AI Khả năng phân tích và lựa chọn Foundation Model phù hợp với bài toán.\nKỹ năng viết prompt hiệu quả bằng các kỹ thuật nâng cao (CoT, few-shot).\nNắm vững kiến trúc RAG — nền tảng quan trọng trong xây dựng chatbot doanh nghiệp.\nHiểu cơ chế hoạt động của Bedrock Agents và cách tự động hóa workflow.\nKỹ Năng Triển Khai Hiểu rõ cách kết hợp Bedrock + SageMaker trong hệ thống ML tích hợp.\nNắm được cách xây dựng chatbot GenAI thực tế cùng với guardrails để đảm bảo an toàn.\nỨng Dụng Vào Công Việc Xây dựng GenAI chatbot: Có thể áp dụng kiến thức Bedrock Agents và RAG cho các dự án chatbot nội bộ. Tối ưu Prompt Engineering: Sử dụng CoT, few-shot vào workflow hiện tại để tăng độ chính xác. Nâng cấp kiến trúc AI: Hiểu rõ hơn cách kết hợp MLOps và GenAI vào sản phẩm thực tế. Trải nghiệm trong event Tham gia workshop là một trải nghiệm giàu giá trị:\nKiến thức sâu rộng, dễ hiểu\nGiảng viên trình bày rõ ràng, từ kiến thức ML truyền thống đến GenAI hiện đại. Demo thực tế giúp dễ hình dung\nSageMaker Studio và Bedrock Agent được demo trực tiếp, giúp hiểu cách build end-to-end. Học nhiều kỹ thuật mới\nRAG, prompt engineering nâng cao, guardrails đều là kỹ năng quan trọng trong AI hiện đại. Cơ hội trao đổi \u0026amp; đặt câu hỏi\nBuổi workshop tạo điều kiện giao lưu với người làm AI/ML khác, mở rộng network và tư duy về ứng dụng AI trong thực tiễn. Bài học rút ra Machine Learning và GenAI cần được kết hợp để tạo ra ứng dụng mạnh mẽ hơn.\nRAG architecture quan trọng trong các ứng dụng AI doanh nghiệp.\nBedrock Agents có thể tự động hóa nhiều luồng công việc phức tạp.\nSageMaker giúp chuẩn hóa và tự động hóa quy trình ML, đặc biệt trong các tổ chức lớn.\nMột số hình ảnh khi tham gia sự kiện "},{"uri":"https://tranvanan5.github.io/internship-report/vi/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Bài thu hoạch “BUILDING AGENTIC AI” Mục Đích Của Sự Kiện Tối ưu context với Amazon Bedrock Xây dựng các AI agent automation với Amazon Bedrock thông qua các kỹ thuật thực hành và các trường hợp sử dụng trong thế giới thực Danh Sách Diễn Giả Nguyen Gia Hung - Head of Solutions Architect, AWS Kien Nguyen - Solutions Architect, AWS Viet Pham - Founder \u0026amp; CEO, Diagflow Kha Van - Community Leader, AWS Thang Ton - Co-Founder \u0026amp; COO, Cloud Thinker Henry Bui - Head of Engineering, Cloud Thinker Nội Dung Nổi Bật Các kỹ thuật tối ưu hóa chi phí và hiệu suất cho các hệ thống AI agent Thời gian release sản phẩm lâu → Mất doanh thu/bỏ lỡ cơ hội Hoạt động kém hiệu quả → Mất năng suất, tốn kém chi phí Không tuân thủ các quy định về bảo mật → Mất an ninh, uy tín Bốn kỹ thuật “Quick Wins” để tối ưu hóa Prompt Caching (Lưu trữ bộ nhớ đệm cho Prompt) Đây là kỹ thuật quan trọng nhất được đề cập, có thể giảm 70-90% chi phí và tăng tốc độ xử lý: Cấu trúc Context: Context window được chia thành 3 phần: (1) System \u0026amp; Tool Schema, (2) Conversation History (Lịch sử hội thoại), và (3) Objective Prompt (Mục tiêu hiện tại). Sai lầm thường gặp: Đa số mọi người chỉ cache phần System Prompt và Tool. Tuy nhiên, phần Conversation History mới là phần tốn kém nhất (có thể chiếm 80-90% chi phí) và thường bị bỏ qua. Chiến lược đúng: Cần đặt “checkpoint” sao cho toàn bộ lịch sử hội thoại cũng được cache. Mặc dù lần chạy đầu tiên (cache write) tốn thêm 25% chi phí, nhưng các lần sau sẽ tiết kiệm được 90%\nContext Compaction (Nén ngữ cảnh - Summarization) Cloud Thinker đã chỉ ra 1 kỹ thuật tóm tắt (Summarization) thông minh để tránh mất cache Cách cũ: Tạo một agent mới để tóm tắt hội thoại cũ. Cách này làm mất toàn bộ cache trước đó và thường làm giảm chất lượng (performance degradation). Cách mới (Cloudthinker technique): Giữ nguyên agent và lịch sử hội thoại (để tận dụng cache đã có), chỉ thay đổi phần “Objective Prompt” thành một task mới là “hãy tóm tắt đoạn hội thoại này”. Cách này giúp tận dụng cache hit, giảm chi phí tóm tắt từ ví dụ 0.3 đô xuống còn 0.03 đô (giảm ~90%) và cải thiện chất lượng đầu ra\nTool Consolidation (Hợp nhất công cụ) Vấn đề với các giao thức mới như MCP (Model Context Protocol) là việc đưa quá nhiều công cụ (ví dụ 50 tool) vào context sẽ làm tràn bộ nhớ (context flooding). Giải pháp: Thay vì đưa toàn bộ schema (cấu trúc dữ liệu) phức tạp vào prompt, hãy sử dụng một “dictionary” đơn giản và gộp các instruction (hướng dẫn) lại. Just-in-time Instruction: Agent có thể sử dụng một lệnh đặc biệt (get instruction) để lấy hướng dẫn chi tiết về cách dùng công cụ chỉ khi cần thiết. Điều này giúp giảm số lượng token phải nạp vào input liên tục\nParallel Tool Calling (Gọi công cụ song song) Thay vì chạy tuần tự (sequential) như mô hình ReAct cũ (năm 2022), các model hiện đại cho phép chạy song song nhiều tool cùng lúc để tiết kiệm thời gian. Tuy nhiên, tính năng này thường không bật mặc định; lập trình viên cần thêm các câu lệnh instruction cụ thể (ví dụ: “maximize efficiency”) để ép model chạy song song\nNhững Gì Học Được Chiến lược quản lý chi phí Input cost: chiếm phần lớn chi phí vận hành AI agents chạy loop. Tức là mỗi vòng, Agent phải nạp lại toàn bộ context (Conversation history, system prompt, memory) Nên loop Agent chạy càng lâu thì sẽ càng đốt tiền -\u0026gt; History càng dài thì input tokens mỗi vòng càng tăng. Solution: dùng Prompt Caching và checkpointing để giảm thiểu chi phí này. Giảm cost lên đến 80-90%. Kỹ thuật “Tóm tắt” (Summarization) thông minh Summarization: Giữ nguyên agent hiện tại và lịch sử hội thoại để tận dụng cache đã có, và giữ được context quality tốt hơn. Với kỹ thuật này, chi phí tóm tắt giảm từ 0.3 đô xuống còn 0.03 đô (giảm ~90%) và chất lượng đầu ra được cải thiện.\nThiết kế công cụ (Tool Design): Tránh “ngập lụt” ngữ cảnh Tool Design: Vấn đề: Quá nhiều tools được đưa vào (ví d: MCP với 50 tools) sẽ làm tràn ngữ cảnh (context flooding). Giải pháp: Tạo 1 lệnh đặc biệt để Agent tự gọi đến lấy instruction chi tiết khi cần, thay vì nhồi nhét toàn bộ schema vào prompt. Giúp context gọn nhẹ, giảm token usage và tăng hiệu suất. Tối ưu hiệu suất: Bắt buộc chạy song song (Parallel Tool Calling) Maximize Efficiency: thêm hướng dẫn cụ thể vào prompt để ép model Implement các tác vụ song song thay vì tuần tự. Trải nghiệm trong event Tham gia workshop “Building Agentic AI” là một trải nghiệm rất thú vị, nâng cao kiến thức về Agentic AI. Một số trải nghiệm nổi bật:\nHọc hỏi từ các diễn giả có chuyên môn cao Các diễn giả đến từ AWS, Cloudthinker, Diaflow đã chia sẻ best practices trong thiết kế ứng dụng hiện đại. Một số hình ảnh khi tham gia sự kiện "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.1-introduction/","title":"Giới thiệu","tags":[],"description":"","content":"Tuyên bố vấn đề Các hệ thống chatbot truyền thống gặp khó khăn khi không có khả năng truy cập thông tin cụ thể từ tài liệu nội bộ, dẫn đến trả lời không chính xác hoặc không liên quan. Workshop này giải quyết vấn đề bằng cách xây dựng một kiến trúc có khả năng:\nTự động hóa: Xử lý và đánh chỉ mục tài liệu PDF tự động Hỏi nội dung: Nhận truy vấn và điều hướng người dùng đến nội dung liên quan Truy xuất tài liệu: Trả lời các câu hỏi phức tạp với trích dẫn nguồn chính xác từ tài liệu Kiến trúc giải pháp Hệ thống được thiết kế theo mô hình RAG (Retrieval-Augmented Generation) kết hợp với AWS Serverless để đảm bảo khả năng mở rộng:\nFrontend Interface: Người dùng tương tác qua React Web Application\nAmazon API Gateway nhận requests từ Frontend AWS Amplify hosting với CloudFront CDN Amazon Cognito xử lý authentication Request Handling:\nApplication Load Balancer định tuyến traffic đến EC2 FastAPI Backend xử lý REST API requests Amazon SQS (FIFO) đảm bảo thứ tự xử lý documents Backend Processing:\nChatHandler: Quản lý hội thoại, lưu session vào Amazon DynamoDB RAG Service: Orchestrate vector search và LLM generation Qdrant Vector Database: Self-hosted trên EC2 cho vector search AI \u0026amp; Data Layer:\nAmazon Bedrock: Sử dụng Claude 3.5 Sonnet (LLM) và Cohere Embed Multilingual v3 (Embeddings) Amazon Textract: OCR và trích xuất text từ PDF Amazon S3: Lưu trữ documents Amazon DynamoDB: Metadata và chat history Admin Dashboard:\nReact-based interface hosted trên AWS Amplify Upload và quản lý documents Monitor processing status View chat history Architect Key Technologies Trong workshop này, bạn sẽ làm việc với các dịch vụ AWS chính sau:\nAmazon Bedrock: Trái tim của AI, cung cấp các Foundation Models (Claude, Cohere) để xử lý ngôn ngữ và sinh embeddings Amazon Textract: Xây dựng IDP pipeline để trích xuất text từ PDF documents Amazon EC2 \u0026amp; VPC: Cơ sở hạ tầng compute và network cho backend services Amazon S3: Lưu trữ documents và static assets Amazon DynamoDB: Lưu trữ metadata, chat history và document status Amazon Cognito: Authentication và user management AWS Amplify: Hosting frontend application với CI/CD tích hợp Amazon SQS: Message queue cho document processing pipeline Qdrant (Self-hosted): Vector database cho semantic search Terraform (IaC): Triển khai toàn bộ hạ tầng dưới dạng mã "},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/","title":"Nhật ký công việc","tags":[],"description":"","content":"Tuần 1: Làm quen với AWS và các dịch vụ cơ bản trong AWS\nTuần 2: Làm công việc A\u0026hellip;\nTuần 3: Làm công việc B\u0026hellip;\nTuần 4: Làm công việc C\u0026hellip;\nTuần 5: Làm công việc D\u0026hellip;\nTuần 6: Làm công việc E\u0026hellip;\nTuần 7: Làm công việc G\u0026hellip;\nTuần 8: Làm công việc H\u0026hellip;\nTuần 9: Làm công việc I\u0026hellip;\nTuần 10: Làm công việc L\u0026hellip;\nTuần 11: Làm công việc M\u0026hellip;\nTuần 12: Làm công việc N\u0026hellip;\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.1-week1/","title":"Worklog Tuần 1","tags":[],"description":"","content":"Mục tiêu tuần 1: Kết nối, làm quen với các thành viên trong First Cloud Journey. Hiểu dịch vụ AWS cơ bản, cách dùng console \u0026amp; CLI. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Đọc và nắm rõ các quy định của FCJ. - Tìm hiểu thêm về Cloud, AI, DevOps. 08/09/2025 08/09/2025 3 - Khám phá và tìm hiểu về những gì mình sẽ học và Cấu trúc hạ tầng của AWS 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tạo AWS Free Tier account - Tìm hiểu công cụ quản lý AWS service - Thực hành: + Tạo AWS account + Thiết lập với Virtual MFA Service + Tạo Admin Group và Admin User + Hỗ trợ xác thực tài khoản 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Quản lí chi phí với AWS Budget -Thực hành: + Tạo Budget + Tạo Cost Budget + Tạo Usage Budget + Tạo RI Budget + Tạo Saving Plans Budget + Dọn Dẹp Tài nguyên 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Sử dụng AWS Support - Tìm hiểu các gói hỗ trợ của AWS -Thực hành:\n+ Truy cập AWS Support\n+ Quản lí yêu cầu hỗ trợ 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 1: Hiểu rõ AWS là gì và cấu trúc toàn cầu của AWS.\nĐã tạo và cấu hình AWS Free Tier account thành công.\nLàm quen với AWS Management Console và biết cách tìm, truy cập, sử dụng dịch vụ từ giao diện web.\nLàm việc với AWS Budget để quản lí và tối ưu chi phí.\nTìm hiểu và sử dụng AWS Support.\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.2-week2/","title":"Worklog Tuần 2","tags":[],"description":"","content":"Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu về VPC. + Subnets\n+ Route Table\n+ Internet Gateway\n+ NAT Gateway\n- Tính năng bảo mật VPC và Multi-VPC 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ 3 -Thực hành:\n+ Subnets\n+ Route Table\n+ Internet Gateway\n+ NAT Gateway + Tạo Security Group 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 -Thực hành: + Tạo máy chủ EC2 + Kiểm tra kết nối + Tạo NAT Gateway + Sử dụng Reachability Analyzer + Tạo EC2 Instance connect Endpoint 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 5 Tham Gia Cloud Day Việt Nam 2025 18/09/2025 18/09/2025 6 -Thực hành: + Thiết lập Hydrid DNS với Route 53 Resolver + Tạo Keypair + Khởi tạo CloudFormation Template + Cấu hình Security Group 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 2: Hiểu các khái niệm quan trọng trong Amazon VPC:\nVPC (Virtual Private Cloud) là gì Subnet (Public \u0026amp; Private subnet) Route Table Internet Gateway NAT Gateway \u0026hellip; Biết cách Thực hiện triển khai môi trường VPC\nTạo VPC\nTạo Subnet\nTạo Internet Gateway\nTạo Route Table\nTạo Security Group\nKích hoạt VPC Flow Logs\nBiết cách triển khai Amazon EC2 Instances\nTham gia Cloud Day Viet Nam 2025 để tiếp cận gần hơn với công nghệ, Cloud, và cập nhật xu hướng\n\u0026hellip;\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.3-week3/","title":"Worklog Tuần 3","tags":[],"description":"","content":"Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 -Thực hành: +Kết nối đến RDGW + Thiết lập DNS + Tạo Route 53 Outbound Endpoint + Tạo Route53 Resolver Rules + Tạo Route 53 Inbound Enpoints + Kiểm tra kết quả + Dọn dẹp tài nguyên 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ 3 -Thực hành: + Thiết lập VPC peering + Tổ chức CloudFormation Template + Tạo Security Group + Tạo máy chủ EC2 + Cập nhật Network ACLs + Tạo kết nối Peering + Cấu hình Route Table + Kích hoạt Cross-peer DNS + Dọn dẹp tài nguyên 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 4 -Thực hành: + Thiết lập AWS Transit Gateway + Tạo Keypair + Khởi tạo CloudFormation Template + Tạo Transit Gateway + Tạo Transit Gateway Attachments + Tạo Transit Gateway Route Table + Thêm Transit Gateway vào VPC Route Tables + Dọn dẹp tài nguyên 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Tìm hiểu 2 loại dịch vụ của AWS: + Compute + Storage\n-Thực hành: + Triển khai infastructure + Tạo Backup plan + Thiết Lập thông báp + Kiểm tra hoạt động + Dọn dẹp tài nguyên 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Thực hành: + Tạo S3 Bucket + Tạo EC2 cho Storage Gateway + Tạo Storage Gateway + Tạo File Shares + Kết nối File Shares ở máy On-premise + Dọn dẹp tài nguyên 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 3: kiến thức AWS Networking:\nRoute 53 Resolver (Inbound/Outbound)\nVPC Peering\nTransit Gateway\nRouting Tables, SG, Network ACLs\nTriển khai hạ tầng bằng CloudFormation\nNắm được Compute \u0026amp; Storage services của AWS\nHiểu Hybrid Cloud:\nAWS Backup Storage Gateway File Shares \u0026amp; S3 Integration Kỹ năng thực hành được củng cố:\nTriển khai – cấu hình – kiểm tra – cleanup tài nguyên Tư duy kiến trúc theo chuẩn AWS Best Practices "},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.4-week4/","title":"Worklog Tuần 4","tags":[],"description":"","content":"Mục tiêu tuần 4: Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu về Amazon S3: - S3 Bucket và S3 Object - Các chức năng chính của S3 - Ứng dụng phổ biến của S3 - Lợi ích. 29/09/2025 29/09/2025 3 - Thực hành: - Tạo S3 Bucket và tải dữ liệu - Bật tính năng static website - Cấu hình Block Public Access - Cấu hình public object - Kiểm Tra Website - Chặn tất cả truy cập công cộng vào S3 - Cấu hình Amazon CloudFront - Kiểm tra Amazon CloudFront 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tìm hiểu Bucket Versioning - Lợi Ích Chính của Versioning - Cách Versioning Hoạt Động - Trạng Thái Versioning - Thực hành: + Bật tính năng versioning cho bucket + Thay đổi nội dung file index.html + Kiểm tra tính năng versioning trên S3 + Kiểm tra tính năng versioning trên Cloudfront + Di chuyển Object 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Tìm hiểu Amazon S3 Cross-Region Replication (CRR): + Lợi Ích Chính của CRR + Cách CRR Hoạt Động + AWS Well-Architected Framework - Thực hành: + Sao chép S3 Object sang region khác + Dọn dẹp tài nguyên 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Tìm hiểu: + Dịch Vụ Lưu Trữ Trên AWS + Amazon Simple Storage Service ( S3 ) - Access Point - Storage Class + S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier + Snow Family - Storage Gateway - Backup 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 4: Tìm hiểu tổng quan Amazon S3:\nS3 Bucket và S3 Object Các chức năng chính của S3 Ứng dụng phổ biến của S3 Hiểu rõ vai trò của Amazon S3 trong kiến trúc AWS.\nHiểu được cơ chế hosting web tĩnh trên S3.\nBiết cách kết hợp S3 + CloudFront để tăng tốc độ và bảo mật.\nHiểu cơ chế kiểm soát phiên bản trong S3.\nBiết cách khôi phục dữ liệu khi file bị ghi đè hoặc xóa.\nHiểu cơ chế đồng bộ dữ liệu đa vùng tự động.\nTìm hiểu tổng quan toàn bộ nhóm dịch vụ lưu trữ AWS\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.5-week5/","title":"Worklog Tuần 5","tags":[],"description":"","content":"Mục tiêu tuần 5: Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tổng quan về AWS Back up - Thực hành: + Tạo S3 bucket và Triển khai cơ sở hạ tầng + Tạo Backup plan + Thiết lập thông báo + Kiểm tra hoạt động + Dọn dẹp tài nguyên 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Thực hành: Chuẩn bị máy ảo VMware Workstation + Export máy ảo từ On-premise + Tải máy ảo lên AWS + Import máy ảo vào AWS + Triển khai EC2 Instance từ AMI + Thiết lập ACL cho S3 Bucket + Export EC2 Instance từ AWS + Export máy ảo từ AMI + Dọn dẹp tài nguyên 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Thực hành: tạo S3 bucket và tạo EC2 Instance sử dụng AMI Storage Gateway + Tạo S3 bucket và Tạo EC2 cho Storage Gateway + Tạo Storage Gateway + Tạo File Shares + Kết nối File shares ở máy On-premise + Dọn dẹp tài nguyên 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Dịch Blog: + AWS Lambda giới thiệu định giá theo bậc (tiered pricing) cho Amazon CloudWatch Logs và các đích ghi nhật ký bổ sung + Hướng dẫn triển khai AWS DMS: Xây dựng khả năng di chuyển cơ sở dữ liệu linh hoạt thông qua thử nghiệm, giám sát và SOP + Các chỉ số AWS KMS trong CloudWatch giúp bạn theo dõi và hiểu rõ hơn về mức độ sử dụng khóa KMS 09/10/2025 09/10/2025 https://docs.google.com/document/d/1s2iY6wbT6nwUDZbXcRchLlcCgJqimc1dLxjrLUX2sGA/edit?usp=sharing https://docs.google.com/document/d/1ZRVDcmVuPFlyDtdTLLiKmqOcVoFf8E2cah8cqb4SjkU/edit?usp=sharing https://docs.google.com/document/d/1GPZh5zNGKg0pY4hYCeKLoZHnl4lnhSgyP6o_zgTcQPI/edit?usp=sharing 6 - Thực hành: Triển khai FSx trên Windows Tạo môi trường thực hành + Tạo một SSD Multi-AZ file system + Tạo một HDD Multi-AZ file system + Tạo file share + Kiểm tra hiệu năng + Giám sát hiệu năng + Kích hoạt chống dữ liệu trùng lặp + Quản lý Session người dùng và mở tệp + Kích hoạt hạn ngạch bộ nhớ của người dùng + Kích hoạt chia sẻ Truy cập liên tục + Mở rộng khả năng thông lượng + Mở rộng dung lượng lưu trữ + Dọn dẹp tài nguyên 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 5: Nắm được cách AWS Backup tự động sao lưu dữ liệu.\nHiểu được quy trình di chuyển hệ thống On-premise lên AWS.\nBiết cách export/import máy ảo sử dụng VM Import/Export service.\nHiểu Storage Gateway là cầu nối Hybrid Cloud.\nNắm cơ chế file caching và lưu trữ đám mây.\nHiểu rõ FSx dùng trong doanh nghiệp cho file sharing Windows.\nNắm cách scale performance và storage.\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.6-week6/","title":"Worklog Tuần 6","tags":[],"description":"","content":"Mục tiêu tuần 6: Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu Dịch vụ bảo mật trên AWS + Share Responsibility Model + Amazon Identity and access management + Amazon Cognito + AWS Organization + AWS Identity Center + Amazon Key Management Service + AWS Security Hub + Hands-on and Additional research 13/10/2025 13/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Tìm hiểu Security Standards - Thực hành: + Kích hoạt Security Hub + Điểm từng bộ tiêu chuẩn + Dọn dẹp tài nguyên - Thực hành: Tối ưu hóa chi phí EC2 với Lambda + Các bước chuẩn bị: 1. Tạo VPC 2. Tạo Security group 3. Tạo EC2 4. Cài đặt Web-Hooks đến Slack + Tạo Tag cho Instance + Tạo Role cho Lambda + Tạo Lambda Function 1. Tạo Lambda Function thực hiện chức năng Stop instances 2. Tạo Lambda Function thực hiện chức năng Start instances + Kiểm tra kết quả + Dọn dẹp tài nguyên 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tìm hiểu AWS Resource Groups - Thực hành: + Tạo EC2 Instance có tag + Quản lý Tags trong AWS Resources + Lọc tài nguyên theo tag + Sử dụng tag bằng CLI + Tạo một Resource Group + Dọn dẹp tài nguyên 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Thực hành: + Tạo IAM User + Tạo IAM Policy + Tạo IAM Role + Chuyển Role + Tiến hành truy cập EC2 console ở AWS Region - Tokyo + Tiến hành truy cập EC2 console ở AWS Region - North Virginia + Tiến hành tạo EC2 instance khi không có và có Tags thỏa điều kiện + Chỉnh sửa Resource Tag trên EC2 Instance + Kiểm tra chính sách + Don dẹp tài nguyên 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Tìm hiểu IAM Permission Boundary là gì - Thực hành: Giới hạn quyền của user với IAM Permission Bourdary + Tạo Policy Giới hạn + Tạo IAM User Giới Hạn + Kiểm tra IAM User Giới + Hạn Dọn dẹp tài nguyên 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 6: Hiểu về các dịch vụ bảo mật trên AWS\nShare Responsibility Model Amazon Identity and access management Amazon Cognito AWS Organization \u0026hellip; Nắm khái niệm Authentication/Authorization qua Cognito.\nNắm kiến thức của IAM, Security Hub, Organizations.\nSử dụng tagging và resource groups để quản lý tài nguyên.\nTối ưu chi phí bằng automation stop/start EC2.\nBiết cách sử dụng IAM Permission Boundary để giới hạn quyền của User\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.7-week7/","title":"Worklog Tuần 7","tags":[],"description":"","content":"Mục tiêu tuần 7: Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu về IAM - Thực hành: + Tạo IAM Group + Tạo IAM User + Kiểm tra quyền + Tạo Admin IAM Role + Cấu hình Switch role + Giới hạn truy cập theo IP + Giới hạn theo thời gian + Dọn dẹp tài nguyên 20/10/2025 20/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Thực hành: Cấp quyền cho ứng dụng với IAM Role + Tạo EC2 Instance + Tạo S3 bucket + Tạo IAM user và access key + Sử dụng access key + Tạo IAM role + Sử dụng IAM role + Dọn dẹp tài nguyên - Tìm hiểu về Database Concepts review 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tìm hiểu về: + Amazon RDS \u0026amp; Amazon Aurora + Redshift - Elasticache 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Ôn Tập giữa kì 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Ôn Tập giữa kì 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 7: Nắm được kiến thức về IAM.\nSử dụng IAM Role\nTạo EC2 instance Tạo S3 bucket để lưu trữ dữ liệu Tạo IAM User + Access Key, dùng để test quyền thủ công Tạo IAM Role và gán vào EC2 So sánh IAM User vs IAM Role trong việc cấp quyền Clean up toàn bộ tài nguyên Làm quen với AWS Management Console và biết cách tìm, truy cập, sử dụng dịch vụ từ giao diện web.\nTìm hiểu các dịch vụ database quan trọng của AWS:\nAmazon RDS – database truyền thống, fully managed Amazon Aurora – database hiệu năng cao tương thích MySQL/PostgreSQL Amazon Redshift – kho dữ liệu (data warehouse) cho phân tích lớn Amazon ElastiCache – caching in-memory (Redis/Memcached) Ôn tập các kiến thức để chuẩn bị cho giữa kì\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.8-week8/","title":"Worklog Tuần 8","tags":[],"description":"","content":"Mục tiêu tuần 8: Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Ôn Tập giữa kì 27/10/2025 27/10/2025 3 - Ôn Tập giữa kì 28/10/2025 28/10/2025 4 - Ôn Tập giữa kì 29/10/2025 29/10/2025 5 - Ôn Tập giữa kì 30/10/2025 30/08/2025 6 - Kiểm tra giữa kì 31/10/2025 31/10/2025 Kết quả đạt được tuần 8: Ôn tập các kiến thức để chuẩn bị cho giữa kì "},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.9-week9/","title":"Worklog Tuần 9","tags":[],"description":"","content":"Mục tiêu tuần 9: Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 Tìm hiểu về Amazon Bedrock 03/11/2025 03/11/2025 https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html 3 - Tìm hiểu RAG 04/11/2025 04/11/2025 https://aws.amazon.com/what-is/retrieval-augmented-generation/ 4 - Tìm hiểu Knowledge Base 05/11/2025 05/11/2025 https://www.atlassian.com/itsm/knowledge-management/what-is-a-knowledge-base 5 - Học và bổ sung kiến thức React, Javascript 06/11/2025 06/11/2025 6 - Học và bổ sung kiến thức React, Javascript 07/11/2025 07/11/2025 Kết quả đạt được tuần 9: Hiểu về Amazon Bedrock, RAG, Knowledge Base\nBiết căn bản các cú pháp và kiến thức cần thiết của React, Javascript\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/2-proposal/","title":"Bản đề xuất","tags":[],"description":"","content":"Academic Research Chatbot Giải pháp AWS RAG-based hỗ trợ học thuật và nghiên cứu học tập thông minh 1. Tóm tắt điều hành Academic Research Chatbot là trợ lý AI hỗ trợ nghiên cứu học thuật, giúp sinh viên và giảng viên tra cứu, tóm tắt và phân tích tài liệu khoa học (PDF, bài báo) thông qua hội thoại tự nhiên có trích dẫn nguồn chính xác.\nĐiểm nổi bật của giải pháp:\nCông nghệ lõi: Kết hợp IDP (Amazon Textract) để xử lý tài liệu (kể cả bản scan) và RAG (Amazon Bedrock - Claude 3.5 Sonnet) để sinh câu trả lời thông minh. Kiến trúc tối ưu: Mô hình Hybrid sử dụng 1 EC2 t3.small kết hợp các dịch vụ Serverless (Amplify, Cognito, S3, DynamoDB) để cân bằng hiệu năng và chi phí. Tính khả thi: Phục vụ ~50 người dùng nội bộ với chi phí vận hành ~60 USD/tháng, thời gian triển khai nhanh (20 ngày) và tận dụng tối đa AWS Free Tier. 2. Tuyên bố vấn đề Vấn đề hiện tại\nSinh viên và researcher phải làm việc với số lượng lớn tài liệu học thuật (paper hội nghị, journal, luận văn, báo cáo kỹ thuật). Nhiều tài liệu là scan PDF cũ (trước năm 2000), không có text layer, khiến việc tìm kiếm nội dung, số liệu, bảng biểu rất tốn thời gian. Các công cụ AI công cộng (ChatGPT, Perplexity, NotebookLM, v.v.) không được kết nối trực tiếp với kho tài liệu nội bộ của trường/khoa, khó đảm bảo bảo mật và quyền truy cập theo môn học hoặc nhóm nghiên cứu. Hạ tầng hiện tại không có một điểm truy cập thống nhất để:\nQuản lý tài liệu nghiên cứu theo bộ môn/đề tài. Cho phép researcher đặt câu hỏi trực tiếp trên chính các paper của mình. Đảm bảo câu trả lời có trích dẫn rõ ràng (paper, trang, bảng, mục). Hệ quả: nghiên cứu viên phải đọc thủ công, note tay, copy số liệu từ nhiều paper; giảng viên khó tổng hợp nhanh thông tin khi chuẩn bị bài giảng hoặc đề tài; dữ liệu học thuật phân tán trên nhiều máy cá nhân, khó chuẩn hóa và tái sử dụng. Giải pháp Academic Research Chatbot đề xuất xây dựng một nền tảng hỏi – đáp học thuật nội bộ dựa trên AWS, nơi:\nDev/Admin nạp kho tài liệu nghiên cứu: Upload PDF vào Amazon S3, metadata được lưu trong Amazon DynamoDB. Một EC2 worker tiêu thụ hàng đợi Amazon SQS, gọi Amazon Textract để OCR, trích xuất text, bảng, biểu mẫu, kể cả tài liệu scan. Worker chuẩn hóa/chunk nội dung, gửi sang Amazon Bedrock Titan Text Embeddings v2 để sinh embedding, và index vào Qdrant trên EC2. Researchers đặt câu hỏi qua giao diện web (Amplify + CloudFront): Câu hỏi được embed, truy vấn Qdrant để lấy các đoạn liên quan nhất (Retrieval). Các đoạn này được chuyển vào Claude 3.5 Sonnet trên Amazon Bedrock để sinh câu trả lời có citation chính xác (paper, page, section, table) và giải thích theo ngữ cảnh học thuật. Toàn bộ truy cập được bảo vệ bởi Amazon Cognito (phân quyền researcher vs admin), log \u0026amp; metric được giám sát qua Amazon CloudWatch + SNS (cảnh báo khi có lỗi worker, queue backlog, CPU EC2 cao). Lợi ích và hoàn vốn đầu tư (ROI)\nGiảm 40–60% thời gian researcher phải bỏ ra để tìm số liệu, F1-score, p-value, sample size, thiết bị thí nghiệm hoặc mô tả phương pháp từ nhiều paper khác nhau. Giảm sai sót khi trích dẫn do quên trang/bảng, vì chatbot luôn trả kèm nguồn và vị trí. Quản lý tri thức nội bộ:\nTài liệu nghiên cứu được tập trung về một kho S3 + DynamoDB, dễ backup, phân quyền, và mở rộng. Có thể tái sử dụng cho nhiều khoá học, đề tài và lab khác nhau mà không phải xây hệ thống mới. Chi phí hạ tầng thấp \u0026amp; dễ kiểm soát:\nMô hình hybrid 1 EC2 + managed AI services giúp chi phí vận hành cho 50 users nội bộ giữ ở mức khoảng \u0026lt; 50 USD/tháng, chủ yếu trả cho EC2, 2–3 VPC endpoint interface và phần sử dụng Bedrock/Textract. Hệ thống được thiết kế để triển khai trong khoảng 20 ngày bởi team 4 người, phù hợp làm dự án nghiên cứu/thực tập nhưng vẫn có chất lượng kiến trúc sản phẩm. Giá trị dài hạn:\nTạo nền tảng để sau này tích hợp thêm dashboard phân tích hành vi học tập, module recommend paper, hoặc mở rộng sang trợ lý học tập đa ngôn ngữ và đa lĩnh vực. 3. Kiến trúc giải pháp Academic Research Chatbot áp dụng mô hình AWS Hybrid RAG Architecture với IDP (Intelligent Document Processing), kết hợp một EC2 duy nhất (FastAPI + Qdrant + Worker) với các dịch vụ AI managed (Textract, Bedrock) để vừa tối ưu chi phí, vừa đảm bảo hiệu năng cho khoảng 50 người dùng nội bộ.\nLuồng xử lý dữ liệu và hội thoại\nDịch vụ AWS sử dụng\nFrontend: Route 53, CloudFront, Amplify (DNS, CDN, Host React App). Auth: Cognito (Xác thực \u0026amp; phân quyền researcher/admin). Compute: EC2 t3.small (FastAPI + Qdrant + Worker). AI/ML: Bedrock (Claude 3.5 Sonnet, Titan Embeddings v2). IDP: Textract (OCR cho PDF scan). Storage: S3, DynamoDB (File PDF gốc + Metadata/Status). Queue: SQS (Hàng đợi xử lý tài liệu). Network: VPC, ALB, VPC Endpoints (Bảo mật, routing, kết nối AWS Services). Monitoring: CloudWatch, SNS (Logs, Metrics, Alerts). CI/CD: CodePipeline, CodeBuild (Auto deploy backend). Thiết kế thành phần\nNgười dùng: Researchers: hỏi – đáp, tra cứu nội dung học thuật. Dev/Admin: upload, quản lý và re-index tài liệu. Xử lý tài liệu (IDP): PDF được Dev/Admin upload lên S3. Worker trên EC2 gọi Textract để OCR và trích xuất text/bảng. Lập chỉ mục (Indexing \u0026amp; Vector DB): Worker chuẩn hoá, chia chunk nội dung. Gọi Bedrock Titan Embeddings v2 tạo embedding. Lưu embedding + metadata vào Qdrant trên EC2. Hội thoại AI (RAG): FastAPI embed câu hỏi, truy vấn Qdrant lấy top-k đoạn liên quan. Gửi context + câu hỏi vào Claude 3.5 Sonnet (Bedrock) để sinh câu trả lời kèm citation. Quản lý người dùng: Cognito xác thực và phân quyền researcher / admin. Lưu trữ \u0026amp; trạng thái: DynamoDB lưu metadata tài liệu (doc_id, status, owner, …) và (tuỳ chọn) lịch sử chat. 4. Triển khai kỹ thuật Các giai đoạn triển khai\nDự án gồm 2 phần — thiết lập trạm thời tiết biên và xây dựng nền tảng thời tiết — mỗi phần trải qua 4 giai đoạn:\nNghiên cứu \u0026amp; chốt kiến trúc: Rà soát yêu cầu (50 researcher, 1 EC2, IDP + RAG). Chốt kiến trúc VPC, EC2 (FastAPI + Qdrant + Worker), Amplify, Cognito, S3, SQS, DynamoDB, Textract, Bedrock. POC \u0026amp; kiểm tra kết nối: Tạo EC2, VPC endpoints, thử gọi Textract, Titan Embeddings, Claude 3.5 Sonnet. Chạy Qdrant đơn giản trên EC2, test insert/search vector. Tạo skeleton FastAPI + một màn hình Chat UI tối giản trên Amplify. Hoàn thiện tính năng chính: Xây /api/chat (FastAPI) + RAG pipeline: embed query → Qdrant → Claude + citation. Xây /api/admin/: upload PDF, lưu S3 + DynamoDB, đưa message vào SQS. Viết Worker trên EC2: SQS → Textract → normalize/chunk → Titan → Qdrant → update DynamoDB. Hoàn thiện Chat UI và Admin UI (upload + xem trạng thái tài liệu). Kiểm thử, tối ưu, triển khai demo nội bộ: Test end-to-end với một tập ~50–100 paper. Thêm CloudWatch Logs/Alarms, SNS notify khi lỗi hoặc queue backlog. Điều chỉnh cấu hình EC2, Qdrant, batch size để tối ưu thời gian và chi phí. Chuẩn bị tài liệu hướng dẫn sử dụng và demo cho nhóm 50 researcher. Yêu cầu kỹ thuật\nFrontend \u0026amp; Auth: React/Next.js host trên AWS Amplify, CDN CloudFront, DNS Route 53. Amazon Cognito quản lý định danh và phân quyền (Researcher/Admin). Backend \u0026amp; Compute: EC2 t3.small (Private Subnet) chạy All-in-one: FastAPI, Qdrant Vector DB và Worker. Xử lý bất đồng bộ: Worker đọc SQS, kích hoạt Textract và Bedrock để index dữ liệu. IDP \u0026amp; RAG: Lưu trữ: S3 (File gốc), DynamoDB (Metadata \u0026amp; Trạng thái). AI Core: Textract (OCR tài liệu scan), Bedrock Titan (Embedding), Claude 3.5 Sonnet (Trả lời câu hỏi). Mạng \u0026amp; Observability: Network: VPC Private Subnet, VPC Endpoints để kết nối bảo mật tới AWS Services. Monitoring: CloudWatch Logs/Metrics + SNS cảnh báo sự cố (CPU cao, lỗi Worker). 5. Lộ trình \u0026amp; Mốc triển khai Dự án được thực hiện trong khoảng 6 tuần với các giai đoạn cụ thể:\nTuần 1-2 (Ngày 1-10): Nghiên cứu \u0026amp; Thiết kế Thiết kế kiến trúc chi tiết, xác định scope, dịch vụ sử dụng. Lên kế hoạch tối ưu chi phí vận hành và triển khai. Tuần 3 (Ngày 11-15): Thiết lập hạ tầng AWS Cấu hình VPC, Subnets, Security Groups, IAM Roles. Triển khai EC2 t3.small, S3 bucket, DynamoDB tables. Thiết lập VPC Endpoints (Gateway + Interface). Tuần 4 (Ngày 16-20): Backend APIs \u0026amp; IDP Pipeline Xây dựng FastAPI endpoints (/api/chat, /api/admin/upload). Tích hợp IDP pipeline: SQS → Worker → Textract → Embeddings → Qdrant. Kết nối Bedrock (Titan Embeddings + Claude 3.5 Sonnet). Tuần 5 (Ngày 21-25): Testing \u0026amp; Error Handling Kiểm thử end-to-end với tập ~50-100 papers. Xử lý edge cases, retry logic, error handling. Tối ưu chunking strategy và retrieval accuracy. Tuần 6 (Ngày 26-30): Deployment \u0026amp; Documentation Hoàn thiện UI/UX cho Admin và Researcher. Thiết lập CloudWatch Alarms + SNS notifications. Chuẩn bị tài liệu hướng dẫn và demo cho nhóm 50 researcher. 6. Ước tính ngân sách Chi phí hạ tầng (ước tính theo tháng)\nCompute \u0026amp; Storage: EC2 t3.small: $10.08 (720h). EBS gp3: $2.40 (30GB). Network: NAT Gateway: $21.60. VPC Interface Endpoints: $14.60 (2 endpoints cho Textract, Bedrock). VPC Gateway Endpoints: FREE (S3, DynamoDB). AI \u0026amp; Operations: Bedrock Claude 3.5 Sonnet: $25.00 (50 users). Bedrock Titan Embeddings: $0.75 (750 papers). CloudWatch + Data Transfer: $1.90. Free Tier (12 tháng đầu)\nWeb \u0026amp; Auth: S3, CloudFront, Cognito, Amplify (FREE). Serverless: DynamoDB, SQS, SNS (Always FREE). IDP: Textract AnalyzeDocument (100 pages/month trong 3 tháng đầu). Tổng cộng: ~$60-76/tháng (tùy mức sử dụng Bedrock).\n7. Đánh giá rủi ro Ma trận rủi ro\nHallucination (AI bịa đặt): Ảnh hưởng cao, xác suất trung bình. Vượt ngân sách (AI Services): Ảnh hưởng trung bình, xác suất trung bình. Sự cố hạ tầng (EC2/Qdrant): Ảnh hưởng cao, xác suất thấp. Chiến lược giảm thiểu\nChất lượng AI: Bắt buộc trích dẫn nguồn (citation), giới hạn context đầu vào từ Qdrant. Chi phí: Thiết lập AWS Budgets/Alarms, kiểm soát số lượng tài liệu ingest. Hạ tầng \u0026amp; Bảo mật: Backup EBS định kỳ, mã hóa dữ liệu (S3/DynamoDB), phân quyền chặt chẽ qua Cognito/IAM. Kế hoạch dự phòng\nSự cố hệ thống: Khôi phục từ Snapshot, tạm dừng ingestion (buffer qua SQS). Vượt chi phí: Tạm khóa tính năng upload mới, giới hạn hạn ngạch truy vấn trong ngày. 8. Kết quả kỳ vọng Cải tiến kỹ thuật:\nChuyển đổi kho tài liệu rời rạc (PDF/Scan) thành tri thức số có thể truy vấn và trích dẫn tự động. Giảm đáng kể thời gian tra cứu thủ công nhờ công nghệ RAG + IDP. Giá trị dài hạn Xây dựng nền tảng nghiên cứu số hóa cho 50+ researcher, dễ dàng mở rộng quy mô. Tạo tiền đề phát triển các tính năng nâng cao: Gợi ý tài liệu, phân tích xu hướng nghiên cứu và hỗ trợ viết tổng quan (Literature Review). "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.2-preparation/","title":"Các bước chuẩn bị","tags":[],"description":"","content":"Yêu cầu Tiên quyết Các yêu cầu cần có để thực hiện workshop này:\nMáy tính khách AWS: Được cấu hình với quyền truy cập vào các dịch vụ AWS cần thiết Môi trường phát triển: Windows, macOS hoặc Linux với các công cụ development cơ bản Kiến thức cơ bản: Hiểu biết về AWS, Python, JavaScript và Docker Tài khoản GitHub: Để clone source code và theo dõi changes Ngân sách AWS: Khoảng $65/tháng cho các resources (EC2, Bedrock, NAT Gateway) Cài đặt công cụ 1. AWS CLI AWS Command Line Interface (AWS CLI) là công cụ để tương tác với AWS services.\nWindows:\n# Download và cài đặt MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi macOS:\nbrew install awscli Linux:\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Verify installation:\naws --version # aws-cli/2.x.x Python/3.x.x 2. Terraform Terraform là công cụ Infrastructure as Code để provision AWS resources.\nWindows:\nchoco install terraform macOS:\nbrew tap hashicorp/tap brew install hashicorp/tap/terraform Linux:\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update \u0026amp;\u0026amp; sudo apt install terraform Verify:\nterraform --version 3. Docker Docker để chạy Qdrant vector database locally và trên EC2.\nWindows/macOS: Download Docker Desktop\nLinux:\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -aG docker $USER Verify:\ndocker --version docker run hello-world 4. Node.js (\u0026gt;= 18) Node.js cho frontend development với React + Vite.\nSử dụng nvm (khuyến nghị):\n# Install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash # Install Node.js 18 nvm install 18 nvm use 18 Verify:\nnode --version # v18.x.x npm --version # 9.x.x 5. Python (\u0026gt;= 3.11) Python cho backend FastAPI application.\nWindows: Download từ python.org (chọn \u0026ldquo;Add to PATH\u0026rdquo;)\nmacOS:\nbrew install python@3.11 Linux:\nsudo apt update sudo apt install python3.11 python3.11-venv python3-pip Verify:\npython --version # Python 3.11.x pip --version 6. Git Git cho version control.\nWindows: Download từ git-scm.com\nmacOS:\nbrew install git Linux:\nsudo apt install git Verify:\ngit --version Clone Repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Cấu hình AWS Credentials Tạo IAM User Đăng nhập AWS Console Navigate to IAM → Users → Create user User name: arc-workshop-user Attach policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Create access key → Download credentials Configure AWS CLI aws configure Nhập thông tin:\nAWS Access Key ID: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name: ap-southeast-1 Default output format: json Verify:\naws sts get-caller-identity Kích hoạt Amazon Bedrock Models Request Model Access AWS Console → Amazon Bedrock → Model access Click Manage model access Chọn models: Anthropic - Claude 3.5 Sonnet (anthropic.claude-3-5-sonnet-20241022-v2:0) Cohere - Embed Multilingual v3 (cohere.embed-multilingual-v3) Click Request model access → Accept Terms → Submit Verify Access # Test Claude aws bedrock get-foundation-model \\ --model-identifier anthropic.claude-3-5-sonnet-20241022-v2:0 \\ --region ap-southeast-1 # Test Cohere aws bedrock get-foundation-model \\ --model-identifier cohere.embed-multilingual-v3 \\ --region ap-southeast-1 Expected: Status Access granted\nChuẩn bị Sample Documents Project có sẵn sample PDFs trong samples/:\nls samples/ # data-structures-sample.pdf # test-sample.pdf Yêu cầu documents Limit Value Format PDF (text-based hoặc scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Checklist Trước khi tiếp tục, đảm bảo:\nAWS CLI installed và configured Terraform installed Docker installed và running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created với đủ permissions Bedrock models được approve (Claude + Cohere) Sample documents sẵn sàng "},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.10-week10/","title":"Worklog Tuần 10","tags":[],"description":"","content":"Mục tiêu tuần 10: Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu về Amazon S3 + Access Point + Storage Class - Tìm hiểu về AWS Security Hub 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Tìm hiểu Amazon DynamoDB - Tìm và ôn lại AWS Backup 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Thực hành: + Cài đặt Bedrock + Claude 3.5 Sonnet + Kỹ thuật viết prompt\n+ Xử lý giới hạn tốc độ 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Học các khái niệm: + Amazon Simple Queue Service (Amazon SQS) + Amazon Simple Notification Service (SNS)\n+ AWS Organizations + Amazon Macie + AWS Direct Connect 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Tìm hiểu cách set up amplify Ôn lại Amazon Cognito 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 10: Nắm vững kiến thức về Amazon S3\nCách hoạt động của S3 – object storage. Các loại Storage Class và khi nào nên dùng (Standard, IA, Glacier…). S3 Access Point để quản lý truy cập theo từng ứng dụng. Biết Security Hub dùng để tổng hợp cảnh báo bảo mật từ nhiều dịch vụ AWS.\nHiểu các tiêu chuẩn như CIS, PCI DSS trong Security Hub.\nHiểu NoSQL trên AWS.\nKiến trúc serverless, partition key, sort key, RCU/WCU.\nKhái niệm backup plan.\nCách tự động hóa sao lưu.\nCác resource hỗ trợ backup.\nHọc các dịch vụ hỗ trợ truyền thông \u0026amp; bảo mật\nAmazon SQS – hàng đợi tin nhắn. Amazon SNS – dịch vụ pub/sub. AWS Organizations – quản lý đa tài khoản. Amazon Macie – bảo vệ dữ liệu nhạy cảm. AWS Direct Connect – kết nối mạng chuyên dụng. Tìm hiểu cách setup AWS Amplify để build frontend + backend.\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.11-week11/","title":"Worklog Tuần 11","tags":[],"description":"","content":"Mục tiêu tuần 11: Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 Tham gia sự kiện AWS Mastery Series #2 17/11/2025 17/11/2025 3 - Tìm hiểu thêm về Infrastrucure as Code (IaC) Tìm hiểu về CI/CD 18/11/2025 18/11/2025 4 - Thực hành: setup Amplify with React + React app created with Vite + Amplify CLI configured + Basic routing configured 19/11/2025 19/11/2025 5 - Lên ý tưởng và xây dựng Template cho project 20/11/2025 20/11/2025 6 - Thu thập và chuẩn bị Document cho projetc 21/11/2025 21/11/2025 Kết quả đạt được tuần 11: Hoàn thiện setup dự án với Amplify + React\nNâng cao kiến thức AWS \u0026amp; DevOps (IaC, CI/CD)\nChuẩn bị đầy đủ tài liệu để bắt đầu xây dựng dự án\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/1-worklog/1.12-week12/","title":"Worklog Tuần 12","tags":[],"description":"","content":"Mục tiêu tuần 12: Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Xây dựng giao diện cho Login Page 24/11/2025 24/11/2025 3 - Xây dựng giao diện cho Chat Page 25/11/2025 25/11/2025 4 - Xây dựng giao diện cho Admin Page 26/11/2025 16/11/2025 5 - chỉnh sửa lại giao diện cho project 27/11/2025 27/11/2025 6 - Tạo template cho ppt và chuẩn bị cho bài thuyết trình 28/11/2025 28/11/2025 Kết quả đạt được tuần 12: Xây dựng giao diện cho project cuối kỳ "},{"uri":"https://tranvanan5.github.io/internship-report/vi/3-blogstranslated/","title":"Các bài blogs đã dịch","tags":[],"description":"","content":"Tại đây sẽ là phần liệt kê, giới thiệu các blogs mà các bạn đã dịch. Ví dụ:\nBlog 1 - AWS Lambda giới thiệu mức giá theo tầng cho nhật ký Amazon CloudWatch và các đích ghi nhật ký bổ sung Blog này giới thiệu các cập nhật mới của AWS Lambda liên quan đến ghi nhật ký (logging), giúp các ứng dụng serverless tối ưu chi phí và linh hoạt hơn khi quản lý log. Bài viết giải thích việc Lambda giờ đây áp dụng định giá theo bậc (tiered pricing) cho CloudWatch Logs, giúp chi phí ghi nhật ký giảm mạnh khi khối lượng log tăng cao. Bên cạnh CloudWatch Logs, Lambda cũng hỗ trợ Amazon S3 và Amazon Data Firehose làm đích ghi nhật ký mới, tạo sự linh hoạt trong lưu trữ, phân tích và tích hợp với các nền tảng quan sát khác. Blog cũng đề cập đến các cải tiến như advanced logging controls, CloudWatch Logs Infrequent Access, và Live Tail để tăng khả năng quan sát. Cuối cùng, bài viết hướng dẫn cách cấu hình log destination và đưa ra các thực hành tối ưu chi phí ghi nhật ký trong môi trường serverless.\nBlog 2 - Hướng dẫn triển khai AWS DMS: Xây dựng khả năng di chuyển cơ sở dữ liệu linh hoạt thông qua thử nghiệm, giám sát và SOP Blog này là một hướng dẫn triển khai AWS Database Migration Service (AWS DMS), tập trung vào cách xây dựng một quy trình di chuyển cơ sở dữ liệu linh hoạt, an toàn và có thể lặp lại. Nội dung nhấn mạnh vào việc thử nghiệm trước khi migration, giám sát trong quá trình thực thi, và xây dựng SOP (Standard Operating Procedures) để đảm bảo việc di chuyển dữ liệu diễn ra trơn tru, giảm rủi ro và dễ bảo trì trong môi trường AWS.\nBlog 3 - Các chỉ số AWS KMS trong CloudWatch giúp bạn theo dõi và hiểu rõ hơn về mức độ sử dụng khóa KMS Blog này nói về tính năng mới của AWS KMS cho phép theo dõi mức độ sử dụng API ở cấp độ từng khóa thông qua Amazon CloudWatch, giúp người dùng dễ dàng nhận biết khóa nào đang được sử dụng nhiều nhất, khóa nào gây tốn chi phí hoặc có nguy cơ vượt hạn ngạch. Trước đây, việc này khá phức tạp và cần xây dựng giải pháp tùy chỉnh bằng CloudTrail và Athena, nhưng giờ đây CloudWatch đã cung cấp số liệu chi tiết cho từng khóa, giúp việc giám sát trở nên đơn giản và trực quan hơn. Bài viết cũng minh họa cách sử dụng các chỉ số mới này để truy vấn và tìm ra những khóa KMS tiêu thụ API nhiều nhất, cũng như thiết lập cảnh báo tự động dựa trên phát hiện bất thường để kịp thời xử lý khi khối lượng request tăng đột biến do lỗi cấu hình hoặc quy trình hoạt động. Nhờ đó, người dùng có thể nâng cao hiệu quả vận hành, giảm rủi ro bảo mật và tối ưu chi phí khi sử dụng AWS KMS.\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.3-bedrock-models/","title":"Kích hoạt Bedrock Models","tags":[],"description":"","content":"Trước khi triển khai giải pháp, bạn cần kích hoạt các mô hình Amazon Bedrock cần thiết trong tài khoản AWS của mình.\nCác bước Kích hoạt Mô hình Tìm kiếm Amazon Bedrock trong AWS Console Truy cập Model catalog từ menu điều hướng bên trái Chọn tên mô hình tương ứng: Anthropic Claude 3.5 Sonnet Anthropic Claude 3 Sonnet Anthropic Claude 3 Haiku Cohere - Embed Multilingual v3 Chọn \u0026ldquo;Open in playground\u0026rdquo; và gửi một tin nhắn thử nghiệm để kích hoạt từng mô hình Lưu ý: Đảm bảo bạn kích hoạt tất cả bốn mô hình trong khu vực ap-southeast-1 (Singapore) vì giải pháp được triển khai trong khu vực này.\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/4-eventparticipated/","title":"Các events đã tham gia","tags":[],"description":"","content":"Event 1 Tên sự kiện: AWS Cloud Mastery Series #1 workshop\nThời gian: 08:30 15/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 2 Tên sự kiện: BUILDING AGENTIC AI\nThời gian: 09:00 ngày 05/12/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.4-aws-cli/","title":"Cấu hình AWS CLI","tags":[],"description":"","content":"Cấu hình AWS CLI Để triển khai và quản lý giải pháp, bạn cần cấu hình AWS Command Line Interface (AWS CLI) với thông tin xác thực của mình.\nCác bước Bước 1: Kiểm tra AWS CLI đã cài đặt aws --version Nếu chưa có, cài đặt:\n# Download và cài đặt MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Bước 2: Tạo IAM User (nếu chưa có) Đăng nhập AWS Console Tìm kiếm \u0026ldquo;IAM\u0026rdquo; → Click IAM Sidebar trái → Users → Create user User name: arc-workshop-user Click Next Attach policies directly, chọn các policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Click Create user Bước 3: Tạo Access Key Vào user vừa tạo → Tab Security credentials Scroll xuống Access keys → Click Create access key Chọn Command Line Interface (CLI) Tick \u0026ldquo;I understand\u0026hellip;\u0026rdquo; → Next Description: ARC Workshop CLI Click Create access key ⚠️ QUAN TRỌNG: Copy hoặc download .csv file\nAccess key ID: AKIAXXXXXXXXXXXXXXXX Secret access key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Bước 4: Configure AWS CLI Mở PowerShell và chạy:\naws configure Nhập thông tin:\nAWS Access Key ID [None]: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name [None]: ap-southeast-1 Default output format [None]: json Bước 5: Verify Configuration Kiểm tra identity:\naws sts get-caller-identity Output mong đợi:\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/arc-workshop-user\u0026#34; } "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.5-data-preparation/","title":"Chuẩn bị Dữ liệu","tags":[],"description":"","content":"Chuẩn bị Dữ liệu Trước khi tạo AWS resources, bạn cần tải xuống tập dữ liệu mẫu để test hệ thống.\nBước 1: Tải Xuống Tập Dữ liệu Truy cập ARC Sample Data Tải dữ liệu về máy tính của bạn Giải nén file, sẽ tạo ra một thư mục có tên DATA Yêu cầu Documents Limit Value Format PDF (text-based hoặc scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Chuẩn bị AWS Resources Bước 2: Tạo S3 Bucket S3 Bucket dùng để lưu trữ documents PDF được upload.\nTìm kiếm S3 trong AWS Console Click Create bucket Cấu hình bucket: Bucket name: arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; (thay \u0026lt;YOUR-ACCOUNT-ID\u0026gt; bằng AWS Account ID của bạn) AWS Region: Asia Pacific (Singapore) ap-southeast-1 Giữ các settings khác mặc định Click Create bucket 💡 Tip: Để lấy AWS Account ID, chạy:\naws sts get-caller-identity --query Account --output text Hoặc tạo bằng CLI:\naws s3 mb s3://arc-documents-$(aws sts get-caller-identity --query Account --output text) --region ap-southeast-1 Bước 3: Tạo DynamoDB Table DynamoDB Table dùng để lưu metadata của documents.\nTìm kiếm DynamoDB trong AWS Console Click Create table Cấu hình table: Table name: arc-documents Partition key: doc_id (String) Sort key: sk (String) Table settings: Default settings Click Create table Hoặc tạo bằng CLI:\naws dynamodb create-table \\ --table-name arc-documents \\ --attribute-definitions \\ AttributeName=doc_id,AttributeType=S \\ AttributeName=sk,AttributeType=S \\ --key-schema \\ AttributeName=doc_id,KeyType=HASH \\ AttributeName=sk,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 Bước 4: Tạo SQS Queue SQS Queue dùng cho IDP pipeline xử lý documents.\nTìm kiếm SQS trong AWS Console Click Create queue Cấu hình queue: Type: Standard Name: arc-document-queue Giữ các settings khác mặc định Click Create queue Hoặc tạo bằng CLI:\naws sqs create-queue --queue-name arc-document-queue --region ap-southeast-1 Bước 5: Verify Resources Kiểm tra tất cả resources đã được tạo:\n# S3 Bucket aws s3 ls | grep arc-documents # DynamoDB Table aws dynamodb describe-table --table-name arc-documents --region ap-southeast-1 --query \u0026#34;Table.TableName\u0026#34; # SQS Queue aws sqs get-queue-url --queue-name arc-document-queue --region ap-southeast-1 Bước 6: Upload Dữ liệu lên S3 Upload các file PDF từ thư mục DATA đã tải về ở Bước 1:\n# Upload tất cả files từ thư mục DATA aws s3 cp DATA/ s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ --recursive # Hoặc upload từng file aws s3 cp DATA/sample-document.pdf s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ 💡 Tip: Thay \u0026lt;YOUR-ACCOUNT-ID\u0026gt; bằng AWS Account ID của bạn\nVerify upload:\naws s3 ls s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ Checklist Trước khi tiếp tục, đảm bảo:\nAWS CLI installed và configured Terraform installed Docker installed và running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created với đủ permissions Bedrock models được approve (Claude + Cohere) S3 Bucket created DynamoDB Table created SQS Queue created Sample documents uploaded to S3 Cấu hình bucket: Bucket name: arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; (thay \u0026lt;YOUR-ACCOUNT-ID\u0026gt; bằng AWS Account ID của bạn) AWS Region: Asia Pacific (Singapore) ap-southeast-1 Giữ các settings khác mặc định Click Create bucket 💡 Tip: Để lấy AWS Account ID, chạy: aws sts get-caller-identity --query Account --output text\nHoặc tạo bằng CLI:\naws s3 mb s3://arc-documents-$(aws sts get-caller-identity --query Account --output text) --region ap-southeast-1 Bước 3: Tạo DynamoDB Table DynamoDB Table dùng để lưu metadata của documents.\nTìm kiếm DynamoDB trong AWS Console Click Create table Cấu hình table: Table name: arc-documents Partition key: doc_id (String) Sort key: sk (String) Table settings: Default settings Click Create table Hoặc tạo bằng CLI:\naws dynamodb create-table \\ --table-name arc-documents \\ --attribute-definitions \\ AttributeName=doc_id,AttributeType=S \\ AttributeName=sk,AttributeType=S \\ --key-schema \\ AttributeName=doc_id,KeyType=HASH \\ AttributeName=sk,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 Bước 4: Tạo SQS Queue SQS Queue dùng cho IDP pipeline xử lý documents.\nTìm kiếm SQS trong AWS Console Click Create queue Cấu hình queue: Type: Standard Name: arc-document-queue Giữ các settings khác mặc định Click Create queue Hoặc tạo bằng CLI:\naws sqs create-queue --queue-name arc-document-queue --region ap-southeast-1 Bước 5: Verify Resources Kiểm tra tất cả resources đã được tạo:\n# S3 Bucket aws s3 ls | grep arc-documents # DynamoDB Table aws dynamodb describe-table --table-name arc-documents --region ap-southeast-1 --query \u0026#34;Table.TableName\u0026#34; # SQS Queue aws sqs get-queue-url --queue-name arc-document-queue --region ap-southeast-1 Bước 6: Upload Dữ liệu lên S3 Upload các file PDF từ thư mục DATA đã tải về ở Bước 1:\n# Upload tất cả files từ thư mục DATA aws s3 cp DATA/ s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ --recursive # Hoặc upload từng file aws s3 cp DATA/sample-document.pdf s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ 💡 Tip: Thay \u0026lt;YOUR-ACCOUNT-ID\u0026gt; bằng AWS Account ID của bạn\nVerify upload:\naws s3 ls s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ Checklist Trước khi tiếp tục, đảm bảo:\nAWS CLI installed và configured Terraform installed Docker installed và running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created với đủ permissions Bedrock models được approve (Claude + Cohere) Sample documents sẵn sàng "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Tổng quan Trong workshop này, chúng ta sẽ xây dựng ARC (Academic Research Chatbot) - một hệ thống chatbot thông minh hoạt động trên nền tảng AWS Serverless. Giải pháp này ứng dụng Generative AI và RAG (Retrieval-Augmented Generation) để hỗ trợ nghiên cứu học thuật, truy vấn tài liệu và trả lời câu hỏi một cách linh hoạt.\nThay vì trả lời các câu hỏi dựa trên kịch bản cố định (rule-based), hệ thống sử dụng mô hình Claude 3.5 Sonnet để hiểu ngôn ngữ tự nhiên, truy vấn dữ liệu từ cơ sở vector database và phản hồi người dùng một cách chính xác.\nMục tiêu Workshop Sau khi hoàn thành workshop, bạn sẽ:\nHiểu kiến trúc RAG và cách áp dụng vào thực tế Triển khai hệ thống chatbot hoàn chỉnh trên AWS Sử dụng Amazon Bedrock (Claude 3.5 Sonnet + Cohere Embed) Xây dựng IDP pipeline với Amazon Textract Implement vector search với Qdrant Deploy infrastructure với Terraform Tích hợp authentication với Amazon Cognito Nội dung Giới thiệu Các bước chuẩn bị Kích hoạt Bedrock Models Cấu hình AWS CLI Chuẩn bị Dữ liệu Triển khai Infrastructure Thiết lập Backend API Thiết lập IDP Pipeline Thiết lập Frontend Sử dụng Chatbot Sử dụng Admin Dashboard Dọn dẹp Tài nguyên "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.6-infrastructure/","title":"Triển khai giải pháp","tags":[],"description":"","content":"Trong phần này, chúng ta sẽ clone repository và triển khai toàn bộ hạ tầng AWS cho hệ thống ARC Chatbot.\nBước 1: Clone Repository Clone repository từ GitHub:\ngit clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Bước 2: Build Dashboard Trước khi triển khai ứng dụng, chúng ta cần build frontend dashboard.\nDi chuyển đến thư mục frontend cd frontend Cài đặt dependencies Chạy lệnh sau để cài đặt các thư viện cần thiết:\nnpm install Build Dashboard Sau khi cài đặt hoàn tất, chạy lệnh build:\nnpm run build Sau khi quá trình hoàn tất, một thư mục dist sẽ được tạo. Kiểm tra file index.html và thư mục assets:\nls dist/ # index.html assets/ Quay lại thư mục gốc của project cd .. Bước 3: Triển khai CDK Application Triển khai ứng dụng CDK. Quá trình sẽ mất khoảng 20-30 phút để triển khai tất cả các tài nguyên.\ncd terraform terraform init terraform apply --auto-approve ⚠️ Note: Nếu bạn gặp lỗi ở bước này, hãy đảm bảo Docker đang chạy trên máy tính của bạn.\n💡 Info: Thay thế \u0026lt;account_id\u0026gt; bằng AWS Account ID thực tế của bạn.\nBước 4: Xác minh Triển khai Sau khi hoàn thành tất cả các bước trên, môi trường của bạn đã được triển khai thành công.\nBạn có thể xác minh triển khai bằng cách kiểm tra:\nAWS Console: Kiểm tra các resources đã được tạo (EC2, S3, Cognito, DynamoDB, etc.) Terraform State: Chạy terraform state list để xem danh sách resources S3 Buckets: Bucket cho documents và frontend đã được tạo EC2 Instance: Instance cho backend đã được khởi tạo Kiểm tra Outputs terraform output Các outputs quan trọng:\nOutput Mô tả api_endpoint Backend API URL cognito_user_pool_id Cognito User Pool ID cognito_client_id Cognito App Client ID s3_bucket_name S3 bucket cho documents cloudfront_url Frontend URL Các Bước Tiếp Theo Bây giờ bạn có thể tiếp tục:\nThiết lập Backend Thiết lập IDP Pipeline Thiết lập Frontend "},{"uri":"https://tranvanan5.github.io/internship-report/vi/6-self-evaluation/","title":"Tự đánh giá","tags":[],"description":"","content":"Trong suốt thời gian thực tập tại Công ty TNHH Amazon Web Services Vietnam từ 08/09/2025 đến 28/11/2025, tôi đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã được trang bị tại trường vào môi trường làm việc thực tế.\nTôi đã tham gia thực tập tại First Cloud Journey, qua đó cải thiện kỹ năng tự học, quản lí thời gian, làm việc tại văn phòng, làm việc nhóm,\u0026hellip;.\nVề tác phong, tôi luôn cố gắng hoàn thành tốt nhiệm vụ, tuân thủ nội quy, và tích cực trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh một cách khách quan quá trình thực tập, tôi xin tự đánh giá bản thân dựa trên các tiêu chí dưới đây:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ☐ ☐ ✅ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ☐ ✅ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ☐ ✅ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ☐ ✅ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ✅ ☐ ☐ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ✅ ☐ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ☐ ☐ ✅ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ☐ ✅ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ☐ ✅ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ☐ ☐ ✅ Cần cải thiện Nâng cao kiến thức chuyên ngành, mở rộng thêm kiến thức Cải thiện trong cách tư duy giải quyết vấn đề Học cách giao tiếp tốt hơn trong giao tiếp hằng ngày và trong công việc, xử lý tình huống Cải thiện trách nhiệm và chủ động trong công việc "},{"uri":"https://tranvanan5.github.io/internship-report/vi/7-feedback/","title":"Chia sẻ, đóng góp ý kiến","tags":[],"description":"","content":" Tại đây bạn có thể tự do đóng góp ý kiến cá nhân về những trải nghiệm khi tham gia chương trình First Cloud Journey, giúp team FCJ cải thiện những vấn đề còn thiếu sót dựa trên các hạng mục sau:\nĐánh giá chung 1. Môi trường làm việc\nMôi trường làm việc rất thân thiện và cởi mở. Các thành viên trong FCJ luôn sẵn sàng hỗ trợ khi mình gặp khó khăn, kể cả ngoài giờ làm việc. Không gian làm việc gọn gàng, thoải mái, giúp mình tập trung tốt hơn.\n2. Sự hỗ trợ của mentor / team admin\nMentor hướng dẫn nhiệt tình, chi tiết, luôn sẵn sàng giải đáp thắc mắc và góp ý để học viên có thể phát triển tốt nhất. Team Admin luôn hỗ trợ tốt trong quá trình thực tậm và luôn chia sẻ các nguồn tài liệu, kiến thức hay.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nKiến thức và nhiệm vụ mình được nhận rất phù hợp với kiến thức mình đã học ở trường, nó giúp mình mở rộng thêm các kĩ năng mới, hỗ trợ cho việc học tập và làm việc của mình sau này. Từ đó, mình có thể củng cố kiến thức nền tảng, nâng cao kiến thức chuyên môn và tăng cơ hội việc làm của mình sau này.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nTrong quá trình thực tập, mình học được nhiều kĩ năng mới như là sử dụng các dịch vụ của AWS trong việc quản lí hạ tầng, quản lí web, nâng cao kĩ năng chuyên môn và học được kĩ năng làm việc, văn hóa tại doanh nghiệp. Được Mentor và doanh nghiệp tạo điều kiện tham gia nhiều sự kiện bổ ích và đuuợc chia sẻ nhiều kiến thức hay hỗ trợ cho sự phát triển của bản thân.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa công ty rất tích cực và vui vẻ. Mọi người rất hòa đồng, luôn sẵn sàng giúp đỡ lẫn nhau trong công việc và học tập. Nhưng không kém phần nghiêm túc khi thực hiện các nhiệm vụ và trách nhiệm khi làm việc.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty luôn hỗ trợ nơi thực tập 1 cách chỉnh chu và phù hợp nhất. Tạo điều kiện cho thực tập sinh được tham gia các sự kiện bổ ích nhằm nâng cao kiến thức chuyên môn, học hỏi thêm nhiều kiến thức bổ ích và giúp cập nhật công nghệ mới.\nMột số câu hỏi khác Điều tôi hài lòng nhất trong quá trình thực tập đó là được làm việc trong môi trường chuyên nghiệp và gặp được các anh/ chị Mentor nhiệt tình. Cùng với đó là được tham gia nhiều sự kiện tầm cỡ như là Việt Nam Cloud Day 2025. Điều tôi nghĩ công ty cần cải thiện là quản lí số lượng thực tập sinh tham gia và việc đăng kí lên văn phòng để nâng cao chất lượng của chương trình thực tập. Tôi sẽ giới thiệu cho bạn bè và khuyên họ nên thực tập ở đây. Vì đây là 1 chương trình thực tập tốt, giúp sinh viên có thể bổ sung kiến thức và tiếp cận được xu hướng việc làm trong thời buổi công nghệ phát triển như hiện nay. "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.7-backend/","title":"Thiết lập Backend API","tags":[],"description":"","content":"Thiết lập Backend API Trong phần này, bạn sẽ cấu hình Backend API (FastAPI) và Qdrant vector database trên EC2.\nKiến trúc Backend Internet → ALB (:80) → EC2 Private Subnet ├── FastAPI Container (:8000) ├── Qdrant Container (:6333) └── SQS Worker (background) 💡 Note: EC2 nằm trong Private Subnet, không có Public IP. Truy cập qua SSM Session Manager.\nBước 1: Truy cập EC2 qua Session Manager EC2 instance đã được tạo trong Private Subnet và không có Public IP. Sử dụng AWS Systems Manager Session Manager để truy cập.\nCách 1: AWS Console Mở AWS Console → EC2 → Instances Chọn instance arc-dev-app-server Click Connect → Session Manager → Connect Cách 2: AWS CLI # Lấy Instance ID từ Terraform output INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Kết nối qua SSM aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 ⚠️ Yêu cầu: Cài đặt Session Manager Plugin\nBước 2: Kiểm tra Services đã chạy EC2 đã được setup tự động qua user_data script khi Terraform tạo instance. Kiểm tra các services:\n# Chuyển sang ec2-user sudo su - ec2-user # Kiểm tra Docker containers docker ps Bạn sẽ thấy 2 containers đang chạy:\napp-fastapi-1 - FastAPI server (port 8000) app-qdrant-1 - Qdrant vector database (port 6333) # Kiểm tra Qdrant curl http://localhost:6333/collections # Kiểm tra FastAPI curl http://localhost:8000/health Bước 3: Deploy Backend Code Backend code sẽ được deploy qua CI/CD Pipeline (CodePipeline → CodeBuild → CodeDeploy). Tuy nhiên, để test nhanh, bạn có thể deploy thủ công:\ncd /home/ec2-user # Clone repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project/backend # Stop containers cũ cd /home/ec2-user/app docker-compose down # Copy backend code cp -r /home/ec2-user/ARC-project/backend/* /home/ec2-user/app/ # Start với code mới docker-compose up -d --build Bước 4: Cấu hình Environment Variables Tạo file .env với các giá trị từ Terraform outputs:\ncd /home/ec2-user/app # Lấy values từ Terraform outputs (chạy trên máy local) # terraform -chdir=terraform output cat \u0026gt; .env \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # AWS Configuration AWS_REGION=ap-southeast-1 # S3 S3_BUCKET_NAME=arc-documents-\u0026lt;account-id\u0026gt; # DynamoDB DYNAMODB_TABLE_NAME=arc-dev-documents # SQS SQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account-id\u0026gt;/arc-dev-document-queue # Qdrant (local container) QDRANT_HOST=qdrant QDRANT_PORT=6333 # Cognito COGNITO_USER_POOL_ID=ap-southeast-1_xxxxx COGNITO_CLIENT_ID=xxxxx # Bedrock BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0 EMBEDDING_MODEL_ID=amazon.titan-embed-text-v2:0 EOF 💡 Tip: Thay \u0026lt;account-id\u0026gt; và các giá trị xxxxx bằng outputs thực tế từ Terraform.\nBước 5: Restart Services # Restart để load .env mới docker-compose down docker-compose up -d # Kiểm tra logs docker-compose logs -f fastapi Bước 6: Verify qua ALB Backend được expose qua Application Load Balancer. Kiểm tra từ máy local:\n# Lấy ALB DNS từ Terraform output ALB_DNS=$(terraform -chdir=terraform output -raw alb_dns_name) # Test health endpoint curl http://$ALB_DNS/health # {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;} Bước 7: Kiểm tra Qdrant Collection # Trên EC2 curl http://localhost:6333/collections # Tạo collection cho documents (nếu chưa có) curl -X PUT http://localhost:6333/collections/documents \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;vectors\u0026#34;: { \u0026#34;size\u0026#34;: 1024, \u0026#34;distance\u0026#34;: \u0026#34;Cosine\u0026#34; } }\u0026#39; 💡 Note: Vector size 1024 tương ứng với Amazon Titan Embeddings v2.\nChecklist Truy cập EC2 qua Session Manager thành công Docker containers đang chạy (fastapi, qdrant) File .env đã được cấu hình Health check qua ALB thành công Qdrant collection đã được tạo Troubleshooting Không thể kết nối Session Manager # Kiểm tra SSM Agent trên EC2 sudo systemctl status amazon-ssm-agent # Kiểm tra IAM Role có policy AmazonSSMManagedInstanceCore Container không start # Xem logs docker-compose logs # Kiểm tra disk space df -h ALB health check fail # Kiểm tra Security Group cho phép port 8000 từ ALB # Kiểm tra FastAPI đang listen trên 0.0.0.0:8000 docker-compose logs fastapi "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.8-idp-pipeline/","title":"Thiết lập IDP Pipeline","tags":[],"description":"","content":"Trong phần này, bạn sẽ setup SQS Worker để xử lý documents qua IDP (Intelligent Document Processing) pipeline.\nIDP Flow Upload → S3 → DynamoDB (UPLOADED) → SQS ↓ EC2 Worker ↓ PyPDF2 (digital) / Textract (scanned) ↓ Chunk Text (1000 tokens) ↓ Cohere Embed Multilingual v3 (Bedrock) ↓ Qdrant (store vectors) ↓ DynamoDB (EMBEDDING_DONE) 💡 Note: Worker sử dụng PyPDF2 cho PDF digital (text-based) và Textract cho PDF scanned (image-based).\nDocument States Status Description UPLOADED File đã upload, đang chờ xử lý IDP_RUNNING Worker đang xử lý TEXTRACT_DONE OCR hoàn tất (chỉ cho scanned PDF) EMBEDDING_DONE Hoàn tất, sẵn sàng sử dụng FAILED Có lỗi xảy ra Bước 1: Truy cập EC2 qua Session Manager # Lấy Instance ID INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Kết nối aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 Sau khi kết nối:\nsudo su - ec2-user cd /home/ec2-user/backend 💡 Note: Trên EC2 có 2 folders:\napp/ - Boilerplate từ user_data script backend/ - Code thực tế được deploy qua CI/CD (chứa run_worker.py) Bước 2: Kiểm tra Worker Code Worker code nằm trong backend/run_worker.py. Kiểm tra file đã có:\nls -la # Phải có: run_worker.py, app/, requirements.txt Bước 3: Cấu hình Environment Đảm bảo file .env có đầy đủ các biến (trong folder backend/):\ncd /home/ec2-user/backend cat .env Các biến quan trọng cho IDP:\nSQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue S3_BUCKET=arc-documents-\u0026lt;account\u0026gt; QDRANT_HOST=localhost QDRANT_PORT=6333 AWS_REGION=ap-southeast-1 Bước 4: Start Worker Option A: Chạy trực tiếp (để debug) # Activate virtual environment (nếu có) source venv/bin/activate # Chạy worker python run_worker.py Worker sẽ hiển thị:\n============================================================ IDP Pipeline - SQS Worker ============================================================ Queue URL: https://sqs.ap-southeast-1.amazonaws.com/xxx/arc-dev-document-queue Bucket: arc-documents-xxx Region: ap-southeast-1 Qdrant: localhost:6333 ------------------------------------------------------------ Processing indefinitely (Ctrl+C to stop)... Option B: Chạy trong background với nohup nohup python run_worker.py \u0026gt; worker.log 2\u0026gt;\u0026amp;1 \u0026amp; # Kiểm tra process ps aux | grep run_worker # Xem logs tail -f worker.log Option C: Chạy trong Docker (recommended) # Thêm worker vào docker-compose.yml docker-compose up -d worker Bước 5: Test IDP Pipeline 5.1 Upload test file lên S3 # Từ máy local aws s3 cp test-sample.pdf s3://arc-documents-\u0026lt;account\u0026gt;/uploads/test-001.pdf 5.2 Tạo record trong DynamoDB aws dynamodb put-item \\ --table-name arc-dev-documents \\ --item \u0026#39;{ \u0026#34;doc_id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-001\u0026#34;}, \u0026#34;sk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;METADATA\u0026#34;}, \u0026#34;filename\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-sample.pdf\u0026#34;}, \u0026#34;s3_key\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;}, \u0026#34;status\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;UPLOADED\u0026#34;}, \u0026#34;uploaded_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;\u0026#39;$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#39;\u0026#34;} }\u0026#39; 5.3 Gửi message vào SQS aws sqs send-message \\ --queue-url https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue \\ --message-body \u0026#39;{ \u0026#34;doc_id\u0026#34;: \u0026#34;test-001\u0026#34;, \u0026#34;s3_key\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;, \u0026#34;filename\u0026#34;: \u0026#34;test-sample.pdf\u0026#34; }\u0026#39; Bước 6: Monitor Processing Xem logs của worker:\n# Nếu chạy trực tiếp # Logs hiển thị trên terminal # Nếu chạy background tail -f worker.log Logs thành công sẽ như sau:\n2024-01-15 10:30:00 - INFO - Received message for doc_id: test-001 2024-01-15 10:30:01 - INFO - Downloading from S3: uploads/test-001.pdf 2024-01-15 10:30:02 - INFO - Extracting text with PyPDF2... 2024-01-15 10:30:03 - INFO - Created 8 chunks from document 2024-01-15 10:30:05 - INFO - Generating embeddings with Cohere... 2024-01-15 10:30:10 - INFO - Stored 8 vectors for test-001 2024-01-15 10:30:10 - INFO - Updated status: EMBEDDING_DONE 2024-01-15 10:30:10 - INFO - Document test-001 processed successfully Bước 7: Verify Processing 7.1 Kiểm tra DynamoDB aws dynamodb get-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --query \u0026#39;Item.{status:status.S,chunks:chunk_count.N}\u0026#39; Expected output:\n{ \u0026#34;status\u0026#34;: \u0026#34;EMBEDDING_DONE\u0026#34;, \u0026#34;chunks\u0026#34;: \u0026#34;8\u0026#34; } 7.2 Kiểm tra Qdrant # Trên EC2 curl -s http://localhost:6333/collections/arc_documents/points/count | jq Expected output:\n{ \u0026#34;result\u0026#34;: { \u0026#34;count\u0026#34;: 8 } } Xử lý Lỗi Vấn đề Nguyên nhân Giải pháp Worker không nhận message SQS URL sai Kiểm tra .env Bedrock timeout Rate limit Tăng retry delay Qdrant connection refused Container chưa start docker-compose up -d qdrant FAILED status Xem error_message trong DynamoDB Fix và retry Retry Failed Document # Cập nhật status về UPLOADED để retry aws dynamodb update-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --update-expression \u0026#34;SET #s = :s\u0026#34; \\ --expression-attribute-names \u0026#39;{\u0026#34;#s\u0026#34;:\u0026#34;status\u0026#34;}\u0026#39; \\ --expression-attribute-values \u0026#39;{\u0026#34;:s\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;UPLOADED\u0026#34;}}\u0026#39; # Gửi lại message vào SQS aws sqs send-message \\ --queue-url $SQS_QUEUE_URL \\ --message-body \u0026#39;{\u0026#34;doc_id\u0026#34;:\u0026#34;test-001\u0026#34;,\u0026#34;s3_key\u0026#34;:\u0026#34;uploads/test-001.pdf\u0026#34;,\u0026#34;filename\u0026#34;:\u0026#34;test-sample.pdf\u0026#34;}\u0026#39; Checklist Truy cập EC2 qua Session Manager Worker code có sẵn Environment variables configured Worker đang chạy Test document uploaded to S3 SQS message sent Worker processed document (logs) Status = EMBEDDING_DONE trong DynamoDB Vectors stored trong Qdrant "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.9-frontend/","title":"Thiết lập Frontend","tags":[],"description":"","content":"Thiết lập Frontend Cấu hình và deploy Frontend React application với AWS Amplify.\nBước 1: Lấy Terraform Outputs cd terraform terraform output Ghi lại: cognito_user_pool_id, cognito_client_id, alb_dns_name\nBước 2: Cấu hình Environment cd ARC-project cp .env.example .env Chỉnh sửa .env:\nVITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_POOL_ID=ap-southeast-1_xxxxxxx VITE_COGNITO_CLIENT_ID=xxxxxxxxxxxxxxxxxxxxxxxxxx VITE_API_URL=http://arc-chatbot-dev-alb-xxxxx.ap-southeast-1.elb.amazonaws.com Bước 3: Install \u0026amp; Test Local npm install npm run dev Mở http://localhost:5173\nBước 4: Build \u0026amp; Deploy npm run build Push code lên GitHub, Amplify sẽ tự động deploy:\ngit add . git commit -m \u0026#34;Update frontend config\u0026#34; git push origin main 💡 Amplify app đã được tạo qua Terraform và connected với GitHub.\nBước 5: Cập nhật Cognito Callback URLs Sau khi có Amplify URL:\naws cognito-idp update-user-pool-client \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --client-id xxxxxxxxxx \\ --callback-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; \\ --logout-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; Bước 6: Tạo Test Users # Admin user aws cognito-idp admin-create-user \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --user-attributes Name=email,Value=admin@example.com \\ --temporary-password \u0026#34;TempPass123!\u0026#34; aws cognito-idp admin-add-user-to-group \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --group-name admin Xử lý Lỗi Lỗi Giải pháp CORS error Kiểm tra FastAPI CORS config \u0026ldquo;User pool does not exist\u0026rdquo; Kiểm tra VITE_COGNITO_POOL_ID Build failed Kiểm tra Amplify environment variables Checklist .env đã cấu hình Local dev server chạy được Amplify deploy thành công Cognito callback URLs đã cập nhật Login/Register hoạt động "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.10-using-chatbot/","title":"Sử dụng Chatbot","tags":[],"description":"","content":"Hướng dẫn sử dụng ARC Chatbot để tìm kiếm thông tin từ tài liệu nghiên cứu.\nTruy cập Local: http://localhost:5173 Production: Amplify URL từ bước trước Bước 1: Đăng nhập Nhập email và password Click Login Redirect đến Chat page Bước 2: Giao diện Chat Sau khi đăng nhập, bạn sẽ thấy:\nSidebar với menu Chat, History Header với user info và dark mode toggle Chat area với welcome message Bước 3: Đặt câu hỏi Nhập câu hỏi vào input box và nhấn Enter hoặc click Send.\nCâu hỏi tốt Loại Ví dụ Định nghĩa \u0026ldquo;What is a stack data structure?\u0026rdquo; So sánh \u0026ldquo;Compare stack and queue\u0026rdquo; Giải thích \u0026ldquo;Explain binary search algorithm\u0026rdquo; Tránh ❌ Quá chung: \u0026ldquo;Tell me about programming\u0026rdquo; ❌ Ngoài tài liệu: \u0026ldquo;What\u0026rsquo;s the weather today?\u0026rdquo; Bước 4: Citations (Trích dẫn) Mỗi câu trả lời có citations hiển thị nguồn tài liệu:\n📚 Sources: [1] data-structures.pdf - Page 12 - Score: 85% [2] algorithms.pdf - Page 45 - Score: 72% Click vào citation để xem chi tiết document.\nField Mô tả [1], [2] Số thứ tự citation Filename Tên file PDF Page Số trang Score Độ liên quan (%) Bước 5: Conversation History Click History trong sidebar Xem danh sách các cuộc hội thoại trước Click vào conversation để load lại Click trash icon để xóa Bước 6: New Chat Click New Chat trong sidebar để bắt đầu cuộc hội thoại mới.\nFeatures Feature Mô tả Streaming Response hiển thị từng phần Markdown Hỗ trợ code blocks, lists, headers Dark Mode Toggle trong header History Lưu và load lại conversations Checklist Đăng nhập thành công Gửi query và nhận response Citations hiển thị đúng Click citation xem document History hoạt động New Chat hoạt động "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.11-admin-dashboard/","title":"Sử dụng Admin Dashboard","tags":[],"description":"","content":"Sử dụng Admin Dashboard Hướng dẫn sử dụng Admin Dashboard để quản lý documents.\nTruy cập URL: http://localhost:5173/admin (hoặc Amplify URL) Yêu cầu: Tài khoản thuộc group admin Bước 1: Đăng nhập Admin Đăng nhập với tài khoản admin (đã tạo ở bước trước).\nBước 2: Dashboard Overview Sau khi đăng nhập, bạn sẽ thấy:\nUpload section (drag \u0026amp; drop) Documents table với pagination Status filter và auto-refresh Bước 3: Upload Tài liệu 3.1. Chọn file Drag \u0026amp; drop PDF files vào vùng upload Hoặc click Browse Files để chọn 3.2. Upload Progress Mỗi file hiển thị progress bar và status:\nuploading - Đang upload success - Upload thành công error - Upload thất bại Bước 4: Document Status Sau khi upload, document sẽ được xử lý qua IDP pipeline:\nStatus Mô tả Thời gian UPLOADED Chờ xử lý - IDP_RUNNING Đang xử lý 1-5 phút EMBEDDING_DONE Sẵn sàng - FAILED Lỗi - 💡 Tip: Bật Auto-refresh (5s) để tự động cập nhật status.\nBước 5: Quản lý Documents Filter theo Status Sử dụng dropdown Status để lọc:\nAll Uploaded Processing Done Failed Pagination Documents được phân trang (5 items/page). Sử dụng pagination controls ở footer.\nView Document Click icon 👁️ để xem chi tiết document.\nDelete Document Click icon 🗑️ để xóa document.\n⚠️ Warning: Xóa document sẽ xóa khỏi S3, DynamoDB và Qdrant.\nBước 6: Processing History Click Processing History link để xem lịch sử xử lý documents.\nXử lý Lỗi Vấn đề Giải pháp Upload failed Kiểm tra file size (\u0026lt;50MB), format (PDF only) Document stuck in IDP_RUNNING Kiểm tra worker logs trên EC2 Document FAILED Xem error message trong Processing History Checklist Đăng nhập admin dashboard Upload document thành công Document processed (EMBEDDING_DONE) Filter/pagination hoạt động Auto-refresh hoạt động "},{"uri":"https://tranvanan5.github.io/internship-report/vi/5-workshop/5.12-cleanup/","title":"Dọn dẹp Tài nguyên","tags":[],"description":"","content":"Dọn dẹp Tài nguyên Sau khi hoàn thành workshop, dọn dẹp AWS resources để tránh phát sinh chi phí.\n⚠️ Cảnh báo: Các bước này sẽ XÓA VĨNH VIỄN tất cả data và resources!\nThứ tự Dọn dẹp Stop services trên EC2 Empty S3 buckets Terraform destroy Verify cleanup Bước 1: Stop Services trên EC2 Kết nối EC2 qua Session Manager:\nINSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 Stop Docker containers:\nsudo su - ec2-user cd /home/ec2-user/app # Stop containers docker-compose down # Remove volumes docker volume rm app_qdrant_storage Bước 2: Empty S3 Buckets S3 buckets phải empty trước khi Terraform destroy:\n# Lấy bucket name từ Terraform output BUCKET=$(terraform -chdir=terraform output -raw s3_bucket_name) # Empty bucket aws s3 rm s3://$BUCKET --recursive # Hoặc force delete aws s3 rb s3://$BUCKET --force Bước 3: Terraform Destroy cd terraform terraform plan -destroy terraform destroy Nhập yes khi được hỏi. Quá trình này mất khoảng 10-15 phút.\nBước 4: Manual Cleanup (nếu cần) Nếu còn resources chưa bị xóa:\n# CloudWatch Log Groups aws logs describe-log-groups --log-group-name-prefix /aws/arc | jq -r \u0026#39;.logGroups[].logGroupName\u0026#39; | xargs -I {} aws logs delete-log-group --log-group-name {} # EC2 Key Pair (nếu tạo manual) aws ec2 delete-key-pair --key-name arc-keypair # Amplify App (nếu tạo manual) aws amplify list-apps | jq -r \u0026#39;.apps[] | select(.name | contains(\u0026#34;arc\u0026#34;)) | .appId\u0026#39; | xargs -I {} aws amplify delete-app --app-id {} Bước 5: Verify Cleanup # Check EC2 aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=*arc*\u0026#34; --query \u0026#39;Reservations[].Instances[].InstanceId\u0026#39; # Check S3 aws s3 ls | grep arc # Check DynamoDB aws dynamodb list-tables --query \u0026#39;TableNames[?contains(@, `arc`)]\u0026#39; # Check Cognito aws cognito-idp list-user-pools --max-results 20 --query \u0026#39;UserPools[?contains(Name, `arc`)]\u0026#39; Tất cả commands trên không nên trả về kết quả.\nBước 6: Check Costs Mở AWS Billing Dashboard Kiểm tra Bills cho tháng hiện tại Set up Budgets alert cho tương lai Xử lý Lỗi Lỗi Giải pháp DependencyViolation Destroy theo thứ tự: terraform destroy -target=module.amplify trước BucketNotEmpty aws s3 rb s3://bucket-name --force DeleteConflict (IAM) Detach policies trước khi delete role Resource in use Đợi vài phút rồi retry Kết luận Chúc mừng bạn đã hoàn thành workshop Academic Research Chatbot (ARC)! 🎉\nNhững gì bạn đã học: Triển khai RAG chatbot trên AWS Sử dụng Amazon Bedrock (Claude 3.5 Sonnet + Cohere Embed) Xây dựng IDP pipeline với PyPDF2/Textract Vector search với Qdrant Authentication với Cognito Infrastructure as Code với Terraform Tài nguyên bổ sung: AWS Bedrock Documentation Qdrant Documentation FastAPI Documentation Checklist Stop Docker containers trên EC2 Empty S3 buckets Terraform destroy thành công Verify không còn resources Check billing Cảm ơn bạn đã tham gia workshop! 🙏\n"},{"uri":"https://tranvanan5.github.io/internship-report/vi/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://tranvanan5.github.io/internship-report/vi/tags/","title":"Tags","tags":[],"description":"","content":""}]